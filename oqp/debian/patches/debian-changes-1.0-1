Description: Upstream changes introduced in version 1.0-1
 This patch has been created by dpkg-source during the package build.
 Here's the last changelog entry, hopefully it gives details on why
 those changes were made:
 .
 openquake (1.0-1) natty; urgency=low
 .
   * Initial release.
 .
 The person named in the Author field signed this changelog entry.
Author: Muharem Hrnjadovic <mh@foldr3.com>

---
The information above should follow the Patch Tagging Guidelines, please
checkout http://dep.debian.net/deps/dep3/ to learn about the format. Here
are templates for supplementary fields that you might want to add:

Origin: <vendor|upstream|other>, <url of original patch>
Bug: <url in upstream bugtracker>
Bug-Debian: http://bugs.debian.org/<bugnumber>
Bug-Ubuntu: https://launchpad.net/bugs/<bugnumber>
Forwarded: <no|not-needed|url proving that it has been forwarded>
Reviewed-By: <name and email of someone who approved the patch>
Last-Update: <YYYY-MM-DD>

--- /dev/null
+++ openquake-1.0/README
@@ -0,0 +1,5 @@
+Please note that the celeryconfig.py file needs to be revised to reflect
+your python-celery setup parameters.
+
+See also http://openquake.org/documentation/installation/manual-installation/
+for more information.
--- /dev/null
+++ openquake-1.0/bin/openquake
@@ -0,0 +1,153 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+
+"""Deterministic Risk Computations based on Hazard, Exposure and Vulnerability
+
+Expects to receive:
+    Shakemap (ground motion per grid cell)
+    Exposure (value per grid cell)
+    Vulnerability functions (multiple lists per grid cell)
+    Region of interest
+
+It can receive these either through gflags (current functionality), or
+through a configuration file.
+
+Expects to compute:
+    A grid of loss-ratio curves and store the results in XML
+    A grid of loss curves and store the results in XML 
+    A map of losses at each interval and store the results in GeoTIFF
+
+"""
+
+import os
+import sys
+
+# this is a hack so that it is easier to test these scripts,
+# it will add the proper directories to the path so that 
+# this script can be run from a checkout
+if os.path.exists(os.path.join(os.path.dirname(os.path.dirname(__file__)),
+                  'openquake')):
+    sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
+    
+from fabric.api import env, local, run
+from multiprocessing import Process
+
+from openquake import flags
+from openquake import logs
+
+from openquake import job
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('config_file', 'openquake-config.gem', 'OpenGEM configuration file')
+
+flags.DEFINE_boolean('profile', False, 'Run profiler?')
+flags.DEFINE_boolean('load_profile', False, 'Load profiler data?')
+flags.DEFINE_string('profile_log', 'gem-risk.profile', 'Profiling log')
+
+flags.DEFINE_boolean('partition', False, 'Partition job?')
+flags.DEFINE_boolean('server', False, 'Launch redis-server and RabbitMQ subprocs')
+flags.DEFINE_boolean('worker', False, 'Launch celery subprocs')
+
+
+def _launch_worker_subprocs():
+    """Start celery as python subprocs.
+
+    There should only be one copy of celery running.
+
+    If a process is already running, don't kill it
+    and carry on.
+
+    If a process is not running, launch it as a
+    multiprocessing.Process
+    """
+    # set the host string (otherwise, the user will be prompted for one)
+    env.host_string = 'localhost'
+    
+    # launch celery if it's not already running
+    if not _is_running('[c]elery'):
+        print "Starting celery..."
+        Process(target=local, args=('celeryd',)).start()
+
+
+def _launch_server_subprocs():
+    """Start redis-server and RabbitMQ as python subprocs.
+
+    There should only be one copy of RabbitMQ running  
+    and (for now) only one copy of redis-server.
+
+    If a process is already running, don't kill it
+    and carry on.
+
+    If a process is not running, launch it as a
+    multiprocessing.Process
+    """
+    # set the host string (otherwise, the user will be prompted for one)
+    env.host_string = 'localhost'
+
+    
+    # launch rabbitmq if it's not already running
+    if not _is_running('[r]abbitmq'):
+        print "Starting rabbitmq..."
+        Process(target=local, args=('sudo rabbitmq-server',)).start()
+
+ 
+    # launch kvs if it's not already running
+    if not _is_running('[r]edis-server'):
+        print "Starting redis-server..."
+        Process(target=local, args=('redis-server',)).start()
+
+
+def _is_running(proc_name):
+    """Checks to see if a process is already
+    running by executing a 'ps aux | grep proc_name'.
+
+    Be sure to surroud the first letter
+    of the proc_name with [] to avoid getting
+    a false positive from the grep
+    (example: '[r]edis-server', instead of 'redis-server').
+    """
+    # set fabric to warn only (so fabric run() calls with no
+    # response don't crash and burn)
+    env.warn_only = True
+
+    is_running = run('ps aux | grep %s' % proc_name) != ''
+    # reset warn only
+    env.warn_only = False
+
+    return is_running
+
+
+if __name__ == '__main__':
+    args = FLAGS(sys.argv)
+    logs.init_logs()
+    
+    # Collect inputs
+    # Determine Processing type
+    # Validate input data
+    
+    # Prepare final configuration, save it
+    # Hash final config, store that
+    
+    # Kick off processing tasks, and wait...
+    # Collate results
+    # Generate output
+    
+    if FLAGS.profile:
+        import cProfile
+        cProfile.run('tasks.main(FLAGS.vulnerability, \
+                        FLAGS.hazard_curves, FLAGS.region, \
+                        FLAGS.exposure, FLAGS.loss_map)', FLAGS.profile_log)
+    elif FLAGS.load_profile:
+        import pstats
+        p = pstats.Stats(FLAGS.profile_log)
+        p.sort_stats('cumulative').print_stats(30)    
+    elif FLAGS.server:
+        # launch redis and rabbitmq
+        _launch_server_subprocs()
+    elif FLAGS.worker:
+        # launch celery
+        _launch_worker_subprocs()
+    else:
+        job.run_job(FLAGS.config_file)
--- /dev/null
+++ openquake-1.0/src/celeryconfig.py
@@ -0,0 +1,24 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+Config for all installed OpenGEM binaries and modules.
+Should be installed by setup.py into /etc/openquake 
+eventually.
+"""
+
+import sys
+
+from openquake import logs
+logs.init_logs()
+
+sys.path.append('.')
+
+BROKER_HOST = "localhost"
+BROKER_PORT = 5672
+BROKER_USER = "celeryuser"
+BROKER_PASSWORD = "celery"
+BROKER_VHOST = "celeryvhost"
+
+CELERY_RESULT_BACKEND = "amqp"
+
+CELERY_IMPORTS = ("openquake.risk.job", "openquake.hazard.tasks")
--- /dev/null
+++ openquake-1.0/src/default.gem
@@ -0,0 +1,86 @@
+# Default values for OpenQuake jobs
+
+[general]
+
+REGION_GRID_SPACING = 0.1
+OUTPUT_DIR = computed_output
+
+
+[HAZARD]
+
+HAZARD_CALCULATION_MODE = Event Based
+NUMBER_OF_LOGIC_TREE_SAMPLES = 40
+NUMBER_OF_SEISMICITY_HISTORIES = 8
+
+COMPUTE_MEAN_HAZARD_CURVE = false
+
+# INPUT FILES
+SOURCE_MODEL_LOGIC_TREE_FILE = ErfLogicTree.inp
+GMPE_LOGIC_TREE_FILE = GmpeLogicTree.inp
+
+GROUND_MOTION_CORRELATION = true
+
+INTENSITY_MEASURE_TYPE = PGA
+COMPONENT = Average Horizontal (GMRotI50)
+PERIOD = 0.0
+DAMPING = 5.0
+INTENSITY_MEASURE_LEVELS = 0.005, 0.007, 0.098, 0.0137, 0.0192, 0.0269, 0.0376, 0.0527, 0.0738, 0.103, 0.145, 0.203, 0.284, 0.397, 0.556, 0.778, 1.09, 1.52, 2.13
+
+MINIMUM_MAGNITUDE = 5.0
+INVESTIGATION_TIME = 50.0
+MAXIMUM_DISTANCE = 200.0
+WIDTH_OF_MFD_BIN = 0.1
+
+GMPE_TRUNCATION_TYPE = 2 Sided
+TRUNCATION_LEVEL = 3
+STANDARD_DEVIATION_TYPE = Total
+REFERENCE_VS30_VALUE = 760.0
+REFERENCE_DEPTH_TO_2PT5KM_PER_SEC_PARAM = 5.0
+SADIGH_SITE_TYPE = Rock
+
+INCLUDE_AREA_SOURCES = true
+TREAT_AREA_SOURCE_AS = Point Sources
+AREA_SOURCE_DISCRETIZATION = 0.1
+AREA_SOURCE_MAGNITUDE_SCALING_RELATIONSHIP = W&C 1994 Mag-Length Rel.
+
+INCLUDE_GRID_SOURCES = true
+TREAT_GRID_SOURCE_AS = Point Sources
+GRID_SOURCE_MAGNITUDE_SCALING_RELATIONSHIP = W&C 1994 Mag-Length Rel.
+
+INCLUDE_FAULT_SOURCE = true
+FAULT_RUPTURE_OFFSET = 5.0
+FAULT_SURFACE_DISCRETIZATION = 1.0
+FAULT_MAGNITUDE_SCALING_SIGMA = 0.0
+RUPTURE_ASPECT_RATIO = 1.5
+RUPTURE_FLOATING_TYPE = Along strike and down dip
+FAULT_MAGNITUDE_SCALING_RELATIONSHIP = W&C 1994 Mag-Length Rel.
+
+INCLUDE_SUBDUCTION_FAULT_SOURCE = true
+SUBDUCTION_FAULT_RUPTURE_OFFSET = 10.0
+SUBDUCTION_FAULT_SURFACE_DISCRETIZATION = 10.0
+SUBDUCTION_FAULT_MAGNITUDE_SCALING_SIGMA = 0.0
+SUBDUCTION_RUPTURE_ASPECT_RATIO = 1.5
+SUBDUCTION_RUPTURE_FLOATING_TYPE = Along strike and down dip
+SUBDUCTION_FAULT_MAGNITUDE_SCALING_RELATIONSHIP = W&C 1994 Mag-Length Rel.
+
+
+
+# GENERATED OUTPUT
+
+[RISK]
+
+RISK_CALCULATION_MODE = Probabilistic Event
+RISK_CELL_SIZE = 0.1
+
+# INPUT
+EXPOSURE = exposure.xml
+VULNERABILITY = vulnerability.xml
+# HAZARD_CURVES = hazard_curves.xml
+
+# GENERATED OUTPUT
+CONDITIONAL_LOSS_POE = 0.01 0.02 0.05
+
+LOSS_CURVES_OUTPUT_PREFIX = risk
+LOSS_MAP = loss_map.tiff
+LOSS_RATIO_MAP = loss_ratio_map.tiff
+
--- /dev/null
+++ openquake-1.0/src/flags.py
@@ -0,0 +1,17 @@
+"""
+Global command-line flags for configuration, plus a wrapper around gflags.
+In the future, we may extend this to use either cement, or the nova 
+gflags extensions.
+"""
+
+# pylint: disable=W0401, W0622, W0614
+
+from gflags import * 
+from gflags import FLAGS
+from gflags import DEFINE_string
+from gflags import DEFINE_boolean, DEFINE_integer
+
+(_, _, _, _) = FLAGS, DEFINE_boolean, DEFINE_integer, DEFINE_string
+
+DEFINE_string('debug', 'warn', 
+    'Turns on debug logging and verbose output')
--- /dev/null
+++ openquake-1.0/src/shapes.py
@@ -0,0 +1,522 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+Collection of base classes for processing 
+spatially-related data."""
+
+import math
+
+import geohash
+import json
+import numpy
+
+from shapely import geometry
+from shapely import wkt
+from scipy.interpolate import interp1d
+
+from openquake import kvs
+from openquake import flags
+from openquake import java
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_integer('distance_precision', 12, 
+    "Points within this geohash precision will be considered the same point")
+
+LineString = geometry.LineString # pylint: disable=C0103
+Point = geometry.Point           # pylint: disable=C0103
+
+
+class Region(object):
+    """A container of polygons, used for bounds checking"""
+    def __init__(self, polygon):
+        self._grid = None
+        self.polygon = polygon
+        self.cell_size = 0.1
+        # TODO(JMC): Make this a multipolygon instead?
+
+    @classmethod
+    def from_coordinates(cls, coordinates):
+        """Build a region from a set of coordinates"""
+        polygon = geometry.Polygon(coordinates)
+        return cls(polygon)
+    
+    @classmethod
+    def from_simple(cls, top_left, bottom_right):
+        """Build a region from two corners (top left, bottom right)"""
+        points = [top_left,
+                  (top_left[0], bottom_right[1]),
+                  bottom_right,
+                  (bottom_right[0], top_left[1])]
+        return cls.from_coordinates(points)
+    
+    @classmethod
+    def from_file(cls, path):
+        """Load a region from a wkt file with a single polygon"""
+        with open(path) as wkt_file:
+            polygon = wkt.loads(wkt_file.read())
+            return cls(polygon=polygon)
+    
+    @property
+    def bounds(self):
+        """Returns a bounding box containing the whole region"""
+        return self.polygon.bounds
+    
+    @property
+    def lower_left_corner(self):
+        """Lower left corner of the containing bounding box"""
+        (minx, miny, _maxx, _maxy) = self.bounds
+        return Site(minx, miny)
+        
+    @property
+    def lower_right_corner(self):
+        """Lower right corner of the containing bounding box"""
+        (_minx, miny, maxx, _maxy) = self.bounds
+        return Site(maxx, miny)
+            
+    @property
+    def upper_left_corner(self):
+        """Upper left corner of the containing bounding box"""
+        (minx, _miny, _maxx, maxy) = self.bounds
+        return Site(minx, maxy)
+        
+    @property
+    def upper_right_corner(self):
+        """Upper right corner of the containing bounding box"""
+        (_minx, _miny, maxx, maxy) = self.bounds
+        return Site(maxx, maxy)
+    
+    @property
+    def grid(self):
+        """Returns a proxy interface that maps lat/lon 
+        to col/row based on a specific cellsize. Proxy is
+        also iterable."""
+        if not self._grid:
+            if not self.cell_size:
+                raise Exception(
+                    "Can't generate grid without cell_size being set")
+            self._grid = Grid(self, self.cell_size)
+        return self._grid
+    
+    @property
+    def sites(self):
+        """ Returns a list of sites created from iterating over self """
+        sites = []
+        
+        for site in self:
+            sites.append(site)
+        
+        return sites
+    
+    def __iter__(self):    
+        if not self.cell_size:
+            raise Exception(
+                "Can't generate grid without cell_size being set")
+        for gridpoint in self.grid:
+            yield gridpoint.site
+
+
+class RegionConstraint(Region):
+    """Extends a basic region to work as a constraint on parsers"""
+    def match(self, point):
+        """Point (specified by Point class or tuple) is contained?"""
+        if isinstance(point, Site):
+            point = point.point
+        if not isinstance(point, geometry.Point): 
+            point = geometry.Point(point[0], point[1])
+        test = self.polygon.contains(point) or self.polygon.touches(point)
+        return test
+
+
+class GridPoint(object):
+    """Simple (trivial) point class"""
+    def __init__(self, grid, column, row):
+        self.column = column
+        self.row = row
+        self.grid = grid
+    
+    def __eq__(self, other):
+        if isinstance(other, Site):
+            other = self.grid.point_at(other)
+        test = (self.column == other.column 
+                and self.row == other.row 
+                and self.grid == other.grid)
+        return test
+    
+    @property
+    def site(self):
+        """Trivial accessor for Site at Grid point"""
+        return self.grid.site_at(self)
+
+    def hash(self):
+        """Ugly hashing function
+        TODO(jmc): Fixme"""
+        return self.__hash__()
+        
+    def __repr__(self):
+        return str(self.__hash__())
+
+    def __hash__(self):
+        return self.column * 1000000000 + self.row 
+        #, int(self.grid.cell_size)
+
+    def __str__(self):
+        return self.__repr__()
+
+class BoundsException(Exception):
+    """Point is outside of region"""
+    pass
+
+
+class Grid(object):
+    """Grid is a proxy interface to Region, which translates
+    lat/lon to col/row"""
+    
+    def __init__(self, region, cell_size):
+        self.region = region
+        self.cell_size = cell_size
+        self.lower_left_corner = self.region.lower_left_corner
+        self.columns = self._longitude_to_column(
+                    self.region.upper_right_corner.longitude) + 1
+        self.rows = self._latitude_to_row(
+                    self.region.upper_right_corner.latitude) + 1
+
+    def check_site(self, site):
+        """Confirm that the site is contained by the region"""
+        return self.check_point(site.point)
+    
+    def check_point(self, point):    
+        """ Confirm that the point is within the polygon 
+        underlying the gridded region"""
+        if (self.region.polygon.contains(point)):
+            return True
+        if self.region.polygon.touches(point):
+            return True
+        raise BoundsException("Point is not on the Grid")
+    
+    def check_gridpoint(self, gridpoint):
+        """Confirm that the point is contained by the region"""
+        point = Point(self._column_to_longitude(gridpoint.column),
+                             self._row_to_latitude(gridpoint.row))
+        return self.check_point(point)
+    
+    def _latitude_to_row(self, latitude):
+        """Calculate row from latitude value"""
+        latitude_offset = math.fabs(latitude - self.lower_left_corner.latitude)
+        return int(round(latitude_offset / self.cell_size))
+
+    def _row_to_latitude(self, row):
+        """Determine latitude from given grid row"""
+        return self.lower_left_corner.latitude + ((row) * self.cell_size)
+
+    def _longitude_to_column(self, longitude):
+        """Calculate column from longitude value"""
+        longitude_offset = longitude - self.lower_left_corner.longitude
+        return int(round(longitude_offset / self.cell_size))
+    
+    def _column_to_longitude(self, column):
+        """Determine longitude from given grid column"""
+        return self.lower_left_corner.longitude + ((column) * self.cell_size)
+
+    def point_at(self, site):
+        """Translates a site into a matrix bidimensional point."""
+        self.check_site(site)
+        row = self._latitude_to_row(site.latitude)
+        column = self._longitude_to_column(site.longitude)
+        return GridPoint(self, column, row)
+    
+    def site_at(self, gridpoint):    
+        """Construct a site at the given grid point"""
+        return Site(self._column_to_longitude(gridpoint.column),
+                             self._row_to_latitude(gridpoint.row))
+    def __iter__(self):
+        for row in range(0, self.rows):
+            for col in range(0, self.columns):
+                try:
+                    point = GridPoint(self, col, row)
+                    self.check_gridpoint(point)
+                    yield point
+                except BoundsException, _e:
+                    print "GACK! at col %s row %s" % (col, row)
+                    print "Point at %s %s isnt on grid" % \
+                        (point.site.longitude, point.site.latitude)
+
+def c_mul(val_a, val_b):
+    """Ugly method of hashing string to integer
+    TODO(jmc): Get rid of points as dict keys!"""
+    return eval(hex((long(val_a) * val_b) & 0xFFFFFFFFL)[:-1])
+
+
+class Site(object):
+    """Site is a dictionary-keyable point"""
+    
+    def __init__(self, longitude, latitude):
+        self.point = geometry.Point(longitude, latitude)
+    
+    @property
+    def coords(self):
+        """Return a tuple with the coordinates of this point"""
+        return (self.longitude, self.latitude)
+
+    @property
+    def longitude(self):
+        """Point x value is longitude"""
+        return self.point.x
+
+    @property
+    def latitude(self):
+        """Point y value is latitude"""
+        return self.point.y
+
+    def __eq__(self, other):
+        return self.hash() == other.hash()
+    
+    def __ne__(self, other):
+        return not self == other
+
+    def equals(self, other):
+        """Verbose wrapper around =="""
+        return self.point.equals(other)
+
+    def hash(self):
+        """Ugly geohashing function, get rid of this!
+        TODO(jmc): Dont use sites as dict keys"""
+        return self._geohash()
+
+    def __hash__(self):
+        if not self:
+            return 0 # empty
+        geohash_val = self._geohash()
+        value = ord(geohash_val[0]) << 7
+        for char in geohash_val:
+            value = c_mul(1000003, value) ^ ord(char)
+        value = value ^ len(geohash_val)
+        if value == -1:
+            value = -2
+        return value
+    
+    def to_java(self):
+        """Converts to a Java Site object"""
+        jpype = java.jvm()
+        loc_class = jpype.JClass("org.opensha.commons.geo.Location")
+        site_class = jpype.JClass("org.opensha.commons.data.Site")
+        # TODO(JMC): Support named sites?
+        return site_class(loc_class(self.latitude, self.longitude))
+    
+    def _geohash(self):
+        """A geohash-encoded string for dict keys"""
+        return geohash.encode(self.point.y, self.point.x, 
+            precision=FLAGS.distance_precision)
+    
+    def __cmp__(self, other):
+        return self.hash() == other.hash()
+    
+    def __repr__(self):
+        return self.__str__()
+
+    def __str__(self):
+        return "<Site(%s, %s)>" % (self.longitude, self.latitude)
+
+
+class Field(object):
+    """Uses a 2 dimensional numpy array to store a field of values."""
+    def __init__(self, field=None, rows=1, cols=1):
+        if field is not None:
+            self.field = field
+        else:
+            self.field = numpy.zeros((rows, cols))
+    
+    def get(self, row, col):
+        """ Return the value at self.field[row][col] 
+            :param row
+            :param col
+        """
+        try:
+            return self.field[row][col]
+        except IndexError:
+            print "Field with shape [%s] doesn't have value at [%s][%s]" % (
+                self.field.shape, row, col)
+    
+    @classmethod
+    def from_json(cls, json_str, grid=None):
+        """Construct a field from a serialized version in
+        json format."""
+        assert grid
+        as_dict = json.JSONDecoder().decode(json_str)
+        return cls.from_dict(as_dict, grid=grid)
+    
+    @classmethod
+    def from_dict(cls, values, transform=math.exp, grid=None):
+        """Construct a field from a dictionary.
+        """
+        assert grid
+        assert grid.cell_size
+        field = numpy.zeros((grid.rows, grid.columns))
+        
+        for key, field_site in values.items(): #pylint: disable=W0612
+            point = grid.point_at(
+                Site(field_site['lon'], field_site['lat']))
+            field[point.row][point.column] = transform(float(field_site['mag']))
+
+        return cls(field)   
+
+class FieldSet(object):
+    """ An iterator for a set of fields """
+    
+    def __init__(self, as_dict, grid):
+        assert grid
+        self.grid = grid
+        self.fields = as_dict.values()[0] # NOTE: There's a junk wrapper
+    
+    @classmethod
+    def from_json(cls, json_str, grid=None):
+        """ Construct a field set from a serialized version in json format """
+        assert grid
+        as_dict = json.JSONDecoder().decode(json_str)
+        return cls(as_dict, grid=grid)
+    
+    def __iter__(self):
+        """Pop off the fields sequentially"""
+        for field in self.fields.values():
+            yield Field.from_dict(field, grid=self.grid)
+
+
+class Curve(object):
+    """This class defines a curve (discrete function)
+    used in the risk domain."""
+
+    @staticmethod
+    def from_json(json_str):
+        """Construct a curve from a serialized version in
+        json format."""
+        as_dict = json.JSONDecoder().decode(json_str)
+        return Curve.from_dict(as_dict)
+
+    @staticmethod
+    def from_dict(values):
+        """Construct a curve from a dictionary.
+        
+        The dictionary keys can be unordered and can be
+        whatever type can be converted to float with float().
+
+        """
+        
+        data = []
+        
+        for key, val in values.items():
+            data.append((float(key), val))
+
+        return Curve(data)
+
+    def __init__(self, values):
+        """Construct a curve from a sequence of tuples.
+        
+        The value on the first position of the tuple is the x value,
+        the value(s) on the second position is the y value(s).
+        
+        This class supports multiple y values for the same
+        x value, for example:
+        
+        Curve([(0.1, 1.0), (0.2, 2.0)]) # single y value
+        Curve([(0.1, (1.0, 0.5)), (0.2, (2.0, 0.5))]) # multiple y values
+        
+        or, with lists:
+        
+        Curve([(0.1, [1.0, 0.5]), (0.2, [2.0, 0.5])])
+        
+        The values can be in any order, for axample:
+        
+        Curve([(0.4, 1.0), (0.2, 2.0), (0.3, 2.0)])
+        
+        """
+
+        # sort the values on x axis
+        values = sorted(values, key=lambda data: data[0])
+        
+        elements = len(values)
+        self.x_values = numpy.empty(elements)
+        self.y_values = numpy.empty(elements)
+
+        if elements and type(values[0][1]) in (tuple, list):
+            self.y_values = numpy.empty((elements, len(values[0][1])))
+
+        for index, (key, val) in enumerate(values):
+            self.x_values[index] = key
+            self.y_values[index] = val
+
+    def __eq__(self, other):
+        return numpy.allclose(self.x_values, other.x_values) \
+                and numpy.allclose(self.y_values, other.y_values)
+
+    def __str__(self):
+        return "X Values: %s\nY Values: %s" % (
+                self.x_values.__str__(), self.y_values.__str__())
+
+    def rescale_abscissae(self, value) :
+        """Return a new curve with each abscissa value multiplied
+        by the value passed as parameter."""
+        
+        result = Curve(())
+        result.x_values = self.x_values * value
+        result.y_values = self.y_values
+        
+        return result
+
+    @property
+    def abscissae(self):
+        """Return the abscissa values of this curve in ascending order."""
+        return self.x_values
+
+    @property
+    def ordinates(self):
+        """Return the ordinate values of this curve in ascending order
+        of the corresponding abscissa values."""
+        return self.y_values
+    
+    @property
+    def is_multi_value(self):
+        """Return true if this curve describes multiple ordinate values,
+        false otherwise."""
+        return self.y_values.ndim > 1
+
+    def ordinate_for(self, x_value, y_index=0):
+        """Return the y value corresponding to the given x value."""
+        
+        y_values = self.y_values
+        
+        if self.y_values.ndim > 1:
+            y_values = self.y_values[:, y_index]
+        
+        return interp1d(self.x_values, y_values)(x_value)
+
+    def abscissa_for(self, y_value):
+        """Return the x value corresponding to the given y value."""
+
+        data = []
+        # inverting the function
+        for x_value in self.abscissae:
+            data.append((self.ordinate_for(x_value), x_value))
+        
+        return Curve(data).ordinate_for(y_value)
+
+    def ordinate_out_of_bounds(self, y_value):
+        """Check if the given value is outside the codomain boundaries."""
+        ordinates = list(self.ordinates)
+        ordinates.sort()
+        
+        return y_value < ordinates[0] or y_value > ordinates[-1]
+
+    def to_json(self):
+        """Serialize this curve in json format."""
+        as_dict = {}
+        
+        for index, x_value in enumerate(self.x_values):
+            if self.y_values.ndim > 1:
+                as_dict[str(x_value)] = list(self.y_values[index])
+            else:
+                as_dict[str(x_value)] = self.y_values[index]
+
+        return json.JSONEncoder().encode(as_dict)
+
+
+EMPTY_CURVE = Curve(())
--- /dev/null
+++ openquake-1.0/src/logs.py
@@ -0,0 +1,64 @@
+# -*- coding: utf-8 -*-
+"""
+Set up some system-wide loggers
+TODO(jmc): init_logs should take filename, or sysout
+TODO(jmc): support debug level per logger.
+
+"""
+import logging
+
+from openquake import flags
+FLAGS = flags.FLAGS
+
+LEVELS = {'debug': logging.DEBUG,
+          'info': logging.INFO,
+          'warn': logging.WARNING,
+          'error': logging.ERROR,
+          'critical': logging.CRITICAL,
+          # The default logging levels are: CRITICAL=50, ERROR=40, WARNING=30,
+          # INFO=20, DEBUG=10, NOTSET=0
+          # The 'validate' log level is defined here as 25 because it is 
+          # considered to be less critical than a WARNING but slightly more
+          # critical than INFO.
+          'validate': 25}
+
+RISK_LOG = logging.getLogger("risk")
+HAZARD_LOG = logging.getLogger("hazard")
+LOG = logging.getLogger()
+
+def init_logs():
+    """Load logging config, and set log levels based on flags"""
+    
+    level = LEVELS.get(FLAGS.debug, logging.ERROR)
+    logging.basicConfig(level=level)
+    logging.getLogger("amqplib").setLevel(logging.ERROR)
+    
+    LOG.setLevel(level)
+    RISK_LOG.setLevel(level)
+    HAZARD_LOG.setLevel(level)   
+
+def make_job_logger(job_id):
+    """Make a special logger object to be used just for a specific job. Acts
+    like normal logging.Logger object, but has additional logging method called
+    validate which basically wraps a call to logger.log() with the level
+    automatically specified as 'validate'."""
+    # 
+    def _validate(msg, *args, **kwargs):
+        """Basically a clone of the standard logger methods (like 'debug()')."""
+        # 'close' validate_logger instance inside this wrapper method
+        # this is nice because now we can just call logger_obj.validate()
+        # to make log entries at the 'validate' level.
+        return validate_logger.log(LEVELS.get('validate'), msg, *args, **kwargs)
+    
+    validate_logger = logging.getLogger(job_id)
+    # monkey patch _validate into the logger object
+    validate_logger.validate = _validate
+    # now the 'validate' logging method can be called on this object
+
+    validate_logger.setLevel(LEVELS.get('validate'))
+
+    # log to file in the CWD
+    log_file_path = "%s.log" % job_id
+    handler = logging.FileHandler(log_file_path)
+    validate_logger.addHandler(handler)
+    return validate_logger
--- /dev/null
+++ openquake-1.0/src/java.py
@@ -0,0 +1,91 @@
+"""Wrapper around our use of jpype.
+Includes classpath arguments, and heap size."""
+
+import logging
+import os
+import sys
+
+import jpype
+
+from openquake.logs import LOG
+from openquake import flags
+FLAGS = flags.FLAGS
+
+flags.DEFINE_boolean('capture_java_debug', True, 
+    "Pipe Java stderr and stdout to python stderr and stdout")
+
+JAVA_CLASSES = {
+    'LogicTreeProcessor' : "org.gem.engine.LogicTreeProcessor",
+    'KVS' : "org.gem.engine.hazard.redis.Cache",
+    'JsonSerializer' : "org.gem.JsonSerializer",
+    "EventSetGen" : "org.gem.calc.StochasticEventSetGenerator",
+    "Random" : "java.util.Random",
+    "GEM1ERF" : "org.gem.engine.hazard.GEM1ERF",
+    "HazardCalculator" : "org.gem.calc.HazardCalculator",
+    "Properties" : "java.util.Properties",
+    "CalculatorConfigHelper" : "org.gem.engine.CalculatorConfigHelper",
+    "Configuration" : "org.apache.commons.configuration.Configuration",
+    "ConfigurationConverter" : 
+        "org.apache.commons.configuration.ConfigurationConverter",
+    "ArbitrarilyDiscretizedFunc" : 
+        "org.opensha.commons.data.function.ArbitrarilyDiscretizedFunc",
+    "ArrayList" : "java.util.ArrayList",
+    "GmpeLogicTreeData" : "org.gem.engine.GmpeLogicTreeData",
+    "AttenuationRelationship" : "org.opensha.sha.imr.AttenuationRelationship",
+    "EqkRupForecastAPI" : "org.opensha.sha.earthquake.EqkRupForecastAPI",
+    "DoubleParameter" : "org.opensha.commons.param.DoubleParameter",
+    "StringParameter" : "org.opensha.commons.param.StringParameter",
+    "ParameterAPI" : "org.opensha.commons.param.ParameterAPI",
+    "DiscretizedFuncAPI" : 
+        "org.opensha.commons.data.function.DiscretizedFuncAPI",
+    "ProbabilityMassFunctionCalc" : "org.gem.calc.ProbabilityMassFunctionCalc",
+}
+
+logging.getLogger('jpype').setLevel(logging.ERROR)
+
+def jclass(class_key):
+    """Wrapper around jpype.JClass for short class names"""
+    jvm()
+    return jpype.JClass(JAVA_CLASSES[class_key])
+
+
+def jvm(max_mem=4000):
+    """Return the jpype module, after guaranteeing the JVM is running and 
+    the classpath has been loaded properly."""
+    jarpaths = (os.path.abspath(
+                    os.path.join(os.path.dirname(__file__), "../lib")),
+                os.path.abspath(
+                    os.path.join(os.path.dirname(__file__), "../dist")))
+    # TODO(JMC): Make sure these directories exist
+    # LOG.debug("Jarpath is %s", jarpaths)
+    if not jpype.isJVMStarted():
+        LOG.debug("Default JVM path is %s" % jpype.getDefaultJVMPath())
+        jpype.startJVM(jpype.getDefaultJVMPath(), 
+            "-Djava.ext.dirs=%s:%s" % jarpaths, 
+        #"-Dnet.spy.log.LoggerImpl=net.spy.memcached.compat.log.Log4JLogger",
+            # "-Dlog4j.debug",
+            "-Dlog4j.configuration=log4j.properties",
+            "-Dlog4j.rootLogger=%s, A1" % (FLAGS.debug.upper()),
+            # "-Dlog4j.rootLogger=DEBUG, A1",
+            "-Xmx%sM" % max_mem)
+        
+        if FLAGS.capture_java_debug:
+            mystream = jpype.JProxy("org.gem.IPythonPipe", inst=sys.stdout)
+            errstream = jpype.JProxy("org.gem.IPythonPipe", inst=sys.stderr)
+            outputstream = jpype.JClass("org.gem.PythonOutputStream")()
+            err_stream = jpype.JClass("org.gem.PythonOutputStream")()
+            outputstream.setPythonStdout(mystream)
+            err_stream.setPythonStdout(errstream)
+        
+            ps = jpype.JClass("java.io.PrintStream")
+            jpype.java.lang.System.setOut(ps(outputstream))
+            jpype.java.lang.System.setErr(ps(err_stream))
+        
+    return jpype
+
+
+# TODO(JMC): Use this later for logging:
+# self.PropertyConfigurator = jpype.JClass(
+#    'org.apache.log4j.PropertyConfigurator')
+# object.__getattribute__(self, 
+#    'PropertyConfigurator').configure(settings.LOG4J_PROPERTIES)
--- /dev/null
+++ openquake-1.0/src/producer.py
@@ -0,0 +1,123 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+Basic parser classes, these emit a series of objects
+while iteratively parsing an underlying file.
+
+TODO(jmc): merge this with the base output class, to
+produce a base codec class that can serialize and deserialize
+the same format.
+"""
+
+from openquake import logs
+
+
+class AttributeConstraint(object):
+    """A constraint that can be used to filter input elements based on some
+    attributes.
+    
+    The constructor requires a dictionary as argument. Items in this dictionary
+    have to match the corresponding ones in the checked site attributes object.
+    
+    """
+
+    def __init__(self, attribute):
+        self.attribute = attribute
+
+    def match(self, compared_attribute):
+        """ Compare self.attribute against the passed in compared_attribute
+        dict. If the compared_attribute dict does not contain all of the 
+        key/value pais from self.attribute, we return false. compared_attribute
+        may have additional key/value pairs.
+        """
+
+        for k, v in self.attribute.items():
+            if not ( k in compared_attribute and compared_attribute[k] == v ):
+                return False
+        
+        return True
+
+class FileProducer(object):
+    """
+    FileProducer is an interface for iteratively parsing
+    a file, and returning a sequence of objects.
+    
+    TODO(jmc): fold the attributes filter in here somewhere.
+    """
+
+    # required attributes for metadata parsing
+    REQUIRED_ATTRIBUTES = ()
+
+    # optional attributes for metadata parsing
+    OPTIONAL_ATTRIBUTES = ()
+
+    def __init__(self, path):
+        logs.LOG.debug('Found data at %s', path)
+        self.path = path
+
+        self.file = open(self.path, 'r')
+
+        # contains the metadata of the node currently parsed
+        self._current_meta = {}
+
+    def __iter__(self):
+        for rv in self._parse():
+            yield rv
+    
+    def reset(self):
+        """Sometimes we like to iterate the filter more than once."""
+        self.file.seek(0)
+        # contains the metadata of the node currently parsed
+        self._current_meta = {}
+
+    def filter(self, region_constraint=None, attribute_constraint=None):
+        """Filters the elements readed by this producer.
+        
+        region_constraint has to be of type shapes.RegionConstraint and
+        specifies the region to which the elements of this producer
+        should be contained.
+        
+        attribute_constraint has to be of type producer.AttributeConstraint
+        and specifies additional attributes to use in the filtring process.
+
+        """
+        for next_val in iter(self):
+            if (attribute_constraint is not None and
+                    (region_constraint is None 
+                        or region_constraint.match(next_val[0])) and
+                    attribute_constraint.match(next_val[1])) or \
+               (attribute_constraint is None and
+                    (region_constraint is None 
+                        or region_constraint.match(next_val[0]))):
+                
+                yield next_val
+
+    def _set_meta(self, element):
+        """Sets the metadata of the node that is currently
+        being processed."""
+        
+        for (required_attr, attr_type) in self.REQUIRED_ATTRIBUTES:
+            attr_value = element.get(required_attr)
+            
+            if attr_value is not None:
+                self._current_meta[required_attr] = attr_type(attr_value)
+            else:
+                error_str = "element %s: missing required " \
+                        "attribute %s" % (element, required_attr)
+                
+                raise ValueError(error_str) 
+
+        for (optional_attr, attr_type) in self.OPTIONAL_ATTRIBUTES:
+            attr_value = element.get(optional_attr)
+            
+            if attr_value is not None:
+                self._current_meta[optional_attr] = attr_type(attr_value)
+
+    def _parse(self):
+        """Parse one logical item from the file.
+
+        Should return a (point, data) tuple.
+        
+        """
+
+        raise NotImplementedError
--- /dev/null
+++ openquake-1.0/src/__init__.py
@@ -0,0 +1,30 @@
+"""
+OpenQuake is an open-source platform for the calculation of hazard, risk, 
+and socio-economic impact. It is a project of the Global Earthquake Model, 
+nd may be extended by other organizations to address additional classes 
+of peril.
+
+For more information, please see the website at http://www.globalquakemodel.org
+This software may be downloaded at http://github.com/gem/openquake
+
+The continuous integration server is at http://openquake.globalquakemodel.org
+Up-to-date sphinx documentation is at http://openquake.globalquakemodel.org/docs
+
+This software is licensed under the LGPL license, for more details 
+please see the LICENSE file.
+
+Copyright (c) 2010, GEM Foundation.
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU Lesser General Public License as published by
+    the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU Lesser General Public License for more details.
+
+    You should have received a copy of the GNU Lesser General Public License
+    along with this program.  If not, see <http://www.gnu.org/licenses/>.
+"""
--- /dev/null
+++ openquake-1.0/src/jobber.py
@@ -0,0 +1,37 @@
+# -*- coding: utf-8 -*-
+"""
+Main jobber module.
+
+This needs to be written to daemonise, pull a job from the queue, execute it
+and then move on to the next one.
+"""
+
+
+
+#     dd                                             tt                dd
+#     dd   eee  pp pp   rr rr    eee    cccc   aa aa tt      eee       dd
+# dddddd ee   e ppp  pp rrr  r ee   e cc      aa aaa tttt  ee   e  dddddd
+#dd   dd eeeee  pppppp  rr     eeeee  cc     aa  aaa tt    eeeee  dd   dd
+# dddddd  eeeee pp      rr      eeeee  ccccc  aaa aa  tttt  eeeee  dddddd
+#               pp
+
+from openquake import logs
+
+LOGGER = logs.LOG
+
+# TODO (ac): This class is not covered by unit tests...
+class Jobber(object):
+    """The Jobber class is responsible to evaluate the configuration settings
+    and to execute the computations in parallel tasks (using the celery
+    framework and the message queue RabbitMQ).
+    """
+
+    @staticmethod
+    def run(job):
+        """Core method of Jobber. It splits the requested computation
+        in blocks and executes these as parallel tasks.
+        """
+
+        LOGGER.debug("running jobber, job_id = %s" % job.job_id)
+        job.launch()
+        LOGGER.debug("Jobber run ended")
--- /dev/null
+++ openquake-1.0/src/xml.py
@@ -0,0 +1,24 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""Constants and helper functions for XML processing,
+including namespaces, and namespace maps."""
+
+
+NRML_SCHEMA_FILE = 'nrml.xsd'
+
+NRML_NS = 'http://openquake.org/xmlns/nrml/0.1'
+GML_NS = 'http://www.opengis.net/gml'
+QUAKEML_NS = 'http://quakeml.org/xmlns/quakeml/1.1'
+
+NRML = "{%s}" % NRML_NS
+GML = "{%s}" % GML_NS
+
+QUAKEML = "{%s}" % QUAKEML_NS
+NSMAP = {None: NRML_NS, "gml": GML_NS}
+NSMAP_WITH_QUAKEML = {None: NRML_NS, "gml": GML_NS, "qml": QUAKEML_NS}
+
+# TODO(fab): remove these when transition to new schema is completed
+NRML_SCHEMA_FILE_OLD = 'old/nrml.xsd'
+GML_NS_OLD = 'http://www.opengis.net/gml/profile/sfgml/1.0'
+GML_OLD = "{%s}" % GML_NS_OLD
+NSMAP_OLD = {None: NRML_NS, "gml": GML_NS_OLD}
\ No newline at end of file
--- /dev/null
+++ openquake-1.0/src/writer.py
@@ -0,0 +1,45 @@
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+Base classes for the output methods of the various codecs.
+"""
+
+
+class FileWriter(object):
+    """Simple output half of the codec process."""
+
+    def __init__(self, path):
+        self.path = path
+        self.file = None
+        self._init_file()
+        self.root_node = None
+    
+    def _init_file(self):
+        """Get the file handle open for writing"""
+        self.file = open(self.path, "w")
+
+    def write(self, point, value):
+        """Write out an individual point (unimplemented)"""
+        raise NotImplementedError
+
+    def write_header(self):
+        """Write out the file header"""
+        pass
+
+    def write_footer(self):
+        """Write out the file footer"""
+        pass
+
+
+    def close(self):
+        """Close and flush the file. Send finished messages."""
+        self.file.close()
+
+    def serialize(self, iterable):
+        """Wrapper for writing all items in an iterable object."""
+        if isinstance(iterable, dict):
+            iterable = iterable.items()
+        self.write_header()
+        for key, val in iterable:
+            self.write(key, val)
+        self.write_footer()
+        self.close()
--- /dev/null
+++ openquake-1.0/src/settings.py
@@ -0,0 +1,109 @@
+""" Django settings for openquake project.  """
+
+#ADD THIS
+import os
+GEOGRAPHIC_ADMIN_DIR = os.path.dirname(__file__)
+
+# http://docs.djangoproject.com/en/dev/topics/testing/#id1
+# Your user must be a postgrest superuser
+# Avoid specifying your password with: ~/.pgpass
+# http://www.postgresql.org/docs/8.3/interactive/libpq-pgpass.html
+TEST_RUNNER = 'django.contrib.gis.tests.run_gis_tests'
+
+DEBUG = True
+TEMPLATE_DEBUG = DEBUG
+
+ADMINS = (
+    # ('Aurea Moemke', 'aurea.moemke@sed.ethz.ch'),
+)
+
+MANAGERS = ADMINS
+
+SPATIALITE_LIBRARY_PATH = '/Library/Frameworks/SQLite3.framework/SQLite3'
+# 'postgresql_psycopg2', 'postgresql', 'mysql', 'sqlite3' or 'oracle'.
+DATABASE_ENGINE = 'postgresql_psycopg2'
+DATABASE_NAME = 'openquake.db' # Or path to database file if using sqlite3.
+DATABASE_USER = ''             # Not used with sqlite3.
+DATABASE_PASSWORD = ''         # Not used with sqlite3.
+DATABASE_HOST = ''             # Set to empty string for localhost.
+DATABASE_PORT = ''             # Set to empty string for default.
+
+# Not used at this point but you'll need it here if you 
+# want to enable a google maps baselayer within your
+# OpenLayers maps
+GOOGLE_MAPS_API_KEY = 'abcdefg'
+
+GIS_DATA_DIR = os.path.join(GEOGRAPHIC_ADMIN_DIR, '../tests/data')
+
+# Local time zone for this installation. Choices can be found here:
+# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
+# although not all choices may be available on all operating systems.
+# If running in a Windows environment this must be set to the same as your
+# system time zone.
+TIME_ZONE = 'America/Chicago'
+
+# Language code for this installation. All choices can be found here:
+# http://www.i18nguy.com/unicode/language-identifiers.html
+LANGUAGE_CODE = 'en-us'
+
+SITE_ID = 1
+
+# If you set this to False, Django will make some optimizations so as not
+# to load the internationalization machinery.
+USE_I18N = True
+
+# Absolute path to the directory that holds media.
+# Example: "/home/media/media.lawrence.com/"
+MEDIA_ROOT = os.path.join(GEOGRAPHIC_ADMIN_DIR, '../media')
+
+# URL that handles the media served from MEDIA_ROOT. Make sure to use a
+# trailing slash if there is a path component (optional in other cases).
+# Examples: "http://media.lawrence.com", "http://example.com/media/"
+MEDIA_URL = '/media/'
+
+# URL prefix for admin media -- CSS, JavaScript and images. Make sure to use a
+# trailing slash.
+# Examples: "http://foo.com/media/", "/media/".
+ADMIN_MEDIA_PREFIX = '/admin_media/'
+
+# Make this unique, and don't share it with anybody.
+SECRET_KEY = '!!0h0h*84l#&e*ki(t1f#)8h0wnj9!#r=w!3wxqy$e7o$3=@b&'
+
+# List of callables that know how to import templates from various sources.
+TEMPLATE_LOADERS = (
+    'django.template.loaders.filesystem.load_template_source',
+    'django.template.loaders.app_directories.load_template_source',
+)
+
+MIDDLEWARE_CLASSES = (
+    'django.middleware.common.CommonMiddleware',
+    'django.contrib.sessions.middleware.SessionMiddleware',
+    'django.contrib.auth.middleware.AuthenticationMiddleware',
+    'django.middleware.doc.XViewMiddleware',
+)
+
+ROOT_URLCONF = 'urls'
+
+TEMPLATE_DIRS = (
+    os.path.join(GEOGRAPHIC_ADMIN_DIR, '../templates'),
+    # Don't forget to use absolute paths, not relative paths.
+)
+
+INSTALLED_APPS = (
+    'django.contrib.auth',
+    'django.contrib.contenttypes',
+    'django.contrib.sessions',
+    'django.contrib.sites',
+    'django.contrib.admin',
+    'django.contrib.databrowse',
+    'django.contrib.gis',
+    'openquake.seismicsources',
+)
+
+LOSS_CURVES_OUTPUT_FILE = 'loss-curves-jobber.xml'
+
+KVS_PORT = 6379
+KVS_HOST = "localhost"
+
+SOURCEGEOM_SHP = 'seismicsources/data/sourcegeometrycatalog.shp'
+WORLD_SHP = 'world/data/TM_WORLD_BORDERS-0.3.shp'
--- /dev/null
+++ openquake-1.0/src/hazard/job.py
@@ -0,0 +1,11 @@
+""" The hazard job mixin proxy. """
+
+from openquake.job.mixins import Mixin
+
+
+class HazJobMixin(Mixin):
+    """ Proxy mixin for mixing in hazard job behaviour """
+    mixins = {}
+
+
+Mixin.register("Hazard", HazJobMixin, order=1)
--- /dev/null
+++ openquake-1.0/src/hazard/__init__.py
@@ -0,0 +1,6 @@
+"""
+Ostensibly core computation methods for hazard engine. Most of the hard
+computation is backed by the Java HazardEngine via the hazard_wrapper
+"""
+
+import openquake.hazard.opensha
--- /dev/null
+++ openquake-1.0/src/hazard/tasks.py
@@ -0,0 +1,120 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+
+"""
+The following tasks are defined in the hazard engine:
+    * generate_erf
+    * compute_hazard_curve
+    * compute_mgm_intensity
+"""
+
+import json
+
+from celery.decorators import task
+from celery.task.sets import subtask
+
+from openquake import job
+from openquake import kvs
+
+from openquake.hazard import job as hazjob
+from openquake.hazard import classical_psha
+from openquake.job import mixins
+
+
+@task
+def generate_erf(job_id):
+    """
+    Stubbed ERF generator 
+
+    Takes a job_id, returns a job_id. 
+
+    Connects to the Java HazardEngine using hazardwrapper, waits for an ERF to
+    be generated, and then writes it to KVS. 
+    """
+
+    # TODO(JM): implement real ERF computation
+
+    erf_key = kvs.generate_product_key(job_id, kvs.tokens.ERF_KEY_TOKEN)
+    kvs.get_client().set(erf_key, json.JSONEncoder().encode([job_id]))
+
+    return job_id
+
+@task
+def compute_ground_motion_fields(job_id, site_list, gmf_id, seed):
+    """ Generate ground motion fields """
+    # TODO(JMC): Use a block_id instead of a site_list
+    hazengine = job.Job.from_kvs(job_id)
+    with mixins.Mixin(hazengine, hazjob.HazJobMixin, key="hazard"):
+        #pylint: disable=E1101
+        hazengine.compute_ground_motion_fields(site_list, gmf_id, seed)
+
+
+def write_out_ses(job_file, stochastic_set_key):
+    """ Write out Stochastic Event Set """
+    hazengine = job.Job.from_file(job_file)
+    with mixins.Mixin(hazengine, hazjob.HazJobMixin, key="hazard"):
+        ses = kvs.get_value_json_decoded(stochastic_set_key)
+        hazengine.write_gmf_files(ses) #pylint: disable=E1101
+
+@task
+def compute_hazard_curve(job_id, site_list, realization, callback=None):
+    """ Generate hazard curve for a given site list. """
+    hazengine = job.Job.from_kvs(job_id)
+    with mixins.Mixin(hazengine, hazjob.HazJobMixin, key="hazard"):
+        keys = hazengine.compute_hazard_curve(site_list, realization)
+
+        if callback:
+            subtask(callback).delay(job_id, site_list)
+
+        return keys
+
+@task
+def compute_mgm_intensity(job_id, block_id, site_id):
+    """
+    Compute mean ground intensity for a specific site.
+    """
+
+    kvs_client = kvs.get_client(binary=False)
+
+    mgm_key = kvs.generate_product_key(job_id, kvs.tokens.MGM_KEY_TOKEN,
+        block_id, site_id)
+    mgm = kvs_client.get(mgm_key)
+
+    if not mgm:
+        # TODO(jm): implement hazardwrapper and make this work.
+        # TODO(chris): uncomment below when hazardwapper is done
+
+        # Synchronous execution.
+        #result = hazardwrapper.apply(args=[job_id, block_id, site_id])
+        #mgm = kvs_client.get(mgm_key)
+        pass
+
+    return json.JSONDecoder().decode(mgm)
+
+@task
+def compute_mean_curves(job_id, sites):
+    """Compute the mean hazard curve for each site given."""
+
+    # pylint: disable=E1101
+    logger = compute_mean_curves.get_logger()
+
+    logger.info("Computing MEAN curves for %s sites (job_id %s)"
+            % (len(sites), job_id))
+
+    return classical_psha.compute_mean_hazard_curves(job_id, sites)
+    #subtask(compute_quantile_curves).delay(job_id, sites)
+
+@task
+def compute_quantile_curves(job_id, sites):
+    """Compute the quantile hazard curve for each site given."""
+
+    # pylint: disable=E1101
+    logger = compute_quantile_curves.get_logger()
+
+    logger.info("Computing QUANTILE curves for %s sites (job_id %s)"
+            % (len(sites), job_id))
+
+    engine = job.Job.from_kvs(job_id)
+    return classical_psha.compute_quantile_hazard_curves(engine, sites)
+    #subtask(serialize_quantile_curves).delay(job_id, sites)
+
--- /dev/null
+++ openquake-1.0/src/hazard/classical_psha.py
@@ -0,0 +1,178 @@
+# -*- coding: utf-8 -*-
+"""
+Collection of functions that compute stuff using
+as input data produced with the classical psha method.
+"""
+
+from numpy import array # pylint: disable=E1101, E0611
+from scipy.stats.mstats import mquantiles
+
+from openquake import kvs
+from openquake.logs import LOG
+
+
+QUANTILE_PARAM_NAME = "QUANTILE_LEVELS"
+
+
+def compute_mean_curve(curves):
+    """Compute a mean hazard curve.
+    
+    The input parameter is a list of arrays where each array
+    contains just the Y values of the corresponding hazard curve.
+    """
+    return array(curves).mean(axis=0)
+
+
+def compute_quantile_curve(curves, quantile):
+    """Compute a quantile hazard curve.
+
+    The input parameter is a list of arrays where each array
+    contains just the Y values of the corresponding hazard curve.
+    """
+    result = []
+
+    if len(array(curves).flat):
+        result = mquantiles(curves, quantile, axis=0)[0]
+    
+    return result
+
+
+def _extract_y_values_from(curve):
+    """Extract from a serialized hazard curve (in json format)
+    the Y values used to compute the mean hazard curve.
+    
+    The serialized hazard curve has this format:
+    {"site_lon": 1.0, "site_lat": 1.0, "curve": [{"x": 0.1, "y": 0.2}, ...]}
+    """
+    y_values = []
+
+    for point in curve:
+        y_values.append(float(point["y"]))
+        
+    return y_values
+
+def _reconstruct_curve_list_from(curve_array):
+    """Reconstruct the x,y hazard curve list from numpy array, and leave
+    out the un-needed x value."""
+    
+    return [{'y': poe} for poe in curve_array]
+    #for poe in curve_array:
+        #curve.append({'y': poe})
+
+def _acceptable(quantile):
+    """Return true if the quantile value taken from the configuration
+    file is valid, false otherwise."""
+    try:
+        quantile = float(quantile)
+        return quantile >= 0.0 and quantile <= 1.0
+
+    except ValueError:
+        return False
+
+
+def curves_at(job_id, site):
+    """Return all the json deserialized hazard curves for
+    a single site (different realizations)."""
+    pattern = "%s*%s*%s*%s" % (kvs.tokens.HAZARD_CURVE_KEY_TOKEN,
+            job_id, site.longitude, site.latitude)
+
+    curves = []
+    raw_curves = kvs.mget_decoded(pattern)
+
+    for raw_curve in raw_curves:
+        curves.append(_extract_y_values_from(raw_curve["curve"]))
+    
+    return curves
+
+def hazard_curve_keys_for_job(job_id, sites, 
+                              hc_token=kvs.tokens.HAZARD_CURVE_KEY_TOKEN):
+    """Return the KVS keys of hazard curves for a given job_id
+    and for a given list of sites.
+    """
+
+    kvs_keys = []
+    for site in sites:
+        pattern = "%s*%s*%s*%s" % (hc_token, job_id, site.longitude, 
+                                   site.latitude)
+        curr_keys = kvs.get_keys(pattern)
+        if curr_keys is not None and len(curr_keys) > 0:
+            kvs_keys.extend(curr_keys)
+    
+    return kvs_keys
+
+def mean_hazard_curve_keys_for_job(job_id, sites):
+    """Return the KVS keys of mean hazard curves for a given job_id
+    and for a given list of sites.
+    """
+    return hazard_curve_keys_for_job(job_id, sites, 
+        kvs.tokens.MEAN_HAZARD_CURVE_KEY_TOKEN)
+    
+def quantile_hazard_curve_keys_for_job(job_id, sites):
+    """Return the KVS keys of quantile hazard curves for a given job_id
+    and for a given list of sites.
+    """
+    return hazard_curve_keys_for_job(job_id, sites, 
+        kvs.tokens.QUANTILE_HAZARD_CURVE_KEY_TOKEN)
+
+def _extract_quantiles_from_config(job):
+    """Extract the set of valid quantiles from the configuration file."""
+    quantiles = []
+
+    if job.has(QUANTILE_PARAM_NAME):
+        raw_quantiles = job.params[QUANTILE_PARAM_NAME].split()
+        quantiles = [float(x) for x in raw_quantiles if _acceptable(x)]
+
+    return quantiles
+
+
+def compute_mean_hazard_curves(job_id, sites):
+    """Compute a mean hazard curve for each site in the list
+    using as input all the pre-computed curves for different realizations."""
+
+    keys = []
+    for site in sites:
+        mean_curve = {"site_lon": site.longitude, "site_lat": site.latitude,
+            "curve": _reconstruct_curve_list_from(compute_mean_curve(
+            curves_at(job_id, site)))}
+
+        key = kvs.tokens.mean_hazard_curve_key(job_id, site)
+        keys.append(key)
+
+        LOG.debug("MEAN curve at %s is %s" % (key, mean_curve))
+
+        kvs.set_value_json_encoded(key, mean_curve)
+
+    return keys
+
+
+def compute_quantile_hazard_curves(job, sites):
+    """Compute a quantile hazard curve for each site in the list
+    using as input all the pre-computed curves for different realizations.
+    
+    The QUANTILE_LEVELS parameter in the configuration file specifies
+    all the values used in the computation.
+    """
+
+    keys = []
+
+    quantiles = _extract_quantiles_from_config(job)
+
+    LOG.debug("List of QUANTILES is %s" % quantiles)
+
+    for site in sites:
+        for quantile in quantiles:
+
+            quantile_curve = {"site_lat": site.latitude,
+                "site_lon": site.longitude, 
+                "curve": _reconstruct_curve_list_from(compute_quantile_curve(
+                curves_at(job.id, site), quantile))}
+
+            key = kvs.tokens.quantile_hazard_curve_key(
+                    job.id, site, quantile)
+            keys.append(key)
+
+            LOG.debug("QUANTILE curve at %s is %s" % (key, quantile_curve))
+
+            kvs.set_value_json_encoded(key, quantile_curve)
+
+    return keys
--- /dev/null
+++ openquake-1.0/src/hazard/opensha.py
@@ -0,0 +1,556 @@
+# -*- coding: utf-8 -*-
+"""
+Wrapper around the OpenSHA-lite java library.
+"""
+
+import math
+import os
+import random
+import numpy
+
+from openquake import java
+from openquake import kvs
+from openquake import logs
+from openquake import settings
+from openquake import shapes
+
+from openquake.hazard import job
+from openquake.hazard import tasks
+from openquake.job.mixins import Mixin
+from openquake.kvs import tokens
+from openquake.output import geotiff
+from openquake.output import hazard as hazard_output
+from openquake import logs
+
+LOG = logs.LOG
+
+# NOTE: this refers to how the values are stored in KVS. In the config
+# file, values are stored untransformed (i.e., the list of IMLs is
+# not stored as logarithms).
+IML_SCALING = {'PGA' : numpy.log,  # pylint: disable=E1101
+               'MMI' : lambda iml: iml,
+               'PGV' : numpy.log, # pylint: disable=E1101
+               'PGD' : numpy.log, # pylint: disable=E1101
+               'SA' : numpy.log,  # pylint: disable=E1101
+              }
+
+HAZARD_CURVE_FILENAME_PREFIX = 'hazardcurve'
+
+def preload(fn): # pylint: disable=E0213
+    """A decorator for preload steps that must run on the Jobber node"""
+    def preloader(self, *args, **kwargs):
+        """Validate job"""
+        self.cache = java.jclass("KVS")(
+                settings.KVS_HOST, 
+                settings.KVS_PORT)
+        self.calc = java.jclass("LogicTreeProcessor")(
+                self.cache, self.key)
+        return fn(self, *args, **kwargs) # pylint: disable=E1102
+    return preloader
+
+
+class BasePSHAMixin(Mixin): 
+    """Contains common functionality for PSHA Mixins."""
+
+    def store_source_model(self, seed):
+        """Generates an Earthquake Rupture Forecast, using the source zones and
+        logic trees specified in the job config file. Note that this has to be
+        done currently using the file itself, since it has nested references to
+        other files."""
+    
+        LOG.info("Storing source model from job config")
+        key = kvs.generate_product_key(self.id, kvs.tokens.SOURCE_MODEL_TOKEN)
+        print "source model key is", key
+        self.calc.sampleAndSaveERFTree(self.cache, key, seed)
+    
+    def store_gmpe_map(self, seed):    
+        """Generates a hash of tectonic regions and GMPEs, using the logic tree
+        specified in the job config file."""
+        key = kvs.generate_product_key(self.id, kvs.tokens.GMPE_TOKEN)
+        print "GMPE map key is", key
+        self.calc.sampleAndSaveGMPETree(self.cache, key, seed)
+
+    def generate_erf(self):
+        """Generate the Earthquake Rupture Forecast from the currently stored
+        source model logic tree."""
+        key = kvs.generate_product_key(self.id, kvs.tokens.SOURCE_MODEL_TOKEN)
+        sources = java.jclass("JsonSerializer").getSourceListFromCache(
+                    self.cache, key)
+        erf = java.jclass("GEM1ERF")(sources)
+        self.calc.setGEM1ERFParams(erf)
+        return erf
+
+    def set_gmpe_params(self, gmpe_map):
+        """Push parameters from configuration file into the GMPE objects"""
+        jpype = java.jvm()
+        gmpe_lt_data = self.calc.createGmpeLogicTreeData()
+        for tect_region in gmpe_map.keySet():
+            gmpe = gmpe_map.get(tect_region)
+            gmpe_lt_data.setGmpeParams(self.params['COMPONENT'], 
+                self.params['INTENSITY_MEASURE_TYPE'], 
+                jpype.JDouble(float(self.params['PERIOD'])), 
+                jpype.JDouble(float(self.params['DAMPING'])), 
+                self.params['GMPE_TRUNCATION_TYPE'], 
+                jpype.JDouble(float(self.params['TRUNCATION_LEVEL'])), 
+                self.params['STANDARD_DEVIATION_TYPE'], 
+                jpype.JDouble(float(self.params['REFERENCE_VS30_VALUE'])), 
+                jpype.JObject(gmpe, java.jclass("AttenuationRelationship")))
+            gmpe_map.put(tect_region, gmpe)
+
+
+    def generate_gmpe_map(self):
+        """Generate the GMPE map from the stored GMPE logic tree."""
+        key = kvs.generate_product_key(self.id, kvs.tokens.GMPE_TOKEN)
+        gmpe_map = java.jclass("JsonSerializer").getGmpeMapFromCache(
+                                                    self.cache,key)
+        self.set_gmpe_params(gmpe_map)
+        return gmpe_map
+        
+    def get_iml_list(self):
+        """Build the appropriate Arbitrary Discretized Func from the IMLs,
+        based on the IMT""" 
+        
+        iml_list = java.jclass("ArrayList")()
+        for val in self.params['INTENSITY_MEASURE_LEVELS'].split(","):
+            iml_list.add(
+                IML_SCALING[self.params['INTENSITY_MEASURE_TYPE']](
+                float(val)))
+        return iml_list
+
+    def parameterize_sites(self, site_list):
+        """Convert python Sites to Java Sites, and add default parameters."""
+        # TODO(JMC): There's Java code for this already, sets each site to have
+        # the same default parameters
+       
+        jpype = java.jvm()
+        jsite_list = java.jclass("ArrayList")()
+        for x in site_list:
+            site = x.to_java()
+            
+            vs30 = java.jclass("DoubleParameter")(jpype.JString("Vs30"))
+            vs30.setValue(float(self.params['REFERENCE_VS30_VALUE']))
+            depth25 = java.jclass("DoubleParameter")("Depth 2.5 km/sec")
+            depth25.setValue(float(
+                    self.params['REFERENCE_DEPTH_TO_2PT5KM_PER_SEC_PARAM']))
+            sadigh = java.jclass("StringParameter")("Sadigh Site Type")
+            sadigh.setValue(self.params['SADIGH_SITE_TYPE'])
+            site.addParameter(vs30)
+            site.addParameter(depth25)
+            site.addParameter(sadigh)
+            jsite_list.add(site)
+        return jsite_list
+
+
+class ClassicalMixin(BasePSHAMixin):
+    """Classical PSHA method for performing Hazard calculations.
+    
+    Implements the JobMixin, which has a primary entry point of execute().
+    Execute is responsible for dispatching celery tasks.
+
+    Note that this Mixin, during execution, will always be an instance of the
+    Job class, and thus has access to the self.params dict, full of config
+    params loaded from the Job configuration file."""
+
+    @preload
+    def execute(self):
+       
+        results = []
+ 
+        source_model_generator = random.Random()
+        source_model_generator.seed(
+                self.params.get('SOURCE_MODEL_LT_RANDOM_SEED', None))
+
+        gmpe_generator = random.Random()
+        gmpe_generator.seed(self.params.get('GMPE_LT_RANDOM_SEED', None))
+        
+        realizations = int(self.params['NUMBER_OF_LOGIC_TREE_SAMPLES'])
+
+        # tally and log the total number of sites
+        # TODO (LB): with a large number of sites, this could get expensive
+        # and we might want to change this
+        total_sites = 0
+        for site_list in self.site_list_generator():
+            total_sites += len(site_list)
+
+        LOG.info('Going to run classical PSHA hazard for %s realizations '\
+                 'and %s sites' % (realizations, total_sites))
+
+        for realization in xrange(0, realizations):
+            LOG.info('Calculating hazard curves for realization %s'
+                     % realization)
+            pending_tasks = []
+            results_per_realization = []
+            self.store_source_model(source_model_generator.getrandbits(32))
+            self.store_gmpe_map(source_model_generator.getrandbits(32))
+            
+            for site_list in self.site_list_generator():
+                pending_tasks.append(tasks.compute_hazard_curve.delay(
+                        self.id, site_list, realization))
+
+            for task in pending_tasks:
+                task.wait()
+                if task.status != 'SUCCESS': 
+                    raise Exception(task.result)
+                results_per_realization.extend(task.result)
+
+            self.write_hazardcurve_file(results_per_realization)
+            results.extend(results_per_realization)
+
+        del results_per_realization
+
+        # compute and serialize mean and quantile hazard curves
+        pending_tasks_mean = []
+        results_mean = []
+        pending_tasks_quantile = []
+        results_quantile = []
+
+        LOG.info('Computing mean and quantile hazard curves')
+        for site_list in self.site_list_generator():
+
+            pending_tasks_quantile.append(tasks.compute_quantile_curves.delay(
+                self.id, site_list))
+            if self.params['COMPUTE_MEAN_HAZARD_CURVE'].lower() == 'true':
+                pending_tasks_mean.append(tasks.compute_mean_curves.delay(
+                    self.id, site_list))
+
+        for task in pending_tasks_mean:
+            task.wait()
+            if task.status != 'SUCCESS': 
+                raise Exception(task.result)
+            results_mean.extend(task.result)
+
+        for task in pending_tasks_quantile:
+            task.wait()
+            if task.status != 'SUCCESS': 
+                raise Exception(task.result)
+            results_quantile.extend(task.result)
+
+        if self.params['COMPUTE_MEAN_HAZARD_CURVE'].lower() == 'true':
+            LOG.info('Serializing mean hazard curves')
+            self.write_hazardcurve_file(results_mean)
+            del results_mean
+
+        # collect hazard curve keys per quantile value
+        quantile_values = {}
+        while len(results_quantile) > 0:
+            quantile_key = results_quantile.pop()
+            curr_qv = tokens.quantile_value_from_hazard_curve_key(
+                quantile_key)
+            if curr_qv not in quantile_values:
+                quantile_values[curr_qv] = []
+            quantile_values[curr_qv].append(quantile_key)
+                
+        LOG.info('Serializing quantile hazard curves for %s quantile values' \
+            % len(quantile_values))
+        for key_list in quantile_values.values():
+            self.write_hazardcurve_file(key_list)
+
+        return results
+
+    def write_hazardcurve_file(self, curve_keys):
+        """Generate a NRML file with hazard curves for a collection of 
+        hazard curves from KVS, identified through their KVS keys.
+        
+        curve_keys is a list of KVS keys of the hazard curves to be
+        serialized.
+
+        The hazard curve file can be written
+        (1) for a set of hazard curves belonging to the same realization
+            (= endBranchLabel) and a set of sites.
+        (2) for a mean hazard curve at a set of sites
+        (3) for a quantile hazard curve at a set of sites
+
+        Mixing of these three cases is not allowed, i.e., all hazard curves
+        from the set of curve_keys have to be either for the same realization,
+        mean, or quantile.
+        """
+
+        # LOG.debug("KEYS (%s): %s" % (len(curve_keys), curve_keys))
+
+        if _is_mean_hazard_curve_key(curve_keys[0]):
+            hc_attrib_update = {'statistics': 'mean'}
+            filename_part = 'mean'
+            curve_mode = 'mean'
+
+        elif _is_quantile_hazard_curve_key(curve_keys[0]):
+
+            # get quantile value from KVS key
+            quantile_value = tokens.quantile_value_from_hazard_curve_key(
+                curve_keys[0])
+            hc_attrib_update = {'statistics': 'quantile',
+                                'quantileValue': quantile_value}
+            filename_part = "quantile-%.2f" % quantile_value
+            curve_mode = 'quantile'
+
+        elif _is_realization_hazard_curve_key(curve_keys[0]):
+            realization_reference_str = \
+                tokens.realization_value_from_hazard_curve_key(curve_keys[0])
+            hc_attrib_update = {'endBranchLabel': realization_reference_str}
+            filename_part = realization_reference_str 
+            curve_mode = 'realization'
+
+        else:
+            error_msg = "no valid hazard curve type found in KVS key"
+            raise RuntimeError(error_msg)
+
+        nrml_file = "hazardcurve-%s.xml" % filename_part
+
+        nrml_path = os.path.join(self['BASE_PATH'], self['OUTPUT_DIR'], 
+            nrml_file)
+        iml_list = [float(param) 
+                    for param
+                    in self.params['INTENSITY_MEASURE_LEVELS'].split(",")]
+        
+        LOG.debug("Generating NRML hazard curve file for mode %s, "\
+            "%s hazard curves: %s" % (curve_mode, len(curve_keys), nrml_file))
+        LOG.debug("IML: %s" % iml_list)
+
+        xmlwriter = hazard_output.HazardCurveXMLWriter(nrml_path)
+        hc_data = []
+        
+        for hc_key in curve_keys:
+
+            if curve_mode == 'mean' and not _is_mean_hazard_curve_key(hc_key):
+                error_msg = "non-mean hazard curve key found in mean mode"
+                raise RuntimeError(error_msg)
+            
+            elif curve_mode == 'quantile':
+                if not _is_quantile_hazard_curve_key(hc_key):
+                    error_msg = "non-quantile hazard curve key found in "\
+                                "quantile mode"
+                    raise RuntimeError(error_msg)
+
+                elif tokens.quantile_value_from_hazard_curve_key(hc_key) != \
+                    quantile_value:
+                    error_msg = "quantile value must be the same for all "\
+                                "hazard curves in an instance file"
+                    raise ValueError(error_msg)
+
+            elif curve_mode == 'realization':
+                if not _is_realization_hazard_curve_key(hc_key):
+                    error_msg = "non-realization hazard curve key found in "\
+                                "realization mode"
+                    raise RuntimeError(error_msg)
+                elif tokens.realization_value_from_hazard_curve_key(
+                    hc_key) != realization_reference_str:
+                    error_msg = "realization value must be the same for all "\
+                                "hazard curves in an instance file"
+                    raise ValueError(error_msg)
+
+            hc = kvs.get_value_json_decoded(hc_key)
+            #LOG.debug("JSON HC: %s" % hc)
+            
+            site_obj = shapes.Site(float(hc['site_lon']), 
+                                   float(hc['site_lat']))
+
+            # extract hazard curve ordinate (PoE) from KVS
+            # NOTE(fab): At the moment, the IMLs are stored along with the 
+            # PoEs in KVS. However, we are using the IML list from config.
+            # The IMLs from KVS are ignored. Note that IMLs from KVS are
+            # in logarithmic form, but the ones from config are not.
+            # The way of storing the HC data in KVS is not very
+            # efficient, we should store the abscissae and ordinates
+            # separately as lists and not make pairs of them
+            curve_poe = []
+            for curve_pair in hc['curve']:
+                curve_poe.append(float(curve_pair['y']))
+
+            hc_attrib = {'investigationTimeSpan': 
+                            self.params['INVESTIGATION_TIME'],
+                         'IML': iml_list,
+                         'IMT': self.params['INTENSITY_MEASURE_TYPE'],
+                         'poE': curve_poe}
+
+            hc_attrib.update(hc_attrib_update)
+            hc_data.append((site_obj, hc_attrib))
+
+        xmlwriter.serialize(hc_data)
+        return nrml_path
+
+    @preload
+    def compute_hazard_curve(self, site_list, realization):
+        """ Compute hazard curves, write them to KVS as JSON,
+        and return a list of the KVS keys for each curve. """
+        jsite_list = self.parameterize_sites(site_list) 
+        hazard_curves = java.jclass("HazardCalculator").getHazardCurvesAsJson(
+            jsite_list,
+            self.generate_erf(),
+            self.generate_gmpe_map(),
+            self.get_iml_list(),
+            float(self.params['MAXIMUM_DISTANCE']))
+        
+        # write the curves to the KVS and return a list of the keys
+        kvs_client = kvs.get_client()
+        curve_keys = []
+        for i in xrange(0, len(hazard_curves)):
+            curve = hazard_curves[i]
+            site = site_list[i]
+            lon = site.longitude
+            lat = site.latitude 
+            curve_key = kvs.tokens.hazard_curve_key(self.id,
+                                                    realization,
+                                                    lon,
+                                                    lat)
+            kvs_client.set(curve_key, curve)
+            curve_keys.append(curve_key)
+        return curve_keys
+
+
+class EventBasedMixin(BasePSHAMixin): # pylint: disable=W0232
+    """Probabilistic Event Based method for performing Hazard calculations.
+
+    Implements the JobMixin, which has a primary entry point of execute().
+    Execute is responsible for dispatching celery tasks.
+    
+    Note that this Mixin, during execution, will always be an instance of the
+    Job class, and thus has access to the self.params dict, full of config
+    params loaded from the Job configuration file."""
+    
+
+    @preload
+    def execute(self):
+        """Main hazard processing block.
+        
+        Loops through various random realizations, spawning tasks to compute
+        GMFs."""
+        results = []
+        
+        source_model_generator = random.Random()
+        source_model_generator.seed(
+                self.params.get('SOURCE_MODEL_LT_RANDOM_SEED', None))
+        
+        gmpe_generator = random.Random()
+        gmpe_generator.seed(self.params.get('GMPE_LT_RANDOM_SEED', None))
+        
+        gmf_generator = random.Random()
+        gmf_generator.seed(self.params.get('GMF_RANDOM_SEED', None))
+        
+        histories = int(self.params['NUMBER_OF_SEISMICITY_HISTORIES'])
+        realizations = int(self.params['NUMBER_OF_LOGIC_TREE_SAMPLES'])
+        LOG.info("Going to run hazard for %s histories of %s realizations each."
+                % (histories, realizations))
+
+        for i in range(0, histories):
+            pending_tasks = []
+            for j in range(0, realizations):
+                self.store_source_model(source_model_generator.getrandbits(32))
+                self.store_gmpe_map(gmpe_generator.getrandbits(32))
+                for site_list in self.site_list_generator():
+                    stochastic_set_id = "%s!%s" % (i, j)
+                    # pylint: disable=E1101
+                    pending_tasks.append(
+                        tasks.compute_ground_motion_fields.delay(
+                            self.id,
+                            site_list,
+                            stochastic_set_id, gmf_generator.getrandbits(32)))
+        
+            for task in pending_tasks:
+                task.wait()
+                if task.status != 'SUCCESS': 
+                    raise Exception(task.result)
+                    
+            # if self.params['OUTPUT_GMF_FILES']
+            for j in range(0, realizations):
+                stochastic_set_id = "%s!%s" % (i, j)
+                stochastic_set_key = kvs.generate_product_key(
+                    self.id, kvs.tokens.STOCHASTIC_SET_TOKEN, 
+                    stochastic_set_id)
+                print "Writing output for ses %s" % stochastic_set_key
+                ses = kvs.get_value_json_decoded(stochastic_set_key)
+                if ses:
+                    results.extend(self.write_gmf_files(ses))
+        return results
+    
+    def write_gmf_files(self, ses):
+        """Generate a GeoTiff file and a NRML file for each GMF."""
+        image_grid = self.region.grid
+        iml_list = [float(param) 
+                    for param
+                    in self.params['INTENSITY_MEASURE_LEVELS'].split(",")]
+
+        LOG.debug("Generating GMF image, grid is %s col by %s rows" % (
+                image_grid.columns, image_grid.rows))
+        LOG.debug("IML: %s" % (iml_list))
+        files = []
+        for event_set in ses:
+            for rupture in ses[event_set]:
+
+                # NOTE(fab): we have to explicitly convert the JSON-decoded 
+                # tokens from Unicode to string, otherwise the path will not
+                # be accepted by the GeoTiffFile constructor
+                common_path = os.path.join(self.base_path, self['OUTPUT_DIR'],
+                        "gmf-%s-%s" % (str(event_set.replace("!", "_")),
+                                       str(rupture.replace("!", "_"))))
+                tiff_path = "%s.tiff" % common_path
+                nrml_path = "%s.xml" % common_path
+                gwriter = geotiff.GMFGeoTiffFile(tiff_path, image_grid, 
+                    init_value=0.0, normalize=True, iml_list=iml_list,
+                    discrete=True)
+                xmlwriter = hazard_output.GMFXMLWriter(nrml_path)
+                gmf_data = {}
+                for site_key in ses[event_set][rupture]:
+                    site = ses[event_set][rupture][site_key]
+                    site_obj = shapes.Site(site['lon'], site['lat'])
+                    point = image_grid.point_at(site_obj)
+                    gwriter.write((point.row, point.column), 
+                        math.exp(float(site['mag'])))
+                    gmf_data[site_obj] = \
+                        {'groundMotion': math.exp(float(site['mag']))}
+
+                gwriter.close()
+                xmlwriter.serialize(gmf_data)
+                files.append(tiff_path)
+                files.append(gwriter.html_path)
+                files.append(nrml_path)
+        return files
+        
+ 
+    @preload
+    def compute_ground_motion_fields(self, site_list, stochastic_set_id, seed):
+        """Ground motion field calculation, runs on the workers."""
+        jpype = java.jvm()
+
+        jsite_list = self.parameterize_sites(site_list)
+        key = kvs.generate_product_key(
+            self.id, kvs.tokens.STOCHASTIC_SET_TOKEN, stochastic_set_id)
+        gmc = self.params['GROUND_MOTION_CORRELATION']
+        correlate = (gmc == "true" and True or False)
+        java.jclass("HazardCalculator").generateAndSaveGMFs(
+                self.cache, key, stochastic_set_id, jsite_list,
+                 self.generate_erf(), 
+                self.generate_gmpe_map(), 
+                java.jclass("Random")(seed), 
+                jpype.JBoolean(correlate))
+
+
+def gmf_id(history_idx, realization_idx, rupture_idx):
+    """ Return a GMF id suitable for use as a KVS key """
+    return "%s!%s!%s" % (history_idx, realization_idx, rupture_idx)
+
+def _is_realization_hazard_curve_key(kvs_key):
+    return (tokens.extract_product_type_from_kvs_key(kvs_key) == \
+                tokens.HAZARD_CURVE_KEY_TOKEN)
+
+def _is_mean_hazard_curve_key(kvs_key):
+    return (tokens.extract_product_type_from_kvs_key(kvs_key) == \
+                tokens.MEAN_HAZARD_CURVE_KEY_TOKEN)
+
+def _is_quantile_hazard_curve_key(kvs_key):
+    return (tokens.extract_product_type_from_kvs_key(kvs_key) == \
+                tokens.QUANTILE_HAZARD_CURVE_KEY_TOKEN)
+
+def hazard_curve_filename(filename_part):
+    return "%s-%s.xml" % (HAZARD_CURVE_FILENAME_PREFIX, filename_part)
+
+def realization_hc_filename(realization):
+    return hazard_curve_filename(realization)
+
+def mean_hc_filename():
+    return hazard_curve_filename('mean')
+
+def quantile_hc_filename(quantile_value):
+    filename_part = "quantile-%.2f" % quantile_value
+    return hazard_curve_filename(filename_part)
+
+job.HazJobMixin.register("Event Based", EventBasedMixin, order=0)
+job.HazJobMixin.register("Classical", ClassicalMixin, order=1)
--- /dev/null
+++ openquake-1.0/src/output/hazard.py
@@ -0,0 +1,252 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+This module provides classes that serialize hazard-related objects
+to NRML format.
+
+Hazard curves:
+
+For the serialization of hazard curves, it currently takes 
+all the lxml object model in memory
+due to the fact that curves can be grouped by IDmodel and
+IML. Couldn't find a way to do so writing an object at a
+time without making a restriction to the order on which
+objects are received.
+
+Ground Motion Fields (GMFs):
+
+GMFs are serialized per object (=Site) as implemented in the base class.
+"""
+
+from lxml import etree
+
+from openquake import logs
+from openquake import shapes
+from openquake import writer
+
+from openquake.xml import NSMAP, NRML, GML, NSMAP_OLD, GML_OLD
+
+LOGGER = logs.HAZARD_LOG
+
+NRML_GML_ID = 'n1'
+HAZARDRESULT_GML_ID = 'hr1'
+SRS_EPSG_4326 = 'epsg:4326'
+
+class HazardCurveXMLWriter(writer.FileWriter):
+    """This class writes an hazard curve into the nrml format."""
+
+    def __init__(self, path):
+        super(HazardCurveXMLWriter, self).__init__(path)
+
+        self.nrml_el = None
+        self.result_el = None
+        self.curves_per_branch_label = {}
+        self.hcnode_counter = 0
+        self.hcfield_counter = 0
+
+    def close(self):
+        """Overrides the default implementation writing all the
+        collected lxml object model to the stream."""
+
+        if self.nrml_el is None:
+            error_msg = "You need to add at least a curve to build " \
+                        "a valid output!"
+            raise RuntimeError(error_msg)
+
+        self.file.write(etree.tostring(self.nrml_el, pretty_print=True,
+            xml_declaration=True, encoding="UTF-8"))
+                
+        super(HazardCurveXMLWriter, self).close()
+            
+    def write(self, point, values):
+        """Writes an hazard curve.
+        
+        point must be of type shapes.Site
+        values is a dictionary that matches the one produced by the
+        parser nrml.NrmlFile
+        """
+        
+        # if we are writing the first hazard curve, create wrapping elements
+        if self.nrml_el is None:
+            
+            # nrml:nrml, needs gml:id
+            self.nrml_el = etree.Element("%snrml" % NRML, nsmap=NSMAP)
+
+            if 'nrml_id' in values:
+                _set_gml_id(self.nrml_el, str(values['nrml_id']))
+            else:
+                _set_gml_id(self.nrml_el, NRML_GML_ID)
+            
+            # nrml:hazardResult, needs gml:id
+            self.result_el = etree.SubElement(self.nrml_el, 
+                "%shazardResult" % NRML)
+            if 'hazres_id' in values:
+                _set_gml_id(self.result_el, str(values['hazres_id']))
+            else:
+                _set_gml_id(self.result_el, HAZARDRESULT_GML_ID)
+
+            # nrml:config
+            config_el = etree.SubElement(self.result_el, "%sconfig" % NRML)
+            
+            # nrml:hazardProcessing
+            hazard_processing_el = etree.SubElement(config_el, 
+                "%shazardProcessing" % NRML)
+            
+            # the following XML attributes are all optional
+            _set_optional_attributes(hazard_processing_el, values,
+                ('investigationTimeSpan', 'IDmodel', 'saPeriod', 'saDamping'))
+
+        # check if we have hazard curves for an end branch label, or
+        # for mean/median/quantile
+        if 'endBranchLabel' in values and 'statistics' in values:
+            error_msg = "hazardCurveField cannot have both an end branch " \
+                        "and a statistics label"
+            raise ValueError(error_msg)
+        elif 'endBranchLabel' in values:
+            curve_label = values['endBranchLabel']
+        elif 'statistics' in values:
+            curve_label = values['statistics']
+        else:
+            error_msg = "hazardCurveField has to have either an end branch " \
+                        "or a statistics label"
+            raise ValueError(error_msg)
+        
+        try:
+            hazard_curve_field_el = self.curves_per_branch_label[curve_label]
+        except KeyError:
+            
+            # nrml:hazardCurveField, needs gml:id
+            hazard_curve_field_el = etree.SubElement(self.result_el, 
+                "%shazardCurveField" % NRML)
+
+            if 'hcfield_id' in values:
+                _set_gml_id(hazard_curve_field_el, str(values['hcfield_id']))
+            else:
+                _set_gml_id(hazard_curve_field_el, 
+                    "hcf_%s" % self.hcfield_counter)
+                self.hcfield_counter += 1
+
+            if 'endBranchLabel' in values:
+                hazard_curve_field_el.set("endBranchLabel", 
+                    str(values["endBranchLabel"]))
+            elif 'statistics' in values:
+                hazard_curve_field_el.set("statistics", 
+                    str(values["statistics"]))
+                if 'quantileValue' in values:
+                    hazard_curve_field_el.set("quantileValue", 
+                        str(values["quantileValue"]))
+
+            # nrml:IML
+            iml_el = etree.SubElement(hazard_curve_field_el, "%sIML" % NRML)
+            iml_el.text = " ".join([str(x) for x in values["IML"]])
+            iml_el.set("IMT", str(values["IMT"]))
+
+            self.curves_per_branch_label[curve_label] = hazard_curve_field_el
+        
+        # nrml:HCNode, needs gml:id
+        hcnode_el = etree.SubElement(hazard_curve_field_el, "%sHCNode" % NRML)
+
+        if 'hcnode_id' in values:
+            _set_gml_id(hcnode_el, str(values['hcnode_id']))
+        else:
+            _set_gml_id(hcnode_el, "hcn_%s" % self.hcnode_counter)
+            self.hcnode_counter += 1
+
+        # nrml:site
+        site_el = etree.SubElement(hcnode_el, "%ssite" % NRML)
+        point_el = etree.SubElement(site_el, "%sPoint" % GML)
+        point_el.set("srsName", SRS_EPSG_4326)
+
+        pos_el = etree.SubElement(point_el, "%spos" % GML)
+        pos_el.text = "%s %s" % (point.longitude, point.latitude)
+
+        # nrml:hazardCurve
+        hc_el = etree.SubElement(hcnode_el, "%shazardCurve" % NRML)
+        poe_el = etree.SubElement(hc_el, "%spoE" % NRML)
+        poe_el.text = " ".join([str(x) for x in values["poE"]])
+
+
+class GMFXMLWriter(writer.FileWriter):
+    """This class serializes ground motion field (GMF) informatiuon
+    to NRML format.
+
+    As of now, only the field information is supported. GMPEParameters is
+    serialized as a stub with the only attribute that is formally required
+    (but doesn't have a useful definition in the schema).
+    Rupture information and full GMPEParameters are currently 
+    not supported."""
+
+    root_tag = NRML + "HazardResult"
+    config_tag = NRML + "Config"
+    gmpe_params_tag = NRML + "GMPEParameters"
+    container_tag = NRML + "GroundMotionFieldSet"
+    field_tag = NRML + "field"
+    site_tag = NRML + "site"
+    pos_tag = GML_OLD + "pos"
+    ground_motion_attr = "groundMotion"
+
+    def write(self, point, val):
+        """Writes GMF for one site.
+
+        point must be of type shapes.Site
+        val is a dictionary:
+
+        {'groundMotion': 0.8}
+        """
+        if isinstance(point, shapes.GridPoint):
+            point = point.site.point
+        if isinstance(point, shapes.Site):
+            point = point.point
+        self._append_site_node(point, val, self.parent_node)
+
+    def write_header(self):
+        """Write out the file header"""
+
+        # TODO(fab): support rupture element (not implemented so far)
+        # TODO(fab): support full GMPEParameters (not implemented so far)
+
+        self.root_node = etree.Element(self.root_tag, nsmap=NSMAP_OLD)
+        config_node = etree.SubElement(self.root_node, self.config_tag, 
+                                       nsmap=NSMAP_OLD)
+        config_node.text = "Config file details go here."
+
+        container_node = etree.SubElement(self.root_node, 
+                                          self.container_tag, nsmap=NSMAP_OLD)
+
+        gmpe_params_node = etree.SubElement(container_node, 
+                                            self.gmpe_params_tag, 
+                                            nsmap=NSMAP_OLD)
+
+        # field element
+        self.parent_node = etree.SubElement(container_node, self.field_tag, 
+                                            nsmap=NSMAP_OLD)
+
+    def write_footer(self):
+        """Write out the file footer"""
+        et = etree.ElementTree(self.root_node)
+        et.write(self.file, pretty_print=True, xml_declaration=True,
+                 encoding="UTF-8")
+    
+    def _append_site_node(self, point, val, parent_node):
+        """Write outer and inner 'site' elements. Outer 'site' elements have
+        attribute 'groundMotion', inner 'site' elements have child element
+        <gml:pos> with lon/lat coordinates."""
+        outer_site_node = etree.SubElement(parent_node, self.site_tag, 
+                                           nsmap=NSMAP_OLD)
+        outer_site_node.attrib[self.ground_motion_attr] = str(
+            val[self.ground_motion_attr])
+
+        inner_site_node = etree.SubElement(outer_site_node, self.site_tag,
+                                           nsmap=NSMAP_OLD)
+        pos_node = etree.SubElement(inner_site_node, self.pos_tag, 
+                                    nsmap=NSMAP_OLD)
+        pos_node.text = "%s %s" % (str(point.x), str(point.y))
+
+
+def _set_optional_attributes(element, value_dict, attr_keys):
+    for curr_key in attr_keys:
+        if curr_key in value_dict:
+            element.set(curr_key, str(value_dict[curr_key]))
+
+def _set_gml_id(element, gml_id):
+    element.set("%sid" % GML, str(gml_id))
--- /dev/null
+++ openquake-1.0/src/output/kml.py
@@ -0,0 +1,75 @@
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+
+""" KML output class
+
+We're cheating a bit with the xml so that we can write stuff as we get it
+rather than generating a giant dom and then writing it to file.
+
+"""
+
+from lxml import etree
+from lxml.builder import E
+
+from openquake import writer
+
+KML_HEADER = """
+<?xml version="1.0" encoding="UTF-8"?>
+<kml xmlns="http://www.opengis.net/kml/2.2">
+  <Document>
+    <name>Paths</name>
+    <description>Examples of paths. Note that the tessellate tag is by default
+      set to 0. If you want to create tessellated lines, they must be authored
+      (or edited) directly in KML.</description>
+    <Style id="yellowLineGreenPoly">
+      <LineStyle>
+        <color>7f00ffff</color>
+        <width>4</width>
+      </LineStyle>
+      <PolyStyle>
+        <color>7f00ff00</color>
+      </PolyStyle>
+    </Style>
+"""
+
+KML_FOOTER = """
+  </Document>
+</kml>
+"""
+
+
+class KmlFile(writer.FileWriter):
+    """Example output class.
+
+    Were this a real class it would probably be doing something much more
+    interesting.
+
+    """
+    def __init__(self, *args, **kw):
+        super(KmlFile, self).__init__(*args, **kw)
+        self.file.write(KML_HEADER.strip())
+
+    def write(self, cell, value):
+        # cell to kml linestring
+        linestring = []
+        for x, y in cell.coords:
+            linestring.append('%f,%f,2357' % (x, y))
+
+        placemark = (E.Placemark(
+                        E.name('foo'),
+                        E.description('bar'),
+                        E.styleUrl('#yellowLineGreenpoly'),
+                        E.LineString(
+                            E.extrude('1'),
+                            E.tesselate('1'),
+                            E.altitudeMode('absolute'),
+                            E.coordinates('\n'.join(linestring))
+                            )
+                        )
+                     )
+
+        self.file.write(etree.tostring(placemark, pretty_print=True))
+
+    def close(self):
+        self.file.write(KML_FOOTER.strip())
+        super(KmlFile, self).close()
+
--- /dev/null
+++ openquake-1.0/src/output/risk.py
@@ -0,0 +1,103 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+Output risk data (loss ratio curves, loss curves, and loss values)
+as nrml-style XML.
+
+"""
+
+from lxml import etree
+
+from openquake import logs
+from openquake import shapes
+from openquake import writer
+from openquake.xml import GML_OLD, NSMAP_OLD, NRML
+
+LOGGER = logs.RISK_LOG
+
+class RiskXMLWriter(writer.FileWriter):
+    """This class writes a risk curve into the nrml format."""
+    curve_tag = NRML + "Curve"
+    abcissa_tag = NRML + "PE"
+    container_tag = NRML + "RiskElementList"
+    
+    def write(self, point, val):
+        if isinstance(point, shapes.GridPoint):
+            point = point.site.point
+        if isinstance(point, shapes.Site):
+            point = point.point
+        self._append_curve_node(point, val, self.parent_node)
+
+    def write_header(self):
+        """Write out the file header"""
+        self.root_node = etree.Element(NRML + "RiskResult", nsmap=NSMAP_OLD)
+        config_node = etree.SubElement(self.root_node, 
+                           NRML + "Config" , nsmap=NSMAP_OLD)
+        config_node.text = "Config file details go here."
+
+        #pylint: disable=W0201
+        self.parent_node = etree.SubElement(self.root_node, 
+                           self.container_tag , nsmap=NSMAP_OLD)
+
+    def write_footer(self):
+        """Write out the file footer"""
+        et = etree.ElementTree(self.root_node)
+        et.write(self.file, pretty_print=True)
+    
+    def _append_curve_node(self, point, val, parent_node):
+        """ This method appends a curve node to the parent node """
+
+        (curve_object, asset_object) = val
+        node = etree.SubElement(parent_node, self.curve_tag, nsmap=NSMAP_OLD)
+        node.attrib['AssetID'] = asset_object['AssetID']    
+        pos = etree.SubElement(node, GML_OLD + "pos", nsmap=NSMAP_OLD)
+        pos.text = "%s %s" % (str(point.x), str(point.y))
+        
+        pe_values = _curve_pe_as_gmldoublelist(curve_object)
+        
+        # This use of not None is b/c of the trap w/ ElementTree find
+        # for nodes that have no child nodes.
+        subnode_pe = self.parent_node.find(
+            NRML + "Common/" + self.abcissa_tag)
+        if subnode_pe is not None:
+            if subnode_pe.find(NRML + "Values").text != pe_values:
+                LOGGER.error("Abcissa doesn't match between \n %s \n %s"
+                    % (subnode_pe.find(NRML + "Values").text, pe_values))
+                raise Exception("Curves must share the same Abcissa!")
+        else:
+            common_node = self.parent_node.find(NRML + "Common")
+            if common_node is None:
+                common_node = etree.Element(NRML + "Common", nsmap=NSMAP_OLD)
+                parent_node.insert(0, common_node)  
+            subnode_pe = etree.SubElement(common_node, 
+                            self.abcissa_tag, nsmap=NSMAP_OLD)
+            etree.SubElement(subnode_pe, 
+                    NRML + "Values", nsmap=NSMAP_OLD).text = pe_values
+
+        LOGGER.debug("Writing xml, object is %s", curve_object)
+        subnode_loss = etree.SubElement(
+            node, NRML + "Values", nsmap=NSMAP_OLD)
+        subnode_loss.text = _curve_vals_as_gmldoublelist(curve_object)
+
+
+class LossCurveXMLWriter(RiskXMLWriter):
+    """Simple serialization of loss curves and loss ratio curves"""
+    curve_tag = NRML + "LossCurve"
+    abcissa_tag = NRML + "LossCurvePE"
+    container_tag = NRML + "LossCurveList"
+    
+
+class LossRatioCurveXMLWriter(RiskXMLWriter):
+    """Simple serialization of loss curves and loss ratio curves"""
+    curve_tag = NRML + "LossRatioCurve"
+    abcissa_tag = NRML + "LossRatioCurvePE"
+    container_tag = NRML + "LossRatioCurveList"
+
+
+def _curve_pe_as_gmldoublelist(curve_object):
+    """ Return the list of abscissae converted to string joined by a space """
+    return " ".join([str(abscissa) for abscissa in curve_object.abscissae])
+
+def _curve_vals_as_gmldoublelist(curve_object):
+    """ Return the list of ordinates converted to string joined by a space """
+    return " ".join([str(ordinate) for ordinate in curve_object.ordinates])
--- /dev/null
+++ openquake-1.0/src/output/curve.py
@@ -0,0 +1,288 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+
+"""
+This module plots curves (hazard/loss/loss ratio) as read from an NRML file.
+As plotting engine, matplotlib is used. The plots are in SVG format.
+"""
+
+import geohash
+import matplotlib
+matplotlib.use('SVG')
+import pylab
+
+from matplotlib.font_manager import FontProperties
+
+from openquake import writer
+from openquake.parser import hazard as hazard_parser
+from openquake.parser import risk as risk_parser
+
+IMAGE_FORMAT = "SVG"
+COLOR_CODES = ('k', 'r', 'g', 'b')
+
+CURVE_BRANCH_PLACEHOLDER = 'Curve'
+
+
+class CurvePlotter(object):
+    """Base class for plotting curves from NRML files to SVG.
+    For a specific curve plot (hazard/loss/loss ratio), 
+    a class has to be derived from this one."""
+
+    def __init__(self, output_base_path, nrml_input_path, curve_title=None):
+
+        # remove trailing .svg extension from output file name (if existing)
+        self.output_base_path = output_base_path
+        if self.output_base_path.endswith(('.svg', '.SVG')):
+            self.output_base_path = self.output_base_path[0:-4]
+
+        self.nrml_input_path = nrml_input_path
+        self.curve_title = curve_title
+        self.data = {}
+        self.svg_filenames = []
+
+        self._parse_nrml_file()
+
+    def _parse_nrml_file(self):
+        """Parse curve data from NRML file into a dictionary, has to
+        be implemented by the derived class."""
+        pass
+
+    def plot(self, autoscale_y=True):
+        """Create a plot for each site in the dataset.
+        The argument autoscale_y auto-scales the ordinate axis (default).
+        If set to false, the displayed ordinate range is 0...1.
+        TODO(fab): let user specify abscissa and ordinate range for plot."""
+
+        for site_data in self.data.values():
+            plot = CurvePlot(site_data['path'])
+            plot.write(site_data['curves'], autoscale_y)
+            plot.close()
+
+    def filenames(self):
+        """ Generator yields the path value for each dict in self.data """
+        for site_data in self.data.values():
+            yield site_data['path']
+
+    def _generate_filename(self, site_hash):
+        """ Return a file name string """
+        site_lat, site_lon = geohash.decode(site_hash)
+        return "%s_%7.3f_%6.3f.svg" % (
+            self.output_base_path, site_lon, site_lat)
+
+
+class RiskCurvePlotter(CurvePlotter):
+    """This class plots loss/loss ratio curves as read from an NRML file. For
+    each Site listed in the NRML file, a separate plot is created. A plot
+    contains only one curve, multiple curves per plot are currently not
+    supported by the NRML schema.
+    TODO(fab): In the future, we may want to plot loss/loss ratio curves
+    per asset, multiple assets per site (according to Vitor)."""
+
+    def __init__(self, output_base_path, nrml_input_path, mode='loss', 
+                 curve_title=None):
+        """mode can be 'loss' or 'loss_ratio', 'loss' is the default."""
+        self.mode = mode
+
+        if curve_title is None:
+            if self.mode == 'loss_ratio':
+                curve_title = 'Loss Ratio Curve'
+            else:
+                curve_title = 'Loss Curve'
+
+        super(RiskCurvePlotter, self).__init__(output_base_path, 
+            nrml_input_path, curve_title)
+
+    def _parse_nrml_file(self):
+        """Parse loss/loss ratio curve data from NRML file into a dictionary."""
+
+        nrml_element = risk_parser.NrmlFile(self.nrml_input_path,
+            mode=self.mode)
+
+        # loss/loss ratio curves have a common *ordinate* for all curves
+        # in an NRML file
+        for (nrml_point, nrml_attr) in nrml_element.filter(
+            region_constraint=None):
+
+            site_hash = nrml_point.hash()
+            curve_id = nrml_attr['AssetID']
+
+            if site_hash not in self.data:
+                self.data[site_hash] = {
+                    # capture SVG filename for each site
+                    'path': self._generate_filename(site_hash),
+                    'curves': {}}
+
+            if curve_id not in self.data[site_hash]['curves']:
+                self.data[site_hash]['curves'][curve_id] = {}
+
+            self.data[site_hash]['curves'][curve_id] = {
+                'abscissa': nrml_attr[nrml_element.abscissa_output_key],
+                'abscissa_property': nrml_element.abscissa_property,
+                'ordinate': nrml_attr[nrml_element.ordinate_output_key],
+                'ordinate_property': nrml_element.ordinate_property,
+                'Site': nrml_point,
+                'curve_title': self.curve_title}
+
+
+class HazardCurvePlotter(CurvePlotter):
+    """This class plots hazard curves as read from an NRML file. For
+    each Site listed in the NRML file, a separate plot is created. A plot
+    can contain multiple hazard curves, one for each logic tree end branch 
+    given in the NRML file."""
+
+    def __init__(self, output_base_path, nrml_input_path, curve_title=None):
+
+        if curve_title is None:
+            curve_title = 'Hazard Curves'
+
+        super(HazardCurvePlotter, self).__init__(output_base_path, 
+            nrml_input_path, curve_title)
+
+    def _parse_nrml_file(self):
+        """Parse hazard curve data from NRML file into a dictionary."""
+
+        nrml_element = hazard_parser.NrmlFile(self.nrml_input_path)
+
+        # we collect hazard curves for one site into one plot
+        # one plot contains hazard curves for several end branches 
+        # of the logic tree
+        # each end branch can have its own abscissa value set
+        for (nrml_point, nrml_attr) in nrml_element.filter(
+            region_constraint=None):
+
+            site_hash = nrml_point.hash()
+            ebl = nrml_attr['endBranchLabel']
+
+            if site_hash not in self.data:
+                self.data[site_hash] = {
+                    # capture SVG filename for each site
+                    'path': self._generate_filename(site_hash),
+                    'curves': {}}
+
+            if ebl not in self.data[site_hash]['curves']:
+                self.data[site_hash]['curves'][ebl] = {}
+
+            self.data[site_hash]['curves'][ebl] = {
+                'abscissa': nrml_attr['IMLValues'], 
+                'abscissa_property': nrml_attr['IMT'],
+                'ordinate': nrml_attr['Values'],
+                'ordinate_property': 'Probability of Exceedance',
+                'Site': nrml_point,
+                'curve_title': self.curve_title}
+
+
+class CurvePlot(writer.FileWriter):
+    """Creates an SVG  plot containing curve data for one site. 
+    At the moment, the class is used for hazard, loss, and loss ratio curves.
+    One plot can contain several curves. In the case of hazard curves, this
+    would be one curve for each end branch of a logic tree. For loss/loss
+    ratio curves, the multiple curve feature is not applicable."""
+
+    image_format = IMAGE_FORMAT
+
+    _plotFig = {'figsize': (10, 10)}
+
+    _plotCurve = {'markersize': None,
+                  'color': 'r',
+                  'colors': ('r', 'b'),
+                  'linestyle': ('steps', '-', '-'),
+                  'linewidth': 1}
+
+    _plotLabels = {'ylabel_size': None,
+                   'ylabel_rotation': None}
+
+    _plotLabelsFont = {'fontname': 'serif',       # sans
+                       'fontweight': 'normal',    # bold
+                       'fontsize':  13}
+
+    _plotAxes = {'ymin': 0.0,
+                 'ymax': 1.0,
+                 'xmin': 0.0,
+                 'xmax': None}
+
+    _plotLegend = {'style'        : 0,
+                   'borderpad'    : 1.0,
+                   'borderaxespad': 1.0,
+                   'markerscale'  : 5.0,
+                   'handletextpad': 0.5,
+                   'handlelength' : 2.0,
+                   'labelspacing' : 0.5}
+
+    _plotLegendFont = {'size'  : 'small',
+                       'style' : 'normal',
+                       'family': ('serif', 'sans-serif', 'monospace')}
+
+    def __init__(self, path):
+        self.color_code_generator = _color_code_generator()
+        super(CurvePlot, self).__init__(path)
+
+    def _init_file(self):
+
+        # set figure size
+        pylab.rcParams['figure.figsize'] = self._plotFig['figsize']
+
+        # init and clear figure
+        pylab.ax = pylab.subplot(111)
+        pylab.clf()
+
+    def write(self, data, autoscale_y=True):
+        """The method expects a dictionary that holds the labels for the
+        separate curves to be plotted as keys. For each key, the corresponding
+        value is a dictionary that holds lists for abscissa and ordinate values,
+        strings for abscissa and ordinate properties, and the title of the plot,
+        and the site as shapes.Site object.."""
+
+        for curve in data: 
+            #pylint: disable=E1101
+            pylab.plot(data[curve]['abscissa'], 
+                       data[curve]['ordinate'], 
+                       color=self.color_code_generator.next(),
+                       linestyle=self._plotCurve['linestyle'][2], 
+                       label=curve)
+
+        # set x and y dimension of plot
+        if autoscale_y is False:
+            pylab.ylim(self._plotAxes['ymin'], self._plotAxes['ymax'])
+
+        curve = data.keys()[0] # We apparently only need to get this once?
+        pylab.xlabel(data[curve]['abscissa_property'], self._plotLabelsFont)
+        pylab.ylabel(data[curve]['ordinate_property'], self._plotLabelsFont)
+
+        self._set_title(curve, data)
+
+        pylab.legend(loc=self._plotLegend['style'],
+                     markerscale=self._plotLegend['markerscale'],
+                     borderpad=self._plotLegend['borderpad'],
+                     borderaxespad=self._plotLegend['borderaxespad'],
+                     handletextpad=self._plotLegend['handletextpad'],
+                     handlelength=self._plotLegend['handlelength'],
+                     labelspacing=self._plotLegend['labelspacing'],
+                     prop=FontProperties(
+                        size=self._plotLegendFont['size'],
+                        style=self._plotLegendFont['style'],
+                        family=self._plotLegendFont['family'][1]))
+
+    def _set_title(self, curve, data):
+        """Set the title of this plot using the given site. Use just
+        the title in case there's not site related."""
+        try:
+            pylab.title("%s for (%7.3f, %6.3f)" % (
+                    data[curve]['curve_title'],
+                    data[curve]['Site'].longitude, 
+                    data[curve]['Site'].latitude))
+        except KeyError:
+            # no site related to this curve
+            pylab.title("%s" % data[curve]['curve_title'])
+
+    def close(self):
+        """Make sure the file is flushed, and send exit event."""
+        pylab.savefig(self.path)
+        pylab.close()
+
+def _color_code_generator():
+    """Generator that walks through a sequence of color codes for matplotlib.
+    When reaching the end of the color code list, start at the beginning again.
+    """
+    while(True): 
+        for code in COLOR_CODES: 
+            yield code
--- /dev/null
+++ openquake-1.0/src/output/geotiff.py
@@ -0,0 +1,365 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+A trivial implementation of the GeoTiff format,
+using GDAL.
+
+In order to make this run, you'll need GDAL installed,
+and on the Mac I couldn't get the brew recipe to work.
+I recommend the DMG framework at 
+http://www.kyngchaos.com/software:frameworks.
+
+I had to add the installed folders to 
+PYTHONPATH in my .bash_profile file to get them to load.
+"""
+
+import numpy
+import os
+
+from osgeo import osr, gdal
+from scipy.interpolate import interp1d
+
+
+from openquake import logs
+from openquake import writer
+
+from openquake.output import template
+
+LOG = logs.LOG
+
+GDAL_FORMAT = "GTiff"
+GDAL_PIXEL_DATA_TYPE = gdal.GDT_Float32
+SPATIAL_REFERENCE_SYSTEM = "WGS84"
+TIFF_BAND = 4
+TIFF_LONGITUDE_ROTATION = 0
+TIFF_LATITUDE_ROTATION = 0
+
+RGB_SEGMENTS, RGB_RED_BAND, RGB_GREEN_BAND, RGB_BLUE_BAND = range(0, 4)
+
+# these are some continuous colormaps, as found on
+# http://soliton.vm.bytemark.co.uk/pub/cpt-city/index.html
+COLORMAP = {'green-red': numpy.array( 
+    ((0.0, 1.0), (0, 255), (255, 0), (0, 0))),
+            'gmt-green-red': numpy.array( 
+    ((0.0, 1.0), (0, 128), (255, 0), (0, 0))),
+            'matlab-polar': numpy.array( 
+    ((0.0, 0.5, 1.0), (0, 255, 255), (0, 255, 0), (255, 255, 0))),
+            'gmt-seis': numpy.array( 
+  ((0.0, 0.1115, 0.2225, 0.3335, 0.4445, 0.5555, 0.6665, 0.7775, 0.8885, 1.0),
+     (170, 255, 255, 255, 255, 255, 90, 0, 0, 0), 
+     (0, 0, 85, 170, 255, 255, 255, 240, 80, 0), 
+     (0, 0, 0, 0, 0, 0, 30, 110, 255, 205)))
+            }
+
+COLORMAP_DEFAULT = 'green-red'
+
+SCALE_UP = 8
+
+class GeoTiffFile(writer.FileWriter):
+    """Rough implementation of the GeoTiff format,
+    based on http://adventuresindevelopment.blogspot.com/2008/12/
+                python-gdal-adding-geotiff-meta-data.html
+    """
+    
+    format = GDAL_FORMAT
+    normalize = True
+    
+    def __init__(self, path, image_grid, init_value=numpy.nan, normalize=False):
+        self.grid = image_grid
+        self.normalize = normalize
+        # NOTE(fab): GDAL initializes the image as columns x rows.
+        # numpy arrays, however, have usually rows as first axis,
+        # and columns as second axis (as it is the convention for
+        # matrices in maths)
+        
+        # initialize raster to init_value values (default in NaN)
+        self.raster = numpy.ones((self.grid.rows, self.grid.columns),
+                                 dtype=numpy.float) * init_value
+        self.alpha_raster = numpy.ones((self.grid.rows, self.grid.columns),
+                                 dtype=numpy.float) * 32.0
+        self.target = None
+        super(GeoTiffFile, self).__init__(path)
+        
+    def _init_file(self):
+        driver = gdal.GetDriverByName(self.format)
+
+        # NOTE(fab): use GDAL data type GDT_Float32 for science data
+        pixel_type = GDAL_PIXEL_DATA_TYPE
+        if self.normalize:
+            pixel_type = gdal.GDT_Byte
+        self.target = driver.Create(self.path, self.grid.columns, 
+            self.grid.rows, TIFF_BAND, pixel_type)
+        
+        corner = self.grid.region.upper_left_corner
+
+        # this is the order of arguments to SetGeoTransform()
+        # top left x, w-e pixel resolution, rotation, 
+        # top left y, rotation, n-s pixel resolution
+        # rotation is 0 if image is "north up" 
+        # taken from http://www.gdal.org/gdal_tutorial.html
+
+        # NOTE(fab): the last parameter (grid spacing in N-S direction) is 
+        # negative, because the reference point for the image is the 
+        # upper left (north-western) corner
+        self.target.SetGeoTransform(
+            [corner.longitude, self.grid.cell_size, TIFF_LONGITUDE_ROTATION, 
+             corner.latitude, TIFF_LATITUDE_ROTATION, -self.grid.cell_size])
+
+        # set the reference info 
+        srs = osr.SpatialReference()
+        srs.SetWellKnownGeogCS(SPATIAL_REFERENCE_SYSTEM)
+        self.target.SetProjection(srs.ExportToWkt())
+    
+    def write(self, cell, value):
+        """Stores the cell values in the NumPy array for later 
+        serialization. Make sure these are zero-based cell addresses."""
+        self.raster[int(cell[0]), int(cell[1])] = float(value)
+        # Set AlphaLayer
+        if value:
+            self.alpha_raster[int(cell[0]), int(cell[1])] = 255.0
+
+    def _normalize(self):
+        """ Normalize the raster matrix """
+        # NOTE(fab): numpy raster does not have to be transposed, although
+        # it has rows x columns
+        if self.normalize:
+            self.raster = self.raster * 254.0 / self.raster.max()
+
+    def close(self):
+        """Make sure the file is flushed, and send exit event"""
+        self._normalize()
+
+        self.target.GetRasterBand(1).WriteArray(self.raster)
+        self.target.GetRasterBand(2).Fill(0.0)
+        self.target.GetRasterBand(3).Fill(0.0)
+
+        # Write alpha channel
+        self.target.GetRasterBand(4).WriteArray(self.alpha_raster)
+
+        # Try to write the HTML wrapper
+        try:
+            self._write_html_wrapper()
+        except AttributeError:
+            pass
+
+        self.target = None  # This is required to flush the file
+
+    def _write_html_wrapper(self):
+        """write an html wrapper that embeds the geotiff."""
+        pass
+    
+    def serialize(self, iterable):
+        # TODO(JMC): Normalize the values
+        maxval = max(iterable.values())
+        for key, val in iterable.items():
+            if self.normalize:
+                val = val/maxval*254
+            self.write((key.column, key.row), val)
+        self.close()
+
+
+class LossMapGeoTiffFile(GeoTiffFile):
+    """ Write RGBA geotiff images for loss maps. Color scale is from
+    0(0x00)-100(0xff). In addition, we write out an HTML wrapper around
+    the TIFF with a color-scale legend."""
+
+    def write(self, cell, value):
+        """Stores the cell values in the NumPy array for later 
+        serialization. Make sure these are zero-based cell addresses."""
+        self.raster[int(cell[0]), int(cell[1])] = float(value)
+
+        # Set AlphaLayer
+        if value:
+            # 0x10 less than full opacity
+            self.alpha_raster[int(cell[0]), int(cell[1])] = float(0xfa)
+
+    def _normalize(self):
+        """ Normalize the raster matrix """
+        if self.normalize:
+            # This gives us a color scale of 0 to 100 with a 16 step.
+            self.raster = numpy.abs((255 * self.raster) / 100.0)
+            modulo = self.raster % 0x10
+            self.raster -= modulo
+
+    def _write_html_wrapper(self):
+        """write an html wrapper that <embed>s the geotiff."""
+
+        if self.path.endswith(('tiff', 'TIFF')):
+            html_path = ''.join((self.path[0:-4], 'html'))
+        else:
+            html_path = ''.join((self.path, '.html'))
+
+        # replace placeholders in HTML template with filename, height, width
+        html_string = template.generate_html(
+            os.path.basename(self.path),
+            width=str(self.target.RasterXSize * SCALE_UP),
+            height=str(self.target.RasterYSize * SCALE_UP),
+            imt='Loss Ratio/percent',
+            template=template.HTML_TEMPLATE_LOSSRATIO)
+
+        with open(html_path, 'w') as f:
+            f.write(html_string)
+
+
+class GMFGeoTiffFile(GeoTiffFile):
+    """Writes RGB GeoTIFF image for ground motion fields. Color scale is
+    from green (value 0.0) to red (value 2.0). In addition, writes an
+    HTML wrapper around the TIFF with a colorscale legend."""
+
+    CUT_LOWER = 0.0
+    CUT_UPPER = 2.0
+    COLOR_BUCKETS = 16 # yields 0.125 step size
+    
+    def __init__(self, path, image_grid, init_value=numpy.nan, 
+                 normalize=True, iml_list=None, discrete=True,
+                 colormap=None):
+        super(GMFGeoTiffFile, self).__init__(path, image_grid, init_value, 
+                                             normalize)
+
+        # NOTE(fab): for the moment, the image is always normalized
+        # and 4-band RGBA (argument normalize is disabled)
+        self.normalize = True
+        self.discrete = discrete
+        self.colormap = COLORMAP_DEFAULT
+
+        if colormap is not None:
+            self.colormap = colormap
+
+        if iml_list is None:
+            self.iml_list, self.iml_step = numpy.linspace(
+                self.CUT_LOWER, self.CUT_UPPER, num=self.COLOR_BUCKETS+1, 
+                retstep=True)
+            self.color_buckets = self.COLOR_BUCKETS
+        else:
+            self.iml_list = numpy.array(iml_list)
+            self.color_buckets = len(iml_list) - 1
+            self.iml_step = None
+
+        # list with pairs of RGB color hex codes and corresponding
+        # floats as values
+        self.colorscale_values = self._generate_colorscale()
+
+        # set image rasters
+        self.raster_r = numpy.zeros((self.grid.rows, self.grid.columns),
+                                    dtype=numpy.int)
+        self.raster_g = numpy.zeros_like(self.raster_r)
+        self.raster_b = numpy.zeros_like(self.raster_r)
+
+    def write(self, cell, value):
+        """This method is redefined, because the one from the base class
+        sets transparency to a high level for zero values. For GMF plots,
+        we want fully opaque images."""
+        self.raster[int(cell[0]), int(cell[1])] = float(value)
+
+    def _normalize(self):
+        """ Normalize the raster matrix """
+
+        # for discrete color scale, digitize raster values into 
+        # IML list values
+        if self.discrete is True:
+            index_raster = numpy.digitize(self.raster.flatten(), self.iml_list)
+
+            # fix out-of-bounds values (set to first/last bin)
+            # NOTE(fab): doing so, the upper end of the color scale is 
+            # never reached
+            numpy.putmask(index_raster, index_raster < 1, 1)
+            numpy.putmask(index_raster, index_raster > len(index_raster)-1,
+                len(index_raster)-1)
+            self.raster = numpy.reshape(self.iml_list[index_raster-1], 
+                                        self.raster.shape)
+
+        # condense desired target value range given in IML list to 
+        # interval 0..1 (because color map segments are given in this scale)
+        self.raster = self._condense_iml_range_to_unity(
+            self.raster, remove_outliers=True)
+
+        self.raster_r, self.raster_g, self.raster_b = _rgb_for(
+            self.raster, COLORMAP[self.colormap])
+
+    def close(self):
+        """Make sure the file is flushed, and send exit event"""
+        self._normalize()
+
+        self.target.GetRasterBand(1).WriteArray(self.raster_r)
+        self.target.GetRasterBand(2).WriteArray(self.raster_g)
+        self.target.GetRasterBand(3).WriteArray(self.raster_b)
+
+        # set alpha channel to fully opaque
+        self.target.GetRasterBand(4).Fill(255)
+
+        # write wrapper before closing file, so that raster dimensions are
+        # still accessible
+        self._write_html_wrapper()
+
+        self.target = None  # This is required to flush the file
+    
+    @property
+    def html_path(self):
+        """Path to the generated html file"""
+        if self.path.endswith(('tiff', 'TIFF')):
+            return ''.join((self.path[0:-4], 'html'))
+        else:
+            return ''.join((self.path, '.html'))       
+
+    def _write_html_wrapper(self):
+        """Write an html wrapper that embeds the geotiff in an <img> tag.
+        NOTE: this cannot be viewed out-of-the-box in all browsers."""
+
+        # replace placeholders in HTML template with filename, height, width
+        # TODO(fab): read IMT from config
+        html_string = template.generate_html(
+            os.path.basename(self.path), 
+            width=str(self.target.RasterXSize * SCALE_UP),
+            height=str(self.target.RasterYSize * SCALE_UP),
+            colorscale=self.colorscale_values,
+            imt='PGA/g')
+
+        with open(self.html_path, 'w') as f:
+            f.write(html_string)
+
+    def _condense_iml_range_to_unity(self, array, remove_outliers=False):
+        """Requires a one- or multi-dim. numpy array as argument."""
+        array = (array - self.iml_list[0]) / (
+            self.iml_list[-1] - self.iml_list[0])
+
+        if remove_outliers is True:
+            # cut values to 0.0-0.1 range (remove outliers)
+            numpy.putmask(array, array < 0.0, 0.0)
+            numpy.putmask(array, array > 1.0, 1.0)
+
+        return array
+
+    def _generate_colorscale(self):
+        """Generate a list of pairs of corresponding RGB hex values and
+        IML values for the colorscale in HTML output."""
+        colorscale = []
+        r, g, b = _rgb_for(self._condense_iml_range_to_unity(self.iml_list),
+                           COLORMAP[self.colormap])
+
+        for idx, iml_value in enumerate(self.iml_list):
+            colorscale.append(("#%02x%02x%02x" % (int(r[idx]), int(g[idx]), 
+                int(b[idx])), str(self.iml_list[idx])))
+
+        return colorscale
+
+def _rgb_for(fractional_values, colormap):
+    """Return a triple (r, g, b) of numpy arrays with R, G, and B 
+    color values between 0 and 255, respectively, for a given numpy array
+    fractional_values between 0 and 1. 
+    colormap is a 2-dim. numpy array with fractional values describing the 
+    color segments in the first row, and R, G, B corner values in the second,
+    third, and fourth row, respectively."""
+    return (_interpolate_color(fractional_values, colormap, RGB_RED_BAND),
+            _interpolate_color(fractional_values, colormap, RGB_GREEN_BAND),
+            _interpolate_color(fractional_values, colormap, RGB_BLUE_BAND))
+
+def _interpolate_color(fractional_values, colormap, rgb_band):
+    """Compute/create numpy array of rgb color value as interpolated
+    from color map. numpy array fractional_values is assumed to hold values
+    between 0 and 1. rgb_band can be 1,2,3 for red, green, and blue color
+    bands, respectively."""
+
+    color_interpolate = interp1d(colormap[RGB_SEGMENTS], colormap[rgb_band])
+    return numpy.reshape(color_interpolate(fractional_values.flatten()), 
+                         fractional_values.shape)
+
--- /dev/null
+++ openquake-1.0/src/output/__init__.py
@@ -0,0 +1,22 @@
+"""
+Constants and helper functions for the output generation.
+Includes simple serializers for test harnesses."""
+
+from openquake import writer
+
+class SimpleOutput(writer.FileWriter):
+    """Fake output class that writes to stdout."""
+    
+    def _init_file(self):
+        pass
+    
+    def close(self):
+        pass
+    
+    def write(self, cell, value):
+        print "%s : %s" % (cell, value)
+    
+    def serialize(self, someiterable):
+        """Dump all the values of a given iterable"""
+        for somekey, somevalue in someiterable.items():
+            print "%s : %s" % (somekey, somevalue)
\ No newline at end of file
--- /dev/null
+++ openquake-1.0/src/output/template.py
@@ -0,0 +1,145 @@
+# -*- coding: utf-8 -*-
+"""HTML template to embed geotiffs of GMF/Loss Ratio maps."""
+
+HTML_TEMPLATE_LOSSRATIO = """<!DOCTYPE html PUBLIC 
+    "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
+<html lang="en-US" xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
+<head>
+<title>Loss Ratio</title>
+</head>
+<body>
+<center>
+<img width="PLACEHOLDER_WIDTH" height="PLACEHOLDER_HEIGHT" 
+    src="PLACEHOLDER_IMAGE_NAME" type="image/tiff" negative="yes"/>
+<br/>
+<table cellspacing="0" border="1" width="20%">
+<tr>
+<th width="10%">Color</th>
+<th width="10%">Percentage</th>
+</tr>
+<tr>
+<td bgcolor="#000000">&nbsp;</td><td>0-6</td>
+</tr>
+<tr>
+<td bgcolor="#0f0000">&nbsp;</td><td>7-12</td>
+</tr>
+<tr>
+<td bgcolor="#1f0000">&nbsp;</td><td>13-18</td>
+</tr>
+<tr>
+<td bgcolor="#2f0000">&nbsp;</td><td>19-25</td>
+</tr>
+<tr>
+<td bgcolor="#3f0000">&nbsp;</td><td>26-31</td>
+</tr>
+<tr>
+<td bgcolor="#4f0000">&nbsp;</td><td>32-37</td>
+</tr>
+<tr>
+<td bgcolor="#5f0000">&nbsp;</td><td>38-43</td>
+</tr>
+<tr>
+<td bgcolor="#6f0000">&nbsp;</td><td>44-50</td>
+</tr>
+<tr>
+<td bgcolor="#7f0000">&nbsp;</td><td>51-56</td>
+</tr>
+<tr>
+<td bgcolor="#8f0000">&nbsp;</td><td>57-62</td>
+</tr>
+<tr>
+<td bgcolor="#9f0000">&nbsp;</td><td>63-69</td>
+</tr>
+<tr>
+<td bgcolor="#af0000">&nbsp;</td><td>70-75</td>
+</tr>
+<tr>
+<td bgcolor="#bf0000">&nbsp;</td><td>76-81</td>
+</tr>
+<tr>
+<td bgcolor="#cf0000">&nbsp;</td><td>82-87</td>
+</tr>
+<tr>
+<td bgcolor="#df0000">&nbsp;</td><td>88-94</td>
+</tr>
+<tr>
+<td bgcolor="#ef0000">&nbsp;</td><td>95-100</td>
+</tr>
+"""
+
+HTML_TEMPLATE_HEADER = """<!DOCTYPE html PUBLIC
+    "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
+<html lang="en-US" xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">
+<head>
+<title>PLACEHOLDER_TITLE</title>
+</head>
+<body>
+<center>
+"""
+
+HTML_TEMPLATE_IMAGE = """
+<img width="PLACEHOLDER_WIDTH" height="PLACEHOLDER_HEIGHT" 
+    src="PLACEHOLDER_IMAGE_NAME" type="image/tiff" negative="yes"/>
+"""
+
+HTML_TEMPLATE_TABLE_START = """
+<br/>
+<table cellspacing="0" border="1" width="20%">
+<tr>
+<th width="10%">Color</th>
+<th width="10%">PLACEHOLDER_IMT</th>
+</tr>
+"""
+
+HTML_TEMPLATE_FOOTER = """
+</table>
+<br/>
+</center>
+</body>
+</html>
+"""
+
+HTML_TEMPLATE_TABLE_ROW = """
+<tr>
+<td bgcolor="PLACEHOLDER_COLOR_HEX_CODE">&nbsp;</td>
+<td>PLACEHOLDER_IML_VALUE</td>
+</tr>
+"""
+
+def generate_html(path, width="", height="", colorscale=None, imt='PGA/g',
+                  title="", template=None):
+    """This function creates an HTML page with a Geotiff image linked, and a 
+    colorscale as HTML table. The HTML can be created from an explicitly 
+    given template, or automatically based on color scale values."""
+
+    if template is None:
+        curr_html = HTML_TEMPLATE_HEADER
+        header_html = curr_html.replace('PLACEHOLDER_TITLE', title)
+
+        curr_html = HTML_TEMPLATE_IMAGE
+        curr_html = curr_html.replace('PLACEHOLDER_IMAGE_NAME', path)
+        curr_html = curr_html.replace('PLACEHOLDER_WIDTH', width)
+        curr_html = curr_html.replace('PLACEHOLDER_HEIGHT', height)
+        image_html = curr_html
+
+        curr_html = HTML_TEMPLATE_TABLE_START 
+        table_start_html = curr_html.replace('PLACEHOLDER_IMT', imt)
+
+        table_body_html = '' 
+        for curr_color in colorscale:
+            curr_row_html = HTML_TEMPLATE_TABLE_ROW
+            curr_row_html = curr_row_html.replace('PLACEHOLDER_COLOR_HEX_CODE', 
+                                                curr_color[0])
+            curr_row_html = curr_row_html.replace('PLACEHOLDER_IML_VALUE', 
+                                                curr_color[1])
+            table_body_html += curr_row_html
+
+        return ''.join((header_html, image_html, table_start_html, 
+                        table_body_html, HTML_TEMPLATE_FOOTER))
+    else:
+        curr_html = template
+        for (token, new_value) in (
+            ('PLACEHOLDER_IMAGE_NAME', path), ('PLACEHOLDER_WIDTH', width),
+            ('PLACEHOLDER_HEIGHT', height)):
+            curr_html = curr_html.replace(token, new_value)
+        return curr_html
--- /dev/null
+++ openquake-1.0/src/kvs/tokens.py
@@ -0,0 +1,101 @@
+# -*- coding: utf-8 -*-
+""" Tokens for kvs keys """
+
+import openquake.kvs
+
+# hazard tokens
+SOURCE_MODEL_TOKEN = 'sources'
+GMPE_TOKEN = 'gmpe'
+JOB_TOKEN = 'job'
+ERF_KEY_TOKEN = 'erf'
+MGM_KEY_TOKEN = 'mgm'
+HAZARD_CURVE_KEY_TOKEN = 'hazard_curve'
+MEAN_HAZARD_CURVE_KEY_TOKEN = 'mean_hazard_curve'
+QUANTILE_HAZARD_CURVE_KEY_TOKEN = 'quantile_hazard_curve'
+STOCHASTIC_SET_TOKEN = 'ses'
+
+# risk tokens
+CONDITIONAL_LOSS_KEY_TOKEN = 'LOSS_AT_'
+EXPOSURE_KEY_TOKEN = 'ASSET'
+GMF_KEY_TOKEN = 'GMF'
+LOSS_RATIO_CURVE_KEY_TOKEN = 'LOSS_RATIO_CURVE'
+LOSS_CURVE_KEY_TOKEN = 'LOSS_CURVE'
+
+def loss_token(poe):
+    """ Return a loss token made up of the CONDITIONAL_LOSS_KEY_TOKEN and 
+    the poe cast to a string """
+    return "%s%s" % (CONDITIONAL_LOSS_KEY_TOKEN, str(poe))
+
+def vuln_key(job_id):
+    """Generate the key used to store vulnerability curves."""
+    return openquake.kvs.generate_product_key(job_id, "VULN_CURVES")
+
+def asset_key(job_id, row, col):
+    """ Return an asset key generated by openquake.kvs._generate_key """
+    return openquake.kvs.generate_key([job_id, row, col,
+        EXPOSURE_KEY_TOKEN])
+
+def loss_ratio_key(job_id, row, col, asset_id):
+    """ Return a loss ratio key generated by openquake.kvs.generate_key """
+    return openquake.kvs.generate_key([job_id, row, col,
+        LOSS_RATIO_CURVE_KEY_TOKEN, asset_id])
+
+def loss_curve_key(job_id, row, col, asset_id):
+    """ Return a loss curve key generated by openquake.kvs.generate_key """
+    return openquake.kvs.generate_key([job_id, row, col, 
+        LOSS_CURVE_KEY_TOKEN, asset_id])
+
+def loss_key(job_id, row, col, asset_id, poe):
+    """ Return a loss key generated by openquake.kvs.generate_key """
+    return openquake.kvs.generate_key([job_id, row, col, loss_token(poe), 
+        asset_id])
+
+def mean_hazard_curve_key(job_id, site):
+    """Return the key used to store a mean hazard curve
+    for a single site."""
+    return openquake.kvs.generate_key([MEAN_HAZARD_CURVE_KEY_TOKEN,
+            job_id, site.longitude, site.latitude])
+
+def quantile_hazard_curve_key(job_id, site, quantile):
+    """Return the key used to store a quantile hazard curve
+    for a single site."""
+    return openquake.kvs.generate_key([QUANTILE_HAZARD_CURVE_KEY_TOKEN,
+            job_id, site.longitude, site.latitude,
+            ("%.2f" % quantile).replace(".", "")])
+
+def quantile_value_from_hazard_curve_key(kvs_key):
+    """Extract quantile value from a KVS key for a quantile hazard curve."""
+    if extract_product_type_from_kvs_key(kvs_key) == \
+        QUANTILE_HAZARD_CURVE_KEY_TOKEN:
+        (part_before, sep, quantile_str) = kvs_key.rpartition(
+            openquake.kvs.MEMCACHE_KEY_SEPARATOR)
+        return float("%s.%s" % (quantile_str[0:-2], quantile_str[-2:]))
+    else:
+        return None
+
+def hazard_curve_key(job_id, realization_num, site_lon, site_lat):
+    """ Result a hazard curve key (for a single site) generated by
+    openquake.kvs.generate_key """
+    return openquake.kvs.generate_key([HAZARD_CURVE_KEY_TOKEN,
+                                       job_id,
+                                       realization_num, 
+                                       site_lon, 
+                                       site_lat])
+
+def realization_value_from_hazard_curve_key(kvs_key):
+    """Extract realization value (as string) from a KVS key 
+    for a hazard curve."""
+    if extract_product_type_from_kvs_key(kvs_key) == HAZARD_CURVE_KEY_TOKEN:
+        
+        # the realization is the third component of the key, after product
+        # token and job ID
+        return kvs_key.split(openquake.kvs.MEMCACHE_KEY_SEPARATOR)[2]
+    else:
+        return None
+
+def extract_product_type_from_kvs_key(kvs_key):
+    (product_type, sep, part_after) = kvs_key.partition(
+        openquake.kvs.MEMCACHE_KEY_SEPARATOR)
+    return product_type
+
+
--- /dev/null
+++ openquake-1.0/src/kvs/reader.py
@@ -0,0 +1,83 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+
+""" Read objects from kvs and translate them into our object model. """
+
+import json
+import math
+from openquake import shapes
+
+class Reader(object):
+    """Read objects from kvs and translate them into
+    our object model.
+    """
+    
+    def __init__(self, client):
+        self.client = client
+    
+    def _check_key_in_cache(self, key):
+        """Raise an error if the given key is not in kvs."""
+        
+        if not self.client.get(key):
+            raise ValueError("There's no value for key %s!" % key)
+        
+    def as_curve(self, key):
+        """Read serialized versions of hazard curves
+        and produce shapes.Curve objects.
+        
+        TODO (ac): How should we handle other metadata?
+        """
+        
+        decoded_model = self._get_and_decode(key)
+        
+        curves = []
+        
+        for raw_curves in decoded_model["hcRepList"]:
+            for curve in raw_curves["probExList"]:
+                curves.append(shapes.Curve(
+                        zip(raw_curves["gmLevels"], curve)))
+        
+        return curves
+    
+    def _get_and_decode(self, key):
+        """Get the value from cache and return the decoded object."""
+        
+        self._check_key_in_cache(key)
+        return json.JSONDecoder().decode(self.client.get(key))
+    
+    def for_nrml(self, key):
+        """Read serialized versions of hazard curves
+        and produce a dictionary as expected by the nrml writer.
+        
+        TODO (ac): What about make this generated by an improved
+        version of the Curve object?
+        """
+        
+        decoded_model = self._get_and_decode(key)
+        
+        curves = {}
+        
+        for set_counter, raw_curves in enumerate(decoded_model["hcRepList"]):
+            
+            for curve_counter, curve in enumerate(raw_curves["probExList"]):
+                data = {}
+                
+                data["IDmodel"] = "FIXED" # fixed, not yet implemented
+                data["timeSpanDuration"] = raw_curves["timeSpan"]
+                data["IMT"] = raw_curves["intensityMeasureType"]
+                data["Values"] = curve
+                data["IMLValues"] = raw_curves["gmLevels"]
+                data["endBranchLabel"] = \
+                        decoded_model["endBranchLabels"][set_counter]
+                
+                # Longitude and latitude and stored internally in the Java side
+                # in radians. That object (org.opensha.commons.geo.Location) is
+                # heavily used in the hazard engine and we don't have unit
+                # tests, so I prefer to convert to decimal degrees here.
+                lon = raw_curves["gridNode"][curve_counter]["location"]["lon"]
+                lat = raw_curves["gridNode"][curve_counter]["location"]["lat"]
+                
+                curves[shapes.Site(math.degrees(lon), math.degrees(lat))] = data
+
+        return curves
+
--- /dev/null
+++ openquake-1.0/src/kvs/__init__.py
@@ -0,0 +1,154 @@
+# -*- coding: utf-8 -*-
+"""
+This module contains generic functions to access
+the underlying kvs systems.
+"""
+
+import json
+import logging
+import uuid
+import openquake.kvs.tokens
+from openquake.kvs.redis import Redis
+
+
+DEFAULT_LENGTH_RANDOM_ID = 8
+INTERNAL_ID_SEPARATOR = ':'
+MAX_LENGTH_RANDOM_ID = 36
+MEMCACHE_KEY_SEPARATOR = '!'
+SITES_KEY_TOKEN = "sites"
+
+
+def flush():
+    """Flush (delete) all the values stored in the underlying kvs system."""
+    get_client(binary=False).flushall()
+
+
+def get_keys(regexp):
+    """Get all KVS keys that match a given regexp pattern."""
+    return get_client(binary=False).keys(regexp)
+     
+def mget(regexp):
+    """Get all the values whose keys satisfy the given regexp.
+
+    Return an empty list if there are no keys satisfying the given regxep.
+    """
+
+    values = []
+
+    keys = get_client(binary=False).keys(regexp)
+
+    if keys:
+        values = get_client(binary=False).mget(keys)
+    
+    return values
+
+
+def mget_decoded(regexp):
+    """Get and decode (from json format) all the values whose keys
+    satisfy the given regexp."""
+
+    decoded_values = []
+    decoder = json.JSONDecoder()
+
+    for value in mget(regexp):
+        decoded_values.append(decoder.decode(value))
+
+    return decoded_values
+
+
+def get(key):
+    """Get value from kvs for external decoding"""
+    return get_client(binary=False).get(key)
+
+
+def get_client(**kwargs):
+    """possible kwargs:
+        binary
+    """
+    return Redis(**kwargs)
+
+
+def generate_key(key_list):
+    """ Create a kvs key """
+    key_list = [str(x).replace(" ", "") for x in key_list]
+    return MEMCACHE_KEY_SEPARATOR.join(key_list)
+
+
+def generate_job_key(job_id):
+    """ Return a job key """
+    return generate_key(("JOB", str(job_id)))
+
+
+def generate_sites_key(job_id, block_id):
+    """ Return sites key """
+
+    sites_key_token = 'sites'
+    return generate_product_key(job_id, sites_key_token, block_id)
+
+
+def generate_product_key(job_id, product, block_id="", site=""):
+    """construct kvs key from several part IDs"""
+    return generate_key([job_id, product, block_id, site])
+
+
+def generate_random_id(length=DEFAULT_LENGTH_RANDOM_ID):
+    """This function returns a random ID by using the uuid4 method. In order
+    to have reasonably short IDs, the ID returned from uuid4() is truncated.
+    This is not optimized for being collision-free. See documentation of uuid:
+    http://docs.python.org/library/uuid.html
+    http://en.wikipedia.org/wiki/Universally_unique_identifier
+    """
+    if length > MAX_LENGTH_RANDOM_ID:
+        length = MAX_LENGTH_RANDOM_ID
+    return str(uuid.uuid4())[0:length]
+
+
+def get_value_json_decoded(key):
+    """ Get value from kvs and json decode """
+    try:
+        value = get_client(binary=False).get(key)
+        decoder = json.JSONDecoder()
+        return decoder.decode(value)
+    except (TypeError, ValueError), e:
+        print "Key was %s" % key
+        print e
+        print "Raw JSON was: %s" % value
+        return None
+
+
+def set_value_json_encoded(key, value):
+    """ Encode value and set in kvs """
+    encoder = json.JSONEncoder()
+
+    try:
+        encoded_value = encoder.encode(value)
+        get_client(binary=False).set(key, encoded_value)
+    except (TypeError, ValueError):
+        raise ValueError("cannot encode value %s to JSON" % value)
+
+    return True
+
+
+def set(key, encoded_value): #pylint: disable=W0622
+    """ Set value in kvs, for objects that have their own encoding method. """
+
+    get_client(binary=False).set(key, encoded_value)
+    return True
+
+
+def _prefix_id_generator(prefix):
+    """Generator for IDs with a specific prefix (prefix + sequence number)."""
+
+    counter = 0
+    while(True):
+        counter += 1
+        yield INTERNAL_ID_SEPARATOR.join((str(prefix), str(counter)))
+
+# generator instance used to generate IDs for blocks
+BLOCK_ID_GENERATOR = _prefix_id_generator("BLOCK")
+
+
+def generate_block_id():
+    """Generate a unique id for a block."""
+    return BLOCK_ID_GENERATOR.next() #pylint: disable=E1101
+
--- /dev/null
+++ openquake-1.0/src/kvs/redis.py
@@ -0,0 +1,76 @@
+# -*- coding: utf-8 -*-
+"""
+   Redis Client from https://github.com/ChristopherMacGown/pynpoint
+   Copyright 2010 Christopher MacGown (http://github.com/ChristopherMacGown)
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+   Modifications by Global Earthquake Model Foundation
+   Copyright 2010 Global Earthquake Model Foundation
+
+   This program is free software: you can redistribute it and/or modify
+   it under the terms of the GNU Lesser General Public License as published by
+   the Free Software Foundation, either version 3 of the License, or
+   (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public License
+   along with this program.  If not, see <http://www.gnu.org/licenses/>.
+"""
+
+from __future__ import absolute_import
+import redis
+
+from openquake import settings
+
+
+class Redis(object):
+    """ A Borg-style wrapper for Redis client class. """
+    __shared_state = {}
+
+    def __new__(cls, host=settings.KVS_HOST, 
+                     port=settings.KVS_PORT, 
+                     **kwargs): #pylint: disable-msg=W0613
+        self = object.__new__(cls)
+        self.__dict__ = cls.__shared_state
+        return self
+
+    def __init__(self, host=settings.KVS_HOST,
+                       port=settings.KVS_PORT,
+                       **kwargs):
+        if not self.__dict__:
+            print "Opening a new redis connection"
+            args = {"host": host,
+                    "port": port,
+                    "db": kwargs.get('db', 0)}
+
+            self.conn = redis.Redis(**args)
+
+    def __getattr__(self, name):
+        def call(*args, **kwargs):
+            """ Pass through the query to our redis connection """
+            cmd = getattr(self.conn, name)
+            return cmd(*args, **kwargs)
+
+        if name in self.__dict__:
+            return self.__dict__.get(name)
+
+        return call
+
+    def get_multi(self, keys):
+        """ Return value of multiple keys identically to the kvs way """
+        return dict(zip(keys, self.mget(keys)))
--- /dev/null
+++ openquake-1.0/src/job/handlers.py
@@ -0,0 +1,138 @@
+""" Handlers for guarantee_file """
+
+import paramiko
+import re
+import socket
+
+from httplib import HTTPConnection, HTTPSConnection
+from paramiko import AuthenticationException, BadAuthenticationType
+
+CREDENTIALS_RE = re.compile("(?P<username>.*?):(?P<password>.*)@")
+HOST_PORT_RE = re.compile("(?:(?:.*@)?)(?P<host>.*?)(?::(?P<port>\d+)|$)")
+
+DEFAULT_SSH_PORT = 22
+
+
+def resolve_handler(parsed_url, base_path):
+    """ Resolve to a File Handler """
+    handlers = {
+        '': FileHandler,
+        'file': FileHandler,
+        'sftp': SFTPHandler,
+        'http': HTTPHandler,
+        'https': HTTPHandler,
+    }
+
+    return handlers[parsed_url.scheme](parsed_url, base_path)
+
+
+class HandlerError(Exception):
+    """ Raised from Handler descendents """
+    pass
+
+
+class Handler(object):
+    """ A generic handler that doesn't do anything. """
+
+    def __init__(self, parsed_url, base_path):
+        self.parsed_url = parsed_url
+        self.base_path = base_path
+
+    @property
+    def credentials(self):
+        """ Extract the credentials from the parsed URL """
+        credentials = (None, None)
+        cred_match = CREDENTIALS_RE.match(self.parsed_url.netloc)
+        if cred_match:
+            credentials = cred_match.group('username', 'password')
+        return credentials
+
+    @property
+    def host_and_port(self):
+        """ Extract the host and port from the parsed URL """
+        return HOST_PORT_RE.match(self.parsed_url.netloc).group('host', 'port')
+
+    @property
+    def filename(self):
+        """ Get the filename from the parsed_url.path """
+        return self.parsed_url.path.split('/')[-1]
+
+    @property
+    def guaranteed_file_path(self):
+        """ Concatenate self.base_path and self.filename """
+        return self.base_path + self.filename
+
+
+class FileHandler(Handler):
+    """ A handler for local paths """
+
+    def handle(self, getter=None): #pylint: disable=W0613
+        """ Write this file to base_path. """ 
+        with open(self.guaranteed_file_path, "w") as writer:
+            with open(self.parsed_url.path, "r") as reader:
+                writer.write(reader.read())
+
+        return self.guaranteed_file_path
+
+
+class SFTPHandler(Handler):
+    """ A handler for files on remote systems accessible via SSH. """
+
+    def handle(self, getter=None):
+        """ Write out the file and return the full path """
+        host, port = self.host_and_port
+        username, password = self.credentials
+        transport = None
+
+        if not port:
+            port = DEFAULT_SSH_PORT
+
+        if not getter:
+            try:
+                transport = paramiko.Transport((host, int(port)))
+                transport.connect(username=username, password=password)
+            except (BadAuthenticationType, AuthenticationException), e:
+                if transport:
+                    transport.close()
+                raise HandlerError("Could not login. Bad Credentials")
+            except socket.error, e:
+                if transport:
+                    transport.close()
+                raise HandlerError(e)
+            getter = paramiko.SFTPClient.from_transport(transport)
+        else:
+            getter = getter(host, port)
+
+        getter.get(self.parsed_url.path, self.guaranteed_file_path)
+
+        if transport and transport.is_active():
+            getter.close()
+            transport.close()
+
+        return self.guaranteed_file_path
+
+
+class HTTPHandler(Handler):
+    """ A handler for files accessible via http/https"""
+
+    def handle(self, getter=None):
+        """ Write out the file and return the full path """
+
+        if not getter:
+            if self.parsed_url.scheme == 'http':
+                getter = HTTPConnection 
+            elif self.parsed_url.scheme == 'https':
+                getter = HTTPSConnection
+            else:
+                raise HandlerError("Unexpected HTTPHandler URI scheme, "
+                                   "expected https or http!")
+
+        host, port = self.host_and_port
+
+        with open(self.guaranteed_file_path, "w") as guaranteed_file:
+            with getter(host, port) as conn:
+                request = conn.request("GET", self.parsed_url.path)
+                response = request.getresponse()
+                guaranteed_file.write(response.read())
+
+        return self.guaranteed_file_path
--- /dev/null
+++ openquake-1.0/src/job/__init__.py
@@ -0,0 +1,418 @@
+# -*- coding: utf-8 -*-
+
+""" A single hazard/risk job """
+
+import hashlib
+import math
+import os
+import re
+import urlparse
+
+from ConfigParser import ConfigParser, RawConfigParser
+
+from openquake import flags
+from openquake import kvs
+from openquake import shapes
+from openquake.logs import LOG
+from openquake.job.handlers import resolve_handler
+from openquake.job.mixins import Mixin
+from openquake.parser import exposure
+
+EXPOSURE = "EXPOSURE"
+INPUT_REGION = "INPUT_REGION"
+HAZARD_CURVES = "HAZARD_CURVES"
+RE_INCLUDE = re.compile(r'^(.*)_INCLUDE')
+SITES_PER_BLOCK = 100
+
+FLAGS = flags.FLAGS
+flags.DEFINE_boolean('include_defaults', True, "Exclude default configs")
+
+def run_job(job_file):
+    """ Given a job_file, run the job. If we don't get results log it """
+    a_job = Job.from_file(job_file)
+    # TODO(JMC): Expose a way to set whether jobs should be partitioned
+    results = a_job.launch()
+    if not results:
+        # TODO (ac): Should we print additional details?
+        LOG.critical("The job configuration is inconsistent, "
+                "aborting computation.")
+    else:
+        for filepath in results:
+            print filepath
+
+
+def parse_config_file(config_file):
+    """
+    We have a single configuration file which may contain a risk section and
+    a hazard section. This input file must be in the ConfigParser format
+    defined at: http://docs.python.org/library/configparser.html.
+
+    There may be a general section which may define configuration includes in
+    the format of "sectionname_include = someconfigname.gem". These too must be
+    in the ConfigParser format.
+    """
+
+    parser = ConfigParser()
+    parser.read(config_file)
+
+    params = {}
+    sections = []
+    for section in parser.sections():
+        for key, value in parser.items(section):
+            key = key.upper()
+            # Handle includes.
+            if RE_INCLUDE.match(key):
+                config_file = "%s/%s" % (os.path.dirname(config_file), value)
+                new_sections, new_params = parse_config_file(config_file)
+                sections.extend(new_sections)
+                params.update(new_params)
+            else:
+                sections.append(section)
+                params[key] = value
+    return sections, params
+
+
+def validate(fn):
+    """Validate this job before running the decorated function."""
+
+    def validator(self, *args):
+        """Validate this job before running the decorated function."""
+        try:
+            # TODO(JMC): Add good stuff here
+            assert self.has(EXPOSURE) or self.has(INPUT_REGION)
+        except AssertionError, e:
+            LOG.exception(e)
+            return []
+        return fn(self, *args)
+
+    return validator
+
+
+def guarantee_file(base_path, file_spec):
+    """Resolves a file_spec (http, local relative or absolute path, git url,
+    etc.) to an absolute path to a (possibly temporary) file."""
+
+    url = urlparse.urlparse(file_spec)
+    return resolve_handler(url, base_path).get()
+
+
+class Job(object):
+    """A job is a collection of parameters identified by a unique id."""
+
+    __cwd = os.path.dirname(__file__)
+    __defaults = [os.path.join(__cwd, "../", "default.gem"), #package
+                    "openquake.gem",        # Sane Defaults
+                    "/etc/openquake.gem",   # Site level configs
+                    "~/.openquake.gem"]     # Are we running as a user?
+
+    @classmethod
+    def default_configs(cls):
+        """ 
+         Default job configuration files, writes a warning if they don't exist.
+        """
+        if not FLAGS.include_defaults:
+            return []
+
+        if not any([os.path.exists(cfg) for cfg in cls.__defaults]):
+            LOG.warning("No default configuration! If your job config doesn't "
+                        "define all of the expected properties things might "
+                        "break.")
+        return cls.__defaults
+
+    @staticmethod
+    def from_kvs(job_id):
+        """Return the job in the underlying kvs system with the given id."""
+        
+        params = kvs.get_value_json_decoded(kvs.generate_job_key(job_id))
+        return Job(params, job_id)
+
+    @staticmethod
+    def from_file(config_file):
+        """ Create a job from external configuration files. """
+        config_file = os.path.abspath(config_file)
+        LOG.debug("Loading Job from %s" % (config_file)) 
+        
+        base_path = os.path.abspath(os.path.dirname(config_file))
+        params = {}
+        sections = []
+        for each_config_file in Job.default_configs() + [config_file]:
+            new_sections, new_params = parse_config_file(each_config_file)
+            sections.extend(new_sections)
+            params.update(new_params)
+        params['BASE_PATH'] = base_path
+        job = Job(params, sections=sections, base_path=base_path)
+        job.config_file = config_file               #pylint: disable=W0201
+        # job.config_file = job.super_config_path   #pylint: disable=W0201
+        return job
+
+    def __init__(self, params, job_id=None, sections=list(), base_path=None):
+        if job_id is None:
+            job_id = kvs.generate_random_id()
+        
+        self.job_id = job_id
+        self.blocks_keys = []
+        self.partition = True
+        self.params = params
+        self.sections = list(set(sections)) # uniquify
+        self.base_path = base_path
+        if base_path:
+            self.to_kvs()
+
+    def has(self, name):
+        """Return true if this job has the given parameter defined
+        and specified, false otherwise."""
+        return self.params.has_key(name) and self.params[name] != ""
+
+    @property
+    def id(self): #pylint: disable=C0103
+        """Return the id of this job."""
+        return self.job_id
+    
+    @property
+    def key(self):
+        """Returns the kvs key for this job."""
+        return kvs.generate_job_key(self.job_id)
+
+    @property
+    def region(self):
+        """Compute valid region with appropriate cell size from config file."""
+        if not self.has('REGION_VERTEX'):
+            return None
+        verts = [float(x) for x in self['REGION_VERTEX'].split(",")]
+        # Flips lon and lat, and builds a list of coord tuples
+        coords = zip(verts[1::2], verts[::2])
+        region = shapes.RegionConstraint.from_coordinates(coords)
+        region.cell_size = float(self['REGION_GRID_SPACING'])
+        return region
+
+    @property
+    def super_config_path(self):
+        """ Return the path of the super config """
+        filename = "%s-super.gem" % self.job_id
+        return os.path.join(self.base_path or '', "./", filename)
+
+    @validate
+    def launch(self):
+        """ Based on the behaviour specified in the configuration, mix in the
+        correct behaviour for the tasks and then execute them.
+        """
+        output_dir = os.path.join(self.base_path, self['OUTPUT_DIR'])
+        if not os.path.exists(output_dir):
+            os.makedirs(output_dir)
+        results = []
+        self._partition()
+        for (key, mixin) in Mixin.ordered_mixins():
+            if key.upper() not in self.sections:
+                continue
+
+            with Mixin(self, mixin, key=key):
+                # The mixin defines a preload decorator to handle the needed
+                # data for the tasks and decorates _execute(). the mixin's
+                # _execute() method calls the expected tasks.
+                LOG.debug("Job %s Launching %s for %s" % (self.id, mixin, key)) 
+                results.extend(self.execute()) #pylint: disable=E1101
+
+        return results
+
+    def _partition(self):
+        """Split the set of sites to compute in blocks and store
+        the in the underlying kvs system.
+        """
+
+        sites = []
+        self.blocks_keys = []
+        region_constraint = self.region
+        
+        # we use the exposure, if specified,
+        # otherwise we use the input region
+        if self.has(EXPOSURE):
+            sites = self._read_sites_from_exposure()
+            LOG.debug("Loaded %s sites from exposure portfolio." % len(sites))
+        elif self.region:
+            sites = self.region.sites
+        else:
+            raise Exception("I don't know how to get the sites!")
+        if self.partition:
+            block_count = 0
+            for block in BlockSplitter(sites, constraint=region_constraint):
+                self.blocks_keys.append(block.id)
+                block.to_kvs()
+                block_count += 1
+            LOG.debug("Job has partitioned %s sites into %s blocks" % (
+                    len(sites), block_count))
+        else:
+            block = Block(sites)
+            self.blocks_keys.append(block.id)
+            block.to_kvs()
+
+    def _read_sites_from_exposure(self):
+        """Read the set of sites to compute from the exposure file specified
+        in the job definition."""
+
+        sites = []
+        path = os.path.join(self.base_path, self[EXPOSURE])
+        reader = exposure.ExposurePortfolioFile(path)
+        constraint = self.region
+        if not constraint:
+            constraint = AlwaysTrueConstraint()
+        else:
+            LOG.debug("Constraining exposure parsing to %s" % 
+                constraint.polygon)
+        for asset_data in reader.filter(constraint):
+            sites.append(asset_data[0])
+
+        return sites
+
+    def __getitem__(self, name):
+        return self.params[name]
+
+    def __eq__(self, other):
+        return self.params == other.params
+        
+    def __str__(self):
+        return str(self.params)
+
+    def _write_super_config(self):
+        """
+            Take our params and write them out as a 'super' config file. 
+            Its name is equal to the job_id, which should be the sha1 of
+            the file in production or a random job in dev.
+        """
+
+        kvs_client = kvs.get_client(binary=False)
+        config = RawConfigParser()
+
+        section = 'openquake'
+        config.add_section(section)
+
+        for key, val in self.params.items():
+            v = kvs_client.get(val)
+            if v:
+                val = v
+            config.set(section, key, val)
+
+        with open(self.super_config_path, "wb") as configfile:
+            config.write(configfile)
+
+    def _slurp_files(self):
+        """Read referenced files and write them into kvs, keyed on their
+        sha1s."""
+        kvs_client = kvs.get_client(binary=False)
+        if self.base_path is None:
+            LOG.debug("Can't slurp files without a base path, homie...")
+            return
+            # raise Exception("Can't slurp files without a base path, homie...")
+        for key, val in self.params.items():
+            if key[-5:] == '_FILE':
+                path = os.path.join(self.base_path, val)
+                with open(path) as data_file:
+                    LOG.debug("Slurping %s" % path)
+                    sha1 = hashlib.sha1(data_file.read()).hexdigest()
+                    data_file.seek(0)
+                    kvs_client.set(sha1, data_file.read())
+                    self.params[key] = sha1
+
+    def to_kvs(self, write_cfg=True):
+        """Store this job into kvs."""
+        self._slurp_files()
+        if write_cfg:
+            self._write_super_config()
+        key = kvs.generate_job_key(self.job_id)
+        kvs.set_value_json_encoded(key, self.params)
+
+    def site_list_generator(self):
+        """Will subset and yield portions of the region, depending on the 
+        the computation mode."""
+        verts = [float(x) for x in self.params['REGION_VERTEX'].split(",")]
+        coords = zip(verts[1::2], verts[::2])
+        region = shapes.Region.from_coordinates(coords)
+        region.cell_size = float(self.params['REGION_GRID_SPACING'])
+        yield [site for site in region]
+
+
+class AlwaysTrueConstraint():
+    """ A stubbed constraint for block splitting """
+
+    #pylint: disable=W0232,W0613,R0201
+
+    def match(self, point):
+        """ stub a match filter to always return true """
+        return True
+
+
+class Block(object):
+    """A block is a collection of sites to compute."""
+
+    def __init__(self, sites, block_id=None):
+        self.sites = tuple(sites)
+        if not block_id:
+            block_id = kvs.generate_block_id()
+        self.block_id = block_id
+
+    def grid(self, region):
+        """Provides an iterator across the unique grid points within a region,
+         corresponding to the sites within this block."""
+        used_points = []
+        for site in self.sites:
+            point = region.grid.point_at(site)
+            if point not in used_points:
+                used_points.append(point)
+                yield point
+
+    def __eq__(self, other):
+        return self.sites == other.sites
+
+    @classmethod
+    def from_kvs(cls, block_id):
+        """Return the block in the underlying kvs system with the given id."""
+
+        raw_sites = kvs.get_value_json_decoded(block_id)
+
+        sites = []
+
+        for raw_site in raw_sites:
+            sites.append(shapes.Site(raw_site[0], raw_site[1]))
+
+        return Block(sites, block_id)
+
+    def to_kvs(self):
+        """Store this block into the underlying kvs system."""
+
+        raw_sites = []
+
+        for site in self.sites:
+            raw_sites.append(site.coords)
+
+        kvs.set_value_json_encoded(self.id, raw_sites)
+
+    @property
+    def id(self): #pylint: disable=C0103
+        """Return the id of this block."""
+        return self.block_id
+
+
+class BlockSplitter(object):
+    """Split the sites into a set of blocks."""
+
+    def __init__(self, sites, sites_per_block=SITES_PER_BLOCK, constraint=None):
+        self.sites = sites
+        self.constraint = constraint
+        self.sites_per_block = sites_per_block
+    
+        if not self.constraint:            
+            self.constraint = AlwaysTrueConstraint()
+    
+    def __iter__(self):
+        filtered_sites = []
+
+        # TODO (ac): Can be done better using shapely.intersects,
+        # but after the shapes.Site refactoring...
+        for site in self.sites:
+            if self.constraint.match(site):
+                filtered_sites.append(site)
+                if len(filtered_sites) == self.sites_per_block:
+                    yield(Block(filtered_sites))
+                    filtered_sites = []
+        if not filtered_sites:
+            return    
+        yield(Block(filtered_sites))
--- /dev/null
+++ openquake-1.0/src/job/mixins.py
@@ -0,0 +1,103 @@
+"""
+   Mixins from https://github.com/ChristopherMacGown/pynpoint
+   Copyright 2010 Christopher MacGown (http://github.com/ChristopherMacGown)
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+   Modifications by Global Earthquake Model Foundation
+   Copyright 2010 Global Earthquake Model Foundation
+
+   This program is free software: you can redistribute it and/or modify
+   it under the terms of the GNU Lesser General Public License as published by
+   the Free Software Foundation, either version 3 of the License, or
+   (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public License
+   along with this program.  If not, see <http://www.gnu.org/licenses/>.
+"""
+
+def loader(target, mixin):
+    """ Load the mixin into the target's class """
+    target.__class__.__bases__ += (mixin,)
+    return target
+
+def unloader(target, mixin):
+    """ Unload the mixin from the target's class __bases__"""
+    bases = list(target.__class__.__bases__)
+    bases.remove(mixin)
+    target.__class__.__bases__ = tuple(bases)
+
+
+class Mixin:
+    """ A callable that handles mixing behaviour into a target, and
+    provides the opener api so we can use it with the python with 
+    syntax.
+    """
+    mixins = {}
+    def __init__(self, target, mixin, key=""):
+        self.key = key.upper() + "_CALCULATION_MODE"
+        self.target = target
+        self.mixin = mixin
+
+    def __enter__(self):
+        return self._load()
+
+    def __exit__(self, *args):
+        self._unload()
+
+    def _load(self):
+        """ Load a mixin, if it's a subclass, also load the proxied mixin"""
+        if issubclass(self.mixin, Mixin):
+            self._load_proxied_mixin()
+
+        return loader(self.target, self.mixin)
+
+    def _unload(self):
+        """ Unload a mixin, if it's a subclass, also load the proxied mixin"""
+        if issubclass(self.mixin, Mixin):
+            self._unload_proxied_mixin()
+
+        return unloader(self.target, self.mixin)
+
+    def _load_proxied_mixin(self):
+        """ Load the proxied mixin requested by the calculation mode """
+
+        calc_mode = self.target[self.key]
+        loader(self.target, self.mixin.mixins[calc_mode]['mixin'])
+
+    def _unload_proxied_mixin(self):
+        """ Unload the proxied mixin requested by the calculation mode """
+        calc_mode = self.target[self.key]
+        unloader(self.target, self.mixin.mixins[calc_mode]['mixin'])
+
+    @classmethod
+    def ordered_mixins(cls):
+        """ Return a list of mixins sorted by the order value specified at 
+        registration """
+
+        return [(k, v['mixin'])
+                 for (k, v)
+                 in sorted(cls.mixins.items(), key=lambda x: x[1]['order'])]
+
+    @classmethod
+    def register(cls, key, mixin, order=0):
+        """ Register a new mixin. We expect a string key, a class, and an 
+        optional order. The order is really only optional in the mixin 
+        proxies."""
+        if not key in cls.mixins:
+            cls.mixins[key] = {'mixin': mixin, 'order': order }
--- /dev/null
+++ openquake-1.0/src/parser/hazard.py
@@ -0,0 +1,205 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+
+"""This module contains a class that parses instance document files of a
+specific flavour of the NRML data format. This flavour is NRML the potential
+outcome of the hazard calculations in the engine. The root element of such
+NRML instance documents is <HazardResultList>.
+"""
+
+from lxml import etree
+
+from openquake import logs
+
+from openquake import producer
+from openquake import shapes
+
+from openquake.xml import NRML_NS, GML_NS_OLD, NRML
+
+LOG = logs.LOG
+
+def _to_site(element):
+    """Convert current GML attributes to Site object"""
+    # lon/lat are in XML attributes 'Longitude' and 'Latitude'
+    # consider them as mandatory
+    pos_el = element.xpath("gml:pos", namespaces={"gml": GML_NS_OLD})
+    coord = [float(x) for x in pos_el[0].text.strip().split()]
+    return shapes.Site(coord[0], coord[1])
+
+
+class NrmlFile(producer.FileProducer):
+    """ This class parses a NRML hazard curve file. The contents of a NRML
+    file is meant to be used as input for the risk engine. The class is
+    implemented as a generator. For each 'Curve' element in the parsed 
+    instance document, it yields a pair of objects, of which the
+    first one is a shapes object of type Site (representing a
+    geographical site as WGS84 lon/lat), and the second one
+    is a dictionary with hazard-related attribute values for this site.
+    
+    The attribute dictionary looks like
+    {'IMT': 'PGA',
+     'IDmodel': 'Model_Id',
+     'timeSpanDuration': 50.0,
+     'endBranchLabel': 'Foo', 
+     'IML': [5.0000e-03, 7.0000e-03, ...], 
+     'Values': [9.8728e-01, 9.8266e-01, ...],
+    }
+
+    Notes:
+    1) TODO(fab): require that attribute values of 'IMT' are from list
+       of allowed values (see NRML XML Schema)
+    2) 'endBranchLabel' can be replaced by 'aggregationType'
+    3) TODO(fab): require that value of 'aggregationType' element is from a
+       list of allowed values (see NRML XML Schema)
+    4) 'saPeriod', 'saDamping', 'calcSettingsID', are optional
+    5) NRML output can also contain hazard maps, parsing of those is not yet
+       implemented
+
+    """
+
+    PROCESSING_ATTRIBUTES = (('IDmodel', str), ('timeSpanDuration', float),
+                             ('saPeriod', float), ('saDamping', float))
+
+    def __init__(self, path):
+        super(NrmlFile, self).__init__(path)
+
+    def _parse(self):
+        for event, element in etree.iterparse(
+                self.file, events=('start', 'end')):
+            if event == 'start' and element.tag == NRML + 'HazardProcessing':
+                self._hazard_curve_meta(element)
+            # The HazardMap is not yet implemented
+            #elif event == 'start' and element.tag == NRML + 'HazardMap':
+             #error_str = "parsing of HazardMap elements is not yet implemented"
+             #raise NotImplementedError(error_str)    
+            elif event == 'end' and element.tag == NRML + 'HazardCurve':
+                yield (_to_site(element), 
+                       self._to_attributes(element))
+    
+    def _hazard_curve_meta(self, element):
+        """ Hazard curve metadata from the element """
+        self._current_hazard_meta = {} #pylint: disable=W0201
+        for (required_attribute, attrib_type) in self.PROCESSING_ATTRIBUTES:
+            id_model_value = element.get(required_attribute)
+            if id_model_value is not None:
+                self._current_hazard_meta[required_attribute]\
+                = attrib_type(id_model_value)
+            else:
+                error_str = "element Hazard Curve metadata: missing required " \
+                    "attribute %s" % required_attribute
+                raise ValueError(error_str)
+
+    def _to_attributes(self, element):
+        """ Build an attributes dict from XML element """
+        
+        attributes = {}
+        
+        float_strip = lambda x: [float(o) for o in x[0].text.strip().split()]
+        string_strip = lambda x: x[0].text.strip()
+        # TODO(JMC): This is hardly efficient, but it's simple for the moment...
+        
+        for (child_el, child_key, etl) in (
+            ('nrml:Values', 'Values', float_strip),
+            ('../nrml:Common/nrml:IMLValues','IMLValues', float_strip),
+            ('../nrml:Common/nrml:IMT', 'IMT', string_strip)):
+            child_node = element.xpath(child_el, 
+                namespaces={"gml": GML_NS_OLD, "nrml": NRML_NS})
+
+            try:
+                attributes[child_key] = etl(child_node)
+            except Exception:
+                error_str = "invalid or missing %s value" % child_key
+                raise ValueError(error_str) 
+
+        # consider all attributes of HazardProcessing element as mandatory 
+        for (required_attribute, attrib_type) in [('endBranchLabel', str)]:
+            (haz_list_element,) = element.xpath("..", 
+                namespaces={"gml": GML_NS_OLD, "nrml": NRML_NS})
+            attr_value = haz_list_element.get(required_attribute)
+            if attr_value is not None:
+                attributes[required_attribute] = \
+                    attrib_type(attr_value)
+            else:
+                error_str = "element endBranchLabel: missing required "\
+                   "attribute %s" % required_attribute
+                raise ValueError(error_str) 
+
+        try:
+            attributes.update(self._current_hazard_meta)
+        except Exception:
+            error_str = "root element (HazardProcessing) is missing"
+            raise ValueError(error_str) 
+        
+        return attributes
+    # 
+    # def filter(self, attribute_constraint=None):
+    #    for next in iter(self):
+    #        if (attribute_constraint is not None and \
+    #                attribute_constraint.match(next)):
+    #            yield next
+
+
+class GMFReader(producer.FileProducer):
+    """ This class parses a NRML GMF (ground motion field) file. 
+    The class is implemented as a generator. For each 'site' element 
+    in the parsed instance document, it yields a pair of objects, of 
+    which the first one is a shapes object of type Site (representing a
+    geographical site as WGS84 lon/lat), and the second one
+    is a dictionary with GMF-related attribute values for this site.
+
+    The attribute dictionary looks like
+    {'groundMotion': 0.8}
+    """
+
+    def __init__(self, path):
+        super(GMFReader, self).__init__(path)
+
+    def _parse(self):
+        site_nesting_level = 0
+        for event, element in etree.iterparse(
+                self.file, events=('start', 'end')):
+            if event == 'start' and element.tag == NRML + 'HazardResult':
+                self._gmf_attributes()
+            elif event == 'start' and element.tag == NRML + 'site':
+                site_nesting_level += 1
+            elif event == 'end' and element.tag == NRML + 'site':
+                site_nesting_level -= 1
+
+                # yield only for outer site elements
+                if site_nesting_level == 0:
+                    yield (self._to_site_data(element))
+
+    def _gmf_attributes(self):
+        """TODO(fab): Collect instance-wide attributes here."""
+        pass
+
+    def _to_site_data(self, element):
+        """this is called on the outer 'site' elements"""
+        
+        attributes = {}
+        attributes['groundMotion'] = float(element.get('groundMotion'))
+        (inner_site_node,) = element.xpath('nrml:site', 
+                namespaces={"gml": GML_NS_OLD, "nrml": NRML_NS})
+        return (_to_site(inner_site_node), attributes)
+
+
+class HazardConstraint(object):
+    """ This class represents a constraint that can be used to filter
+    VulnerabilityFunction elements from an VulnerabilityModel XML instance 
+    document based on their attributes. The constructor requires a dictionary
+    as argument. Items in this dictionary have to match the corresponding ones
+    in the checked attribute object.
+    """
+    def __init__(self, attribute):
+        self.attribute = attribute
+
+    def match(self, compared_attribute):
+        """ Compare self.attribute against the passed in compared_attribute
+        dict. If the compared_attribute dict does not contain all of the 
+        key/value pais from self.attribute, we return false. compared_attribute
+        may have additional key/value pairs.
+        """
+        for k, v in self.attribute.items():
+            if not (k in compared_attribute and compared_attribute[k] == v):
+                return False
+        return True
--- /dev/null
+++ openquake-1.0/src/parser/esri.py
@@ -0,0 +1,141 @@
+#!/usr/bin/env python
+# encoding: utf-8
+
+"""This module contains classes to parse ESRI stuff ported from
+the Java version of the risk engine.
+
+This module is not currently used and there isn't any test coverage.
+"""
+
+import math
+import struct
+
+class Grid:
+    """ESRIGrid format as per http://en.wikipedia.org/wiki/ESRI_grid."""
+    def __init__(self, rows, columns, no_data_value=9999):
+        self.columns = columns
+        self.rows = rows
+        self.no_data_value = no_data_value
+
+    def is_no_data_value(self, val):
+        """ Return true if the value matches our grid no_data_value """
+        return val == self.no_data_value
+        
+    def check_column(self, point):
+        """ Raise if the point is out of bounds """
+        if (self.columns < point.column or point.column < 1):
+            raise Exception("Point is not on the Grid")
+            
+    def check_row(self, point):
+        """ Raise if the point is out of bounds """
+        if (self.rows < point.row or point.row < 1):
+            raise Exception("Point is not on the Grid")
+
+class Point:
+    """Simple (trivial) point class."""
+    def __init__(self, row, column):
+        self.column = column
+        self.row = row
+
+class Site:
+    """Site has lat and long."""
+    def __init__(self, latitude, longitude):
+        self.latitude = latitude
+        self.longitude = longitude
+
+class BaseExposureReader(object):
+    """Base class for reading exposure data from file formats."""
+    def __init__(self, filename, exposure_definition):
+        self.filename = filename
+        self.definition = exposure_definition
+        self.exposure_file = open(filename, "rb")
+
+class ESRIBinaryFileExposureReader(BaseExposureReader):
+    """Parses and loads ESRI formatted exposure data from files."""
+    
+    def __init__(self, filename, exposure_definition):
+        super(ESRIBinaryFileExposureReader, self).__init__(
+                filename, exposure_definition)
+    
+    def read_at(self, site):
+        """ Return the unpacked value of the point at the site """
+        point = self.definition.point_at(site)
+        position = self.position_of(point)
+        print "pos is %s" % position
+        self.exposure_file.seek(position)
+        val = self.exposure_file.read(4)
+        return float(struct.unpack("<f", val)[0])
+        
+    def position_of(self, point):
+        """ Return the row/column of the point in bytes """
+        rows_offset = (point.row - 1) * self.definition.grid.columns
+        rows_offset_in_bytes = rows_offset * 4 # All points are doubles
+        columns_offset_in_bytes = (point.column - 1) * 4
+        return rows_offset_in_bytes + columns_offset_in_bytes
+
+class AsciiFileHazardIMLReader(BaseExposureReader):
+    """ Stubbed reader for Ascii Hazard IML """
+    pass
+
+class ESRIRasterMetadata():
+    """Object loaded from (various) ESRI header files."""
+    def __init__(self, cell_size, grid, lower_left_corner):
+        self.cell_size = cell_size
+        self.grid = grid
+        self.lower_left_corner = lower_left_corner
+    
+    @classmethod
+    def load_esri_header(cls, filename):
+        """ Build an ESRIRasterMetadata object from an ESRI header file """
+        with open(filename, "r") as header_file:
+            columns = int(header_file.readline().split()[1])
+            rows = int(header_file.readline().split()[1])
+            xllcorner = float(header_file.readline().split()[1])
+            yllcorner = float(header_file.readline().split()[1])
+            cell_size = float(header_file.readline().split()[1])
+            no_data_value = int(header_file.readline().split()[1])
+
+        lower_left_corner = Site(xllcorner, yllcorner)
+        grid = Grid(rows, columns, no_data_value)
+        return cls(cell_size, grid, lower_left_corner)
+        
+    @classmethod
+    def load_hazard_iml(cls, filename):
+        """ Build an ESRIRasterMetadata object from an hazard IML file """
+        with open(filename, "r") as header_file:
+            header_file.readline() # Skip one line
+            tokens = header_file.readline().split()
+            print tokens
+            # [78 36  27.225  39.825  0.05]
+            rows = int(tokens[1])
+            xllcorner = float(tokens[2])
+            yllcorner = float(tokens[3])
+            columns = int(tokens[0].replace("[", ""))
+            cell_size = float(tokens[4].replace("]", ""))
+
+        lower_left_corner = Site(xllcorner, yllcorner)
+        grid = Grid(rows, columns, 0)
+        return cls(cell_size, grid, lower_left_corner)
+
+    def _latitude_to_row(self, latitude):
+        """Calculate row from latitude value."""
+        latitude_offset = math.fabs(latitude - self.lower_left_corner.latitude)
+        print "lat offset = %s" % latitude_offset
+        return int(self.grid.rows - (latitude_offset / self.cell_size)) + 1
+
+    def _longitude_to_column(self, longitude):
+        """Calculate column from longitude value."""
+        longitude_offset = longitude - self.lower_left_corner.longitude
+        print "long offset = %s" % longitude_offset
+        return int((longitude_offset / self.cell_size) + 1)
+
+    def point_at(self, site):
+        """Translates a site into a matrix bidimensional point."""
+        print "%s, %s" % (site.latitude, site.longitude)
+        row = self._latitude_to_row(site.latitude)
+        column = self._longitude_to_column(site.longitude)
+        result = Point(row, column)
+        print "%s, %s" % (row, column)
+        self.grid.check_row(result)
+        self.grid.check_column(result)
+        return result
--- /dev/null
+++ openquake-1.0/src/parser/risk.py
@@ -0,0 +1,94 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+
+"""This module contains a class that parses NRML instance document files
+for the output of risk computations. At the moment, loss and loss curve data
+is supported. The root element of such NRML instance documents 
+is <RiskResult>.
+"""
+
+from lxml import etree
+
+from openquake import producer
+from openquake import shapes
+from openquake.xml import NRML_NS, GML_NS_OLD, NRML
+
+def _to_site(element):
+    """Convert current GML attributes to Site object."""
+    # lon/lat are in XML attributes 'Longitude' and 'Latitude'
+    # consider them as mandatory
+    pos_el = element.xpath("gml:pos", namespaces={"gml": GML_NS_OLD})
+    coord = [float(x) for x in pos_el[0].text.strip().split()]
+    return shapes.Site(coord[0], coord[1])
+
+class NrmlFile(producer.FileProducer):
+    """ This class parses a NRML loss/loss ratio curve file. 
+    The class is implemented as a generator. 
+    For each curve element in the parsed 
+    instance document, it yields a pair of objects, of which the
+    first one is a shapes object of type Site (representing a
+    geographical site as WGS84 lon/lat), and the second one
+    is a dictionary with risk-related attribute values for this site.
+    
+    The attribute dictionary looks like
+    {'POE': [0.2, 0.02, ...], 
+     'Values': [0.0, 1280.0, ...],
+     'Property': 'Loss' # 'Loss Ratio'
+    }
+    """
+
+    def __init__(self, path, mode='loss'):
+        self.mode = mode
+
+        self.ordinate_property = 'Probability of Exceedance'
+        self.abscissa_element = 'Values'
+
+        self.abscissa_output_key = 'Values'
+        self.ordinate_output_key = 'POE'
+        self.property_output_key = 'Property'
+
+        if self.mode == 'loss_ratio':
+            self.abscissa_property = 'Loss Ratio'
+            self.ordinate_element = 'LossRatioCurvePE'
+            self.abscissa_container = 'LossRatioCurve'
+        else:
+            self.abscissa_property = 'Loss'
+            self.ordinate_element = 'LossCurvePE'
+            self.abscissa_container = 'LossCurve'
+
+        super(NrmlFile, self).__init__(path)
+
+    def _parse(self):
+        for event, element in etree.iterparse(
+                self.file, events=('start', 'end')):
+            if event == 'end' and \
+                element.tag == NRML + self.abscissa_container:
+                yield (_to_site(element), 
+                       self._to_attributes(element))
+
+    def _to_attributes(self, element):
+        """ Build an attributes dict from XML element """
+        
+        attributes = {self.property_output_key: self.abscissa_property}
+        
+        float_strip = lambda x: [float(o) for o in x[0].text.strip().split()]
+        # TODO(JMC): This is hardly efficient, but it's simple for the moment...
+        
+        for (child_el, child_key, etl) in (
+            ('nrml:%s' % self.abscissa_element, self.abscissa_output_key, 
+                float_strip),
+            ('../nrml:Common/nrml:%s/nrml:Values' % self.ordinate_element,
+                self.ordinate_output_key, float_strip)):
+            
+            child_node = element.xpath(child_el, 
+                namespaces={"gml": GML_NS_OLD, "nrml": NRML_NS})
+
+            try:
+                attributes[child_key] = etl(child_node)
+            except Exception:
+                error_str = "invalid or missing %s value" % child_key
+                raise ValueError(error_str) 
+        
+        attributes["AssetID"] = element.attrib.get("AssetID", None)
+        return attributes
+
--- /dev/null
+++ openquake-1.0/src/parser/nshmp.py
@@ -0,0 +1,33 @@
+"""
+jpype config stuff for Java NSHMP stuff
+"""
+
+
+import os
+from openquake import logs
+
+LOG = logs.LOG
+
+JAVA_NAMESPACE = "org.gem.engine.hazard.models.nshmp.us."
+JAVA_CLASS = JAVA_NAMESPACE+ "NshmpUsData"
+BOUNDING_BOX = (24.6, 50.0, -125.0, -65.5)
+# BOUNDING_BOX = (35.0, 35.0, -120, -120.0)
+
+JAVA_MODELS = [(JAVA_NAMESPACE +"NshmpWusFaultData", "WUS/WUSfaults"), 
+                (JAVA_NAMESPACE +"NshmpCaliforniaFaultData", "CA/CA_faults"),
+                (JAVA_NAMESPACE +"NshmpCascadiaSubductionData", "Cascadia"),
+                (JAVA_NAMESPACE +"NshmpCeusFaultData", "CEUS/CEUS.faults"),
+                (JAVA_NAMESPACE +"NewNshmpWusGridData", "WUS/WUSmap"),
+                (JAVA_NAMESPACE +"NewNshmpCaliforniaGridData", "CA/CA_map"),
+                (JAVA_NAMESPACE +"NewNshmpCeusGridData", "CEUS/CEUS.map.input"),
+                ]
+
+def init_paths(base_path, jpype):
+    """ Init the java paths """
+
+    for class_name, subdir in JAVA_MODELS:
+        path = os.path.abspath("%s/%s" % (base_path, subdir))
+        java_class = jpype.JClass(class_name)
+        LOG.debug("Setting inDir on %s to %s", class_name, path+"/")
+        java_class.inDir = path+"/"
+    
--- /dev/null
+++ openquake-1.0/src/parser/__init__.py
@@ -0,0 +1,3 @@
+"""A variety of file format parsers, including many of our 
+internally defined NRML format, in both XML and YAML. Also
+parsers for well-known ESRI binary and ascii formats."""
\ No newline at end of file
--- /dev/null
+++ openquake-1.0/src/parser/exposure.py
@@ -0,0 +1,101 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""Parsers to read exposure files, including exposure portfolios.
+These can include building, population, critical infrastructure,
+and other asset classes."""
+
+from lxml import etree
+
+from openquake import producer
+from openquake import shapes
+from openquake.xml import NRML, GML_OLD
+
+# do not use namespace for now
+RISKML_NS = ''
+
+
+def _to_site(element):
+    """Convert current GML attributes to Site object"""
+    # lon/lat are in XML attribute gml:pos
+    # consider them as mandatory
+
+    pos = element.find("%spos" % GML_OLD).text
+    
+    try:
+        lat, lon = [float(x.strip()) for x in pos.split()]
+        return shapes.Site(lon, lat)
+    except Exception:
+        error_str = "element AssetInstance: no valid lon/lat coordinates"
+        raise ValueError(error_str)
+
+
+class ExposurePortfolioFile(producer.FileProducer):
+    """ This class parses an ExposurePortfolio XML (part of riskML?) file.
+    The contents of such a file is meant to be used as input for the risk 
+    engine. The class is implemented as a generator. 
+    For each 'AssetInstance' element in the parsed 
+    instance document, it yields a pair of objects, of which the
+    first one is a shapely.geometry object of type Point (representing a
+    geographical site as WGS84 lon/lat), and the second one
+    is a dictionary with exposure-related attribute values for this site.
+    
+    The attribute dictionary looks like
+    {'PortfolioID': 'PAV01',
+     'PortfolioDescription': 'Collection of existing buildings in downtown Pavia',
+     'AssetID': '01',
+     'AssetDescription': 'Moment-resisting non-ductile concrete frame low rise',
+     'AssetValue': 150000,
+     'VulnerabilityFunction': 'RC/DMRF-D/LR'
+    }
+
+    Note: at the time of writing this class the author has no access to the
+    XML Schema, so all XML attributes from the example instance documents are
+    assumed to be mandatory.
+
+    """
+
+    REQUIRED_ATTRIBUTES = (('PortfolioID', str), 
+                           ('PortfolioDescription', str))
+
+    def __init__(self, path):
+        super(ExposurePortfolioFile, self).__init__(path)
+
+    def _parse(self):
+        for event, element in etree.iterparse(
+                self.file, events=('start', 'end')):
+
+            if event == 'start' and element.tag == \
+                    '%sExposureParameters' % NRML:
+
+                self._set_meta(element)
+            elif event == 'end' and element.tag == '%sAssetInstance' % NRML:
+                yield (_to_site(element), 
+                       self._to_site_attributes(element))
+
+    def _to_site_attributes(self, element):
+        """Build a dict of all node attributes"""
+        site_attributes = {}
+
+        site_attributes["AssetID"] = element.find("%sAssetID" % NRML).text
+        site_attributes["AssetValue"] = float(element.find(
+                "%sAssetValue" % NRML).text)
+
+        # consider all attributes of AssetInstance element as mandatory
+        for required_attribute in (('AssetDescription', str),
+                                   ('VulnerabilityFunction', str)):
+            attr_value = element.get(required_attribute[0])
+            if attr_value is not None:
+                site_attributes[required_attribute[0]] = \
+                    required_attribute[1](attr_value)
+            else:
+                error_str = "element AssetInstance: missing required " \
+                    "attribute %s" % required_attribute[0]
+                raise ValueError(error_str) 
+
+        try:
+            site_attributes.update(self._current_meta)
+        except Exception:
+            error_str = "root element (ExposurePortfolio) is missing"
+            raise ValueError(error_str)
+
+        return site_attributes
--- /dev/null
+++ openquake-1.0/src/parser/vulnerability.py
@@ -0,0 +1,104 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+Codec for processing vulnerability curves
+from XML files.
+
+A DOM version of the vulnerability model parser,
+that takes into account the really small size of this input file.
+
+"""
+
+from lxml import etree
+
+from openquake import logs
+from openquake import kvs
+from openquake import shapes
+from openquake.xml import NRML
+
+LOGGER = logs.RISK_LOG
+
+EMPTY_CODE = 'EMPTY'
+VULNERABILITY_CURVE_KEY_TOKEN = 'vulnerability_curves'
+
+def register_vuln_curves(vuln_curves, job_id):
+    """This fuction writes a set of vulnerability curves to the kvs system.
+    It requires a dict as input, which holds the vulnerability curve codes
+    as keys, and the corresponding Curve objects as values.
+    job_id is part of the memcache key that points to the curves.
+    The function returns a dict with the curves."""
+
+    # get already registered curves
+    vulnerability_curves = load_vuln_curves_from_kvs(job_id)
+
+    # append new curves, or update existing curves
+    vulnerability_curves.update(vuln_curves)
+
+    # write to memcache
+    write_vuln_curves_to_kvs(job_id, vulnerability_curves)
+
+    return vulnerability_curves
+
+def delete_vuln_curves(job_id):
+    """Deletes vulnerability curves for a given job_id from the kvs system."""
+    kvs.get_client(binary=False).delete(kvs.tokens.vuln_key(job_id))
+
+def load_vulnerability_model(job_id, path):
+    """Loads and registers all the vulnerability functions defined."""
+
+    vulnerability_curves = {}
+
+    # register empty curve
+    vulnerability_curves[EMPTY_CODE] = shapes.EMPTY_CURVE.to_json()
+
+    vulnerability_model = etree.parse(path).getroot()
+
+    intensity_measure_values = [float(x) for x in
+            vulnerability_model.find(
+            ".//%sValues" % NRML).text.strip().split()]
+    
+    for vulnerability_function in vulnerability_model.findall(
+            ".//%sDiscreteVulnerability" % NRML):
+
+        loss_ratio_values = [float(x) for x in
+                vulnerability_function.find(
+                '%sLossRatios' % NRML).text.strip().split()]
+                
+        coefficient_variation_values = [float(x) for x in
+                vulnerability_function.find(
+                '%sCoefficientsVariation' % NRML).text.strip().split()]
+
+        # convert raw values into our object model
+        curve_data = []
+        pairs = zip(loss_ratio_values, coefficient_variation_values)
+        
+        for idx, iml in enumerate(intensity_measure_values):
+            curve_data.append(('%s' % iml, pairs[idx]))
+
+        vulnerability_curves[vulnerability_function.attrib['ID']] = \
+            shapes.Curve(curve_data).to_json()
+
+    write_vuln_curves_to_kvs(job_id, vulnerability_curves)
+
+def write_vuln_curves_to_kvs(job_id, vulnerability_curves):
+    """ JSON encode vulnerability curve and write to KVS """
+    success = kvs.set_value_json_encoded(kvs.tokens.vuln_key(job_id), 
+        vulnerability_curves)
+
+    if success is not True:
+        raise RuntimeError(
+            "Vulnerability module: cannot write "
+            "vulnerability functions to the kvs")
+
+def load_vuln_curves_from_kvs(job_id):
+    """ Get JSON decoded vulnerability curve from kvs """
+    vulnerability_curves_mc = kvs.get_value_json_decoded(
+        kvs.tokens.vuln_key(job_id))
+
+    vulnerability_curves = {}
+
+    if vulnerability_curves_mc is not None:
+        for k, v in vulnerability_curves_mc.items():
+            vulnerability_curves["%s" % k] = shapes.Curve.from_json(v)
+
+    return vulnerability_curves
--- /dev/null
+++ openquake-1.0/src/risk/__init__.py
@@ -0,0 +1,8 @@
+"""
+Core computation methods for the risk 'engine'
+These include Classical PSHA-based risk analysis,
+and deterministic analysis based on either a set of GMF files,
+or a single GMF file."""
+
+import openquake.risk.job
+import openquake.risk.job.probabilistic
--- /dev/null
+++ openquake-1.0/src/risk/engines.py
@@ -0,0 +1,100 @@
+# -*- coding: utf-8 -*-
+"""
+Top-level managers for computation classes.
+"""
+
+from openquake import logs
+from openquake import kvs
+from openquake import shapes
+
+from openquake.parser import vulnerability
+from openquake.risk import classical_psha_based
+
+LOGGER = logs.RISK_LOG
+
+# TODO (ac): This class is not covered by unit tests...
+class ClassicalPSHABasedLossRatioCalculator(object):
+    """Computes loss ratio curves based on hazard curves and 
+    exposure portfolios"""
+
+    def __init__(self, job_id, block_id):
+        """ Prepare the calculator for computations"""
+
+        self.job_id = job_id
+        self.block_id = block_id
+
+        self.vuln_curves = \
+                vulnerability.load_vuln_curves_from_kvs(self.job_id)
+
+        # self.vuln_curves is a dict of {string: Curve}
+        LOGGER.debug("ProbabilisticLossRatioCalculator init: vuln curves are")
+
+        for k, v in self.vuln_curves.items():
+            LOGGER.debug("%s: %s" % (k, v))
+ 
+    def compute_loss_ratio_curve(self, gridpoint):
+        """ Returns the loss ratio curve for a single gridpoint"""
+
+        # check in kvs if hazard and exposure for gridpoint are there
+        kvs_key_hazard = kvs.generate_product_key(self.job_id, 
+            kvs.tokens.HAZARD_CURVE_KEY_TOKEN, self.block_id, gridpoint)
+       
+        hazard_curve_json = kvs.get_client(binary=False).get(kvs_key_hazard)
+        LOGGER.debug("hazard curve as JSON: %s" % hazard_curve_json)
+ 
+        hazard_curve = shapes.EMPTY_CURVE
+        hazard_curve.from_json(hazard_curve_json)
+
+        LOGGER.debug("hazard curve at key %s is %s" % (kvs_key_hazard, 
+            hazard_curve))
+
+        if hazard_curve is None:
+            LOGGER.debug("no hazard curve found")
+            return None
+
+        kvs_key_exposure = kvs.generate_product_key(self.job_id, 
+            kvs.tokens.EXPOSURE_KEY_TOKEN, self.block_id, gridpoint)
+        
+        asset = kvs.get_value_json_decoded(kvs_key_exposure)
+
+        LOGGER.debug("asset at key %s is %s" % (kvs_key_exposure, asset))
+
+        if asset is None:
+            LOGGER.debug("no asset found")
+            return None
+
+        LOGGER.debug("compute method: vuln curves are")
+        for k, v in self.vulnerability_curves.items(): #pylint: disable=E1101
+            LOGGER.debug("%s: %s" % (k, v.values))
+
+        #pylint: disable=E1101
+        vulnerability_curve = \
+            self.vulnerability_curves[asset['VulnerabilityFunction']]
+
+        # selected vuln function is Curve
+        return classical_psha_based.compute_loss_ratio_curve(
+            vulnerability_curve, hazard_curve)
+    
+    def compute_loss_curve(self, gridpoint, loss_ratio_curve):
+        """Return the loss curve based on loss ratio and exposure."""
+        
+        if loss_ratio_curve is None:
+            return None
+
+        kvs_key_exposure = kvs.generate_product_key(self.job_id,
+            kvs.tokens.EXPOSURE_KEY_TOKEN, self.block_id, gridpoint)
+
+        asset = kvs.get_value_json_decoded(kvs_key_exposure)
+
+        if asset is None:
+            return None
+
+        return classical_psha_based.compute_loss_curve(
+            loss_ratio_curve, asset['AssetValue'])
+
+
+def compute_loss(loss_curve, pe_interval):
+    """Interpolate loss for a specific probability of exceedance interval"""
+    loss = classical_psha_based.compute_conditional_loss(loss_curve, 
+                                                         pe_interval)
+    return loss
--- /dev/null
+++ openquake-1.0/src/risk/tasks.py
@@ -0,0 +1,9 @@
+# -*- coding: utf-8 -*-
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+"""
+Tasks in the risk engine include the following:
+
+ * Input parsing
+ * Various calculation steps
+ * Output generation
+"""
\ No newline at end of file
--- /dev/null
+++ openquake-1.0/src/risk/common.py
@@ -0,0 +1,68 @@
+# -*- coding: utf-8 -*-
+"""
+This module defines the functions that can be applied to loss ratio
+or loss curves.
+"""
+
+from numpy import mean # pylint: disable=E1101, E0611
+
+from openquake import shapes
+
+def compute_conditional_loss(curve, probability):
+    """Return the loss (or loss ratio) corresponding to the given
+    probability of exceedance. Return zero if the probability
+    of exceedance if out of bounds.
+    """
+
+    if curve.ordinate_out_of_bounds(probability):
+        return 0.0
+
+    return curve.abscissa_for(probability)
+
+
+def compute_loss_curve(loss_ratio_curve, asset):
+    """Compute the loss curve for a specific asset value."""
+
+    if not asset: 
+        return shapes.EMPTY_CURVE
+
+    return loss_ratio_curve.rescale_abscissae(asset)
+
+
+def _compute_mid_mean_pe(loss_ratio_curve):
+    """Compute a new loss ratio curve taking the mean values."""
+
+    data = []
+    loss_ratios = loss_ratio_curve.abscissae
+    pes = loss_ratio_curve.ordinates
+
+    for idx in xrange(len(loss_ratios) - 1):
+        data.append((mean([loss_ratios[idx],
+                loss_ratios[idx + 1]]),
+                mean([pes[idx], pes[idx + 1]])))
+
+    return shapes.Curve(data)
+
+
+def _compute_mid_po(loss_ratio_pe_mid_curve):
+    """Compute a loss ratio curve that has probabilities of occurrence
+    as ordinate values."""
+
+    data = []
+    loss_ratios = loss_ratio_pe_mid_curve.abscissae
+    pes = loss_ratio_pe_mid_curve.ordinates
+
+    for idx in xrange(len(loss_ratios) - 1):
+        data.append((mean([loss_ratios[idx],
+                loss_ratios[idx + 1]]), pes[idx] - pes[idx + 1]))
+
+    return  shapes.Curve(data)
+
+
+def compute_mean_loss(curve):
+    """Compute the mean loss (or loss ratio)."""
+
+    mid_pos_curve = _compute_mid_po(_compute_mid_mean_pe(curve))
+
+    return sum(i*j for i, j in zip(
+            mid_pos_curve.abscissae, mid_pos_curve.ordinates))
--- /dev/null
+++ openquake-1.0/src/risk/classical_psha_based.py
@@ -0,0 +1,146 @@
+# -*- coding: utf-8 -*-
+"""
+This module defines the functions used to compute loss ratio and loss curves
+using the classical psha based approach.
+"""
+
+from numpy import linspace # pylint: disable=F0401,E0611
+import scipy # pylint: disable=F0401
+
+from numpy import isnan # pylint: disable=F0401,E0611
+from scipy import sqrt # pylint: disable=F0401,E0611
+from scipy import stats # pylint: disable=F0401,E0611
+from scipy import log # pylint: disable=F0401,E0611
+
+from openquake import shapes
+
+STEPS_PER_INTERVAL = 5
+
+def compute_loss_ratio_curve(vuln_function, hazard_curve):
+    """Compute a loss ratio curve for a specific hazard curve (e.g., site),
+    by applying a given vulnerability function."""
+
+    if vuln_function is None:
+        vuln_function = shapes.EMPTY_CURVE
+
+    lrem = _compute_lrem(vuln_function)
+    lrem_po = _compute_lrem_po(vuln_function, lrem, hazard_curve)
+    loss_ratios = _generate_loss_ratios(vuln_function)
+    loss_ratio_curve = _compute_loss_ratio_curve_from_lrem_po(
+                            loss_ratios, lrem_po)
+
+    return loss_ratio_curve
+
+
+def _compute_lrem_po(vuln_function, lrem, hazard_curve):
+    """Compute the loss ratio * probability of occurrence matrix.""" 
+
+    current_column = 0
+    lrem_po = [None] * len(lrem)
+    imls = vuln_function.abscissae
+
+    for iml in imls:
+        prob_occ = hazard_curve.ordinate_for(iml)
+        for row in range(len(lrem_po)):
+            if not lrem_po[row]: 
+                lrem_po[row] = [None] * len(vuln_function.abscissae)
+            lrem_po[row][current_column] = lrem[row][current_column] * prob_occ
+        current_column += 1
+    
+    return lrem_po
+
+
+def _compute_loss_ratio_curve_from_lrem_po(loss_ratios, lrem_po):
+    """Compute the loss ratio curve."""
+    
+    data = []
+    for row in range(len(lrem_po)-1):
+        prob_occ = 0.0
+        for column in range(len(lrem_po[row])):
+            prob_occ += lrem_po[row][column]
+        data.append((loss_ratios[row], prob_occ))
+
+    return shapes.Curve(data)
+
+
+# @state.memoize
+def _generate_loss_ratios(vuln_function):
+    """Loss ratios are a function of the vulnerability curve."""
+
+# TODO (ac): We should always have covs, fix the caller
+# TODO (ac): Extract domain, something like vuln_function.means
+    if vuln_function.is_multi_value:
+        loss_ratios = list(vuln_function.ordinates[:, 0])
+    else:
+        loss_ratios = list(vuln_function.ordinates)
+
+    # we need to add 0.0 as first value
+    loss_ratios.insert(0, 0.0)
+    return _split_loss_ratios(loss_ratios)
+
+
+# @state.memoize
+def _compute_lrem(vuln_function, distribution=None):
+    """Compute the loss ratio exceedance matrix."""
+
+    if not distribution:
+        distribution = stats.lognorm
+        # this is so we can memoize the thing
+
+    loss_ratios = _generate_loss_ratios(vuln_function)
+
+    current_column = 0
+    lrem = [None] * (len(loss_ratios)+1)
+
+    def fix_value(prob):
+        """Fix negative probabilities for values close to zero."""
+        if isnan(prob):
+            return 0.0
+        if prob < 0.00001: 
+            return 0.0
+        else: 
+            return prob
+
+    for iml in vuln_function.abscissae:
+        mean = vuln_function.ordinate_for(iml)
+        cov = vuln_function.ordinate_for(iml, 1)
+        stddev = cov * mean
+        variance = stddev ** 2.0
+        mu = log(mean ** 2.0 / sqrt(variance + mean ** 2.0) )
+        sigma = sqrt(log((variance / mean ** 2.0) + 1.0))
+        
+        for row in range(len(loss_ratios)+1):
+            if not lrem[row]: 
+                lrem[row] = [None] * len(vuln_function.abscissae)
+            # last loss ratio is fixed to be 1
+            if row < len(loss_ratios): 
+                next_ratio = loss_ratios[row]
+            else: 
+                next_ratio = 1.0
+            
+            lrem[row][current_column] = fix_value(
+                    distribution.sf(next_ratio, 
+                    sigma, scale=scipy.exp(mu)))
+
+        current_column += 1
+
+    return lrem
+
+
+def _split_loss_ratios(loss_ratios, steps=STEPS_PER_INTERVAL):
+    """Split the loss ratios.
+
+    steps is the number of steps we make to go from one loss
+    ratio to the other. For example, if we have [1.0, 2.0]:
+
+        steps = 1 produces [1.0, 2.0]
+        steps = 2 produces [1.0, 1.5, 2.0]
+        steps = 3 produces [1.0, 1.33, 1.66, 2.0]
+    """
+
+    splitted_ratios = set()
+    for idx in range(len(loss_ratios) - 1):
+        splitted_ratios.update(linspace(
+                loss_ratios[idx], loss_ratios[idx + 1], steps + 1))
+
+    return list(sorted(splitted_ratios))
--- /dev/null
+++ openquake-1.0/src/risk/probabilistic_event_based.py
@@ -0,0 +1,176 @@
+# -*- coding: utf-8 -*-
+"""
+This module defines the functions used to compute loss ratio and loss curves
+using the probabilistic event based approach.
+"""
+
+import math
+from numpy import array # pylint: disable=E1101, E0611
+from numpy import linspace # pylint: disable=E1101, E0611
+from numpy import histogram # pylint: disable=E1101, E0611
+from numpy import where # pylint: disable=E1101, E0611
+
+from openquake import kvs
+from openquake import shapes
+from openquake.logs import LOG
+
+DEFAULT_NUMBER_OF_SAMPLES = 25
+
+def compute_loss_ratios(vuln_function, ground_motion_field_set):
+    """Compute loss ratios using the ground motion field set passed."""
+    if vuln_function == shapes.EMPTY_CURVE or not \
+                        ground_motion_field_set["IMLs"]:
+        return []
+    
+    imls = vuln_function.abscissae
+    loss_ratios = []
+    
+    # seems like with numpy you can only specify a single fill value
+    # if the x_new is outside the range. Here we need two different values,
+    # depending if the x_new is below or upon the defined values
+    for ground_motion_field in ground_motion_field_set["IMLs"]:
+        if ground_motion_field < imls[0]:
+            loss_ratios.append(0.0)
+        elif ground_motion_field > imls[-1]:
+            loss_ratios.append(imls[-1])
+        else:
+            loss_ratios.append(vuln_function.ordinate_for(
+                    ground_motion_field))
+    
+    return array(loss_ratios)
+
+
+def compute_loss_ratios_range(vuln_function):
+    """Compute the range of loss ratios used to build the loss ratio curve."""
+    loss_ratios = vuln_function.ordinates[:, 0]
+    return linspace(0.0, loss_ratios[-1], num=DEFAULT_NUMBER_OF_SAMPLES)
+
+
+def compute_cumulative_histogram(loss_ratios, loss_ratios_range):
+    "Compute the cumulative histogram."
+    
+    # TODO(JMC): I think this is wrong. where doesn't return zero values.
+    invalid_ratios = lambda ratios: len(where(array(ratios) <= 0.0)[0])
+
+    hist = histogram(loss_ratios, bins=loss_ratios_range)
+    hist = hist[0][::-1].cumsum()[::-1]
+
+    # ratios with value 0.0 must be deleted
+    hist[0] = hist[0] - invalid_ratios(loss_ratios)
+    return hist
+
+
+def compute_rates_of_exceedance(cum_histogram, tses):
+    """Compute the rates of exceedance for the given cumulative histogram
+    using the passed tses (tses is time span * number of realizations)."""
+    if tses <= 0:
+        raise ValueError("TSES is not supposed to be less than zero!")
+    
+    return (array(cum_histogram).astype(float) / tses)
+
+
+def compute_probs_of_exceedance(rates_of_exceedance, time_span):
+    """Compute the probabilities of exceedance using the given rates of
+    exceedance unsing the passed time span."""
+    probs_of_exceedance = []
+    for idx in xrange(len(rates_of_exceedance)):
+        probs_of_exceedance.append(1 - math.exp(
+                (rates_of_exceedance[idx] * -1) \
+                *  time_span))
+    
+    return array(probs_of_exceedance)
+
+
+def compute_loss_ratio_curve(vuln_function, ground_motion_field_set):
+    """Compute the loss ratio curve using the probailistic event approach."""
+
+    # with no gmfs (no earthquakes), an empty curve is enough
+    if not ground_motion_field_set["IMLs"]:
+        return shapes.EMPTY_CURVE
+
+    loss_ratios = compute_loss_ratios(vuln_function, ground_motion_field_set)
+    loss_ratios_range = compute_loss_ratios_range(vuln_function)
+
+    probs_of_exceedance = compute_probs_of_exceedance(
+            compute_rates_of_exceedance(compute_cumulative_histogram(
+            loss_ratios, loss_ratios_range), ground_motion_field_set["TSES"]),
+            ground_motion_field_set["TimeSpan"])
+
+    return _generate_curve(loss_ratios_range, probs_of_exceedance)
+
+
+def _generate_curve(losses, probs_of_exceedance):
+    """Generate a loss ratio (or loss) curve, given a set of losses
+    and corresponding probabilities of exceedance. This function
+    is intended to be used internally."""
+
+    data = []
+    for idx in xrange(len(losses) - 1):
+        mean_loss_ratios = (losses[idx] + \
+                losses[idx + 1]) / 2
+        data.append((mean_loss_ratios, probs_of_exceedance[idx]))
+
+    return shapes.Curve(data)
+
+
+class AggregateLossCurve(object):
+    """Aggregate a set of loss curves and produce the resulting loss curve."""
+
+    @staticmethod
+    def from_kvs(job_id):
+        """Return an aggregate curve using the computed
+        loss curves in the kvs system."""
+        client = kvs.get_client(binary=False)
+        keys = client.keys("%s*%s*" % (job_id,
+                kvs.tokens.LOSS_CURVE_KEY_TOKEN))
+
+        LOG.debug("Found %s stored loss curves..." % len(keys))
+
+        aggregate_curve = AggregateLossCurve()
+
+        for key in keys:
+            aggregate_curve.append(shapes.Curve.from_json(kvs.get(key)))
+        
+        return aggregate_curve
+
+    def __init__(self):
+        self.size = None
+        self.values = []
+
+    def append(self, loss_curve):
+        """Add the given loss curve to the set of curves used to 
+        compute the final aggregate curve."""
+        
+        size = len(loss_curve.abscissae)
+        
+        if self.size is None:
+            self.size = size
+        
+        if size != self.size:
+            raise ValueError("Loss curves must be of the same size, \
+                    expected %s, but was %s" % (self.size, size))
+        
+        self.values.append(loss_curve.abscissae)
+
+    @property
+    def losses(self):
+        """Return the losses used to compute the aggregate curve."""
+        result = []
+
+        if not self.values:
+            return result
+
+        return array(self.values).sum(axis=0)
+
+# TODO (ac): Test with pre computed data
+    def compute(self, tses, time_span):
+        """Compute the aggregate loss curve."""
+        losses = self.losses
+        loss_range = linspace(losses.min(), losses.max(),
+                num=DEFAULT_NUMBER_OF_SAMPLES)
+
+        probs_of_exceedance = compute_probs_of_exceedance(
+                compute_rates_of_exceedance(compute_cumulative_histogram(
+                losses, loss_range), tses), time_span)
+
+        return _generate_curve(losses, probs_of_exceedance)
--- /dev/null
+++ openquake-1.0/src/risk/job/probabilistic.py
@@ -0,0 +1,223 @@
+# pylint: disable=W0232
+
+""" Probabilistic Event Mixin: 
+
+    Defines the behaviour of a Job. Calls the compute_risk task
+
+"""
+
+import json
+
+from celery.exceptions import TimeoutError
+
+from openquake import job
+from openquake import kvs
+from openquake import logs
+from openquake import shapes
+
+from openquake.risk import common
+from openquake.risk import probabilistic_event_based
+from openquake.risk import job as risk_job
+from openquake.parser import exposure
+from openquake.parser import vulnerability
+from openquake.risk.job import output, RiskJobMixin
+
+
+LOGGER = logs.LOG
+
+DEFAULT_CONDITIONAL_LOSS_POE = 0.01
+
+def preload(fn):
+    """ Preload decorator """
+    def preloader(self, *args, **kwargs):
+        """A decorator for preload steps that must run on the Jobber"""
+
+        self.store_exposure_assets()
+        self.store_vulnerability_model()
+
+        return fn(self, *args, **kwargs)
+    return preloader
+
+
+class ProbabilisticEventMixin:
+    """ Mixin for Probalistic Event Risk Job """
+
+    @preload
+    @output
+    def execute(self):
+        """ Execute a ProbabilisticLossRatio Job """
+
+        results = []
+        tasks = []
+        for block_id in self.blocks_keys:
+            LOGGER.debug("starting task block, block_id = %s of %s" 
+                        % (block_id, len(self.blocks_keys)))
+            # pylint: disable-msg=E1101
+            tasks.append(risk_job.compute_risk.delay(self.id, block_id))
+
+        # task compute_risk has return value 'True' (writes its results to
+        # memcache).
+        for task in tasks:
+            try:
+                # TODO(chris): Figure out where to put that timeout.
+                task.wait(timeout=None)
+            except TimeoutError:
+                # TODO(jmc): Cancel and respawn this task
+                return []
+        return results # TODO(jmc): Move output from being a decorator
+
+    def slice_gmfs(self, block_id):
+        """Load and collate GMF values for all sites in this block. """
+        # TODO(JMC): Confirm this works regardless of the method of haz calc.
+        histories = int(self['NUMBER_OF_SEISMICITY_HISTORIES'])
+        realizations = int(self['NUMBER_OF_LOGIC_TREE_SAMPLES'])
+        num_ses = histories * realizations
+        
+        block = job.Block.from_kvs(block_id)
+        sites_list = block.sites
+        gmfs = {}
+        for site in sites_list:
+            risk_point = self.region.grid.point_at(site)
+            key = "%s!%s" % (risk_point.row, risk_point.column)
+            gmfs[key] = []
+            
+        for i in range(0, histories):
+            for j in range(0, realizations):
+                key = kvs.generate_product_key(
+                        self.id, kvs.tokens.STOCHASTIC_SET_TOKEN, "%s!%s" % 
+                            (i, j))
+                fieldset = shapes.FieldSet.from_json(kvs.get(key), 
+                    self.region.grid)
+
+                for field in fieldset:
+                    for key in gmfs.keys():
+                        (row, col) = key.split("!")
+                        gmfs[key].append(field.get(int(row), int(col)))
+                                        
+        for key, gmf_slice in gmfs.items():
+            (row, col) = key.split("!")
+            key_gmf = kvs.generate_product_key(self.id,
+                kvs.tokens.GMF_KEY_TOKEN, col, row)
+            LOGGER.debug( "GMF_SLICE for %s X %s : \n\t%s" % (
+                    col, row, gmf_slice ))
+            timespan = float(self['INVESTIGATION_TIME'])
+            gmf = {"IMLs": gmf_slice, "TSES": num_ses * timespan, 
+                    "TimeSpan": timespan}
+            kvs.set_value_json_encoded(key_gmf, gmf)
+
+    def store_exposure_assets(self):
+        """ Load exposure assets and write to memcache """
+        
+        exposure_parser = exposure.ExposurePortfolioFile("%s/%s" % 
+            (self.base_path, self.params[job.EXPOSURE]))
+
+        for site, asset in exposure_parser.filter(self.region):
+            # TODO(JMC): This is kludgey
+            asset['lat'] = site.latitude
+            asset['lon'] = site.longitude
+            gridpoint = self.region.grid.point_at(site)
+            asset_key = kvs.tokens.asset_key(self.id, gridpoint.row, 
+                gridpoint.column)
+            kvs.get_client().rpush(asset_key, json.JSONEncoder().encode(asset))
+
+    def store_vulnerability_model(self):
+        """ load vulnerability and write to memcache """
+        vulnerability.load_vulnerability_model(self.id,
+            "%s/%s" % (self.base_path, self.params["VULNERABILITY"]))
+    
+    def compute_risk(self, block_id, **kwargs): #pylint: disable=W0613
+        """This task computes risk for a block of sites. It requires to have
+        pre-initialized in memcache:
+         1) list of sites
+         2) gmfs
+         3) exposure portfolio (=assets)
+         4) vulnerability
+
+        TODO(fab): make conditional_loss_poe (set of probabilities of exceedance
+        for which the loss computation is done) a list of floats, and read it from
+        the job configuration.
+        """
+
+        conditional_loss_poes = [float(x) for x in self.params.get(
+                    'CONDITIONAL_LOSS_POE', "0.01").split()]
+        self.slice_gmfs(block_id)
+
+        #pylint: disable=W0201 
+        self.vuln_curves = \
+                vulnerability.load_vuln_curves_from_kvs(self.job_id)
+
+        # TODO(jmc): DONT assumes that hazard and risk grid are the same
+        block = job.Block.from_kvs(block_id)
+        
+        for point in block.grid(self.region):
+            key = kvs.generate_product_key(self.job_id, 
+                kvs.tokens.GMF_KEY_TOKEN, point.column, point.row)
+            gmf_slice = kvs.get_value_json_decoded(key)
+            
+            asset_key = kvs.tokens.asset_key(self.id, point.row, point.column)
+            asset_list = kvs.get_client().lrange(asset_key, 0, -1)
+            for asset in [json.JSONDecoder().decode(x) for x in asset_list]:
+                LOGGER.debug("processing asset %s" % (asset))
+                loss_ratio_curve = self.compute_loss_ratio_curve(
+                        point.column, point.row, asset, gmf_slice)
+                if loss_ratio_curve is not None:
+
+                    # compute loss curve
+                    loss_curve = self.compute_loss_curve(
+                            point.column, point.row, 
+                            loss_ratio_curve, asset)
+                    
+                    for loss_poe in conditional_loss_poes:
+                        self.compute_conditional_loss(point.column, point.row,
+                                loss_curve, asset, loss_poe)
+        return True
+    
+    def compute_conditional_loss(self, col, row, loss_curve, asset, loss_poe):
+        """ Compute the conditional loss for a loss curve and probability of 
+        exceedance """
+
+        loss_conditional = common.compute_conditional_loss(loss_curve, loss_poe)
+        key = kvs.tokens.loss_key(self.id, row, col, asset["AssetID"], loss_poe)
+
+        LOGGER.debug("RESULT: conditional loss is %s, write to key %s" % (
+            loss_conditional, key))
+        kvs.set(key, loss_conditional)
+
+    def compute_loss_ratio_curve(self, col, row, asset, gmf_slice ): # site_id
+        """Compute the loss ratio curve for a single site."""
+        # If the asset has a vuln function code we don't have loaded, return
+        # fail
+        vuln_function = self.vuln_curves.get(
+                asset["VulnerabilityFunction"], None)
+        if not vuln_function:
+            LOGGER.error("Unknown vulnerability function %s for asset %s"
+                % (asset["VulnerabilityFunction"], asset["AssetID"]))
+            return None
+
+        loss_ratio_curve = probabilistic_event_based.compute_loss_ratio_curve(
+                vuln_function, gmf_slice)
+        # NOTE(JMC): Early exit if the loss ratio is all zeros
+        if not False in (loss_ratio_curve.ordinates == 0.0):
+            return None
+        key = kvs.tokens.loss_ratio_key(self.id, row, col, asset["AssetID"])
+        
+        LOGGER.warn("RESULT: loss ratio curve is %s, write to key %s" % (
+                loss_ratio_curve, key))
+            
+        kvs.set(key, loss_ratio_curve.to_json())
+        return loss_ratio_curve
+
+    def compute_loss_curve(self, column, row, loss_ratio_curve, asset):
+        """Compute the loss curve for a single site."""
+        if asset is None:
+            return None
+        
+        loss_curve = loss_ratio_curve.rescale_abscissae(asset["AssetValue"])
+        key = kvs.tokens.loss_curve_key(self.id, row, column, asset["AssetID"])
+
+        LOGGER.warn("RESULT: loss curve is %s, write to key %s" % (
+                loss_curve, key))
+        kvs.set(key, loss_curve.to_json())
+        return loss_curve
+
+RiskJobMixin.register("Probabilistic Event", ProbabilisticEventMixin)
--- /dev/null
+++ openquake-1.0/src/risk/job/aggregate_loss_curve.py
@@ -0,0 +1,71 @@
+# -*- coding: utf-8 -*-
+"""
+A mixin that is able to compute and plot an aggregate loss curve.
+"""
+
+import os
+
+from openquake.logs import LOG
+from openquake.output import curve
+from openquake.risk import probabilistic_event_based as prob
+
+
+def filename(job_id):
+    """Return the name of the generated file."""
+    return "%s-aggregate-loss-curve.svg" % job_id
+
+def for_plotting(loss_curve):
+    """Translate a loss curve into a dictionary compatible to
+    the interface defined in CurvePlot.write."""
+    data = {}
+
+    data["AggregateLossCurve"] = {}
+    data["AggregateLossCurve"]["abscissa"] = tuple(loss_curve.abscissae)
+    data["AggregateLossCurve"]["ordinate"] = tuple(loss_curve.ordinates)
+    data["AggregateLossCurve"]["abscissa_property"] = "Loss"
+    data["AggregateLossCurve"]["ordinate_property"] = "PoE"
+    data["AggregateLossCurve"]["curve_title"] = "Aggregate Loss Curve"
+    
+    return data
+
+class AggregateLossCurveMixin:
+    """This class computes and plots an aggregate loss curve given a set
+    of pre computed curves stored in the underlying kvs system."""
+    
+    def __init__(self):
+        pass
+    
+    def tses(self):
+        """Return the tses parameter, using the mixed config file."""
+        histories = int(self.params["NUMBER_OF_SEISMICITY_HISTORIES"])
+        realizations = int(self.params["NUMBER_OF_LOGIC_TREE_SAMPLES"])
+        
+        num_ses = histories * realizations
+        return num_ses * self.time_span()
+
+    def time_span(self):
+        """Return the time span parameter, using the mixed config file."""
+        return float(self.params["INVESTIGATION_TIME"])
+
+    def execute(self):
+        """Execute the logic of this mixin."""
+
+        if not self.has("AGGREGATE_LOSS_CURVE"):
+            LOG.debug("AGGREGATE_LOSS_CURVE parameter not specified, " \
+                    "skipping aggregate loss curve computation...")
+
+            return []
+
+        aggregate_loss_curve = prob.AggregateLossCurve.from_kvs(self.id)
+        
+        path = os.path.join(self.base_path,
+                self.params["OUTPUT_DIR"], filename(self.id))
+
+        plotter = curve.CurvePlot(path)
+        plotter.write(for_plotting(aggregate_loss_curve.compute(
+                self.tses(), self.time_span())), autoscale_y=False)
+
+        plotter.close()
+        LOG.debug("Aggregate loss curve stored at %s" % path)
+
+        return [path] # why?
--- /dev/null
+++ openquake-1.0/src/risk/job/__init__.py
@@ -0,0 +1,120 @@
+# -*- coding: utf-8 -*-
+""" Mixin proxy for risk jobs, and associated
+Risk Job Mixin decorators """
+
+import json
+import os
+
+from openquake.output import geotiff
+from openquake import job
+from openquake.job import mixins
+from openquake import kvs 
+from openquake import logs
+from openquake import shapes
+from openquake.output import curve
+from openquake.output import risk as risk_output
+
+from openquake.risk.job.aggregate_loss_curve import AggregateLossCurveMixin
+
+from celery.decorators import task
+
+LOG = logs.LOG
+
+def output(fn):
+    """ Decorator for output """
+    def output_writer(self, *args, **kwargs):
+        """ Write the output of a block to kvs. """
+        fn(self, *args, **kwargs)
+        conditional_loss_poes = [float(x) for x in self.params.get(
+                    'CONDITIONAL_LOSS_POE', "0.01").split()]
+        #if result:
+        results = []
+        for block_id in self.blocks_keys:
+            #pylint: disable=W0212
+            results.extend(self._write_output_for_block(self.job_id, block_id))
+        for loss_poe in conditional_loss_poes:
+            results.extend(self.write_loss_map(loss_poe))
+        return results
+
+    return output_writer
+
+
+@task
+def compute_risk(job_id, block_id, **kwargs):
+    """ A task for computing risk, calls the mixed in compute_risk method """
+    engine = job.Job.from_kvs(job_id)
+    with mixins.Mixin(engine, RiskJobMixin, key="risk") as mixed:
+        mixed.compute_risk(block_id, **kwargs)
+        
+
+class RiskJobMixin(mixins.Mixin):
+    """ A mixin proxy for Risk jobs """
+    mixins = {}
+    
+    def _write_output_for_block(self, job_id, block_id):
+        """ Given a job and a block, write out a plotted curve """
+        decoder = json.JSONDecoder()
+        loss_ratio_curves = []
+        block = job.Block.from_kvs(block_id)
+        for point in block.grid(self.region):
+            asset_key = kvs.tokens.asset_key(self.id, point.row, point.column)
+            asset_list = kvs.get_client().lrange(asset_key, 0, -1)
+            for asset in [decoder.decode(x) for x in asset_list]:
+                site = shapes.Site(asset['lon'], asset['lat'])
+                key = kvs.tokens.loss_ratio_key(
+                        job_id, point.row, point.column, asset["AssetID"])
+                loss_ratio_curve = kvs.get(key)
+                if loss_ratio_curve:
+                    loss_ratio_curve = shapes.Curve.from_json(loss_ratio_curve)
+                    loss_ratio_curves.append((site, (loss_ratio_curve, asset)))
+
+        LOG.debug("Serializing loss_ratio_curves")
+        filename = "%s-block-%s.xml" % (
+            self['LOSS_CURVES_OUTPUT_PREFIX'], block_id)
+        path = os.path.join(self.base_path, self['OUTPUT_DIR'], filename)
+        output_generator = risk_output.LossRatioCurveXMLWriter(path)
+        # TODO(JMC): Take mean or max for each site
+        output_generator.serialize(loss_ratio_curves)
+        
+        filename = "%s-block-%s.svg" % (
+            self['LOSS_CURVES_OUTPUT_PREFIX'], block_id)
+        curve_path = os.path.join(self.base_path, self['OUTPUT_DIR'], filename)
+
+        plotter = curve.RiskCurvePlotter(curve_path, path, 
+            mode='loss_ratio')
+        plotter.plot(autoscale_y=False)
+        
+        results = [path]
+        results.extend(list(plotter.filenames()))
+        return results
+    
+    def write_loss_map(self, loss_poe):
+        """ Iterates through all the assets and maps losses at loss_poe """
+        decoder = json.JSONDecoder()
+        # Make a special grid at a higher resolution
+        risk_grid = shapes.Grid(self.region, float(self['RISK_CELL_SIZE']))
+        filename = "losses_at-%s.tiff" % (loss_poe)
+        path = os.path.join(self.base_path, self['OUTPUT_DIR'], filename) 
+        output_generator = geotiff.LossMapGeoTiffFile(path, risk_grid, 
+                init_value=0.0, normalize=True)
+        for point in self.region.grid:
+            asset_key = kvs.tokens.asset_key(self.id, point.row, point.column)
+            asset_list = kvs.get_client().lrange(asset_key, 0, -1)
+            for asset in [decoder.decode(x) for x in asset_list]:
+                key = kvs.tokens.loss_key(self.id, point.row, point.column, 
+                        asset["AssetID"], loss_poe)
+                loss = kvs.get(key)
+                LOG.debug("Loss for asset %s at %s %s is %s" % 
+                    (asset["AssetID"], asset['lon'], asset['lat'], loss))
+                if loss:
+                    loss_ratio = float(loss) / float(asset["AssetValue"])
+                    risk_site = shapes.Site(asset['lon'], asset['lat'])
+                    risk_point = risk_grid.point_at(risk_site)
+                    output_generator.write(
+                            (risk_point.row, risk_point.column), loss_ratio)
+        output_generator.close()
+        return [path]
+
+
+mixins.Mixin.register("Risk", RiskJobMixin, order=2)
+mixins.Mixin.register("AggregateLossCurve", AggregateLossCurveMixin, order=3)
--- /dev/null
+++ openquake-1.0/src/risk/job/classical_psha.py
@@ -0,0 +1,110 @@
+#pylint: disable-all
+""" Mixin for Classical PSHA Risk Calculation """
+
+
+class ClassicalPSHABasedMixin:
+    """ STUB STUB STUB """
+    def store_hazard_curves(self):
+        """ Get the regions from the region file and store them in kvs
+        """
+
+        # load hazard curve file and write to memcache_client
+        # TODO(JMC): Replace this with GMF slicing        
+        nrml_parser = hazard.NrmlFile("%s/%s" % (self.base_path,
+            self.params[job.HAZARD_CURVES]))
+        attribute_constraint = producer.AttributeConstraint({'IMT' : 'MMI'})
+        sites_hash_list = []
+
+        for site, hazard_curve_data in \
+            nrml_parser.filter(self.region_constraint, attribute_constraint):
+
+            gridpoint = self.region_constraint.grid.point_at(site)
+
+            # store site hashes in memcache
+            # TODO(fab): separate this from hazard curves. Regions of interest
+            # should not be taken from hazard curve input, should be 
+            # idependent from the inputs (hazard, exposure)
+            sites_hash_list.append((str(gridpoint), 
+                                   (site.longitude, site.latitude)))
+
+            hazard_curve = shapes.Curve(zip(hazard_curve_data['IML'], 
+                                                hazard_curve_data['Values']))
+
+            memcache_key_hazard = kvs.generate_product_key(self.id,
+                hazard.HAZARD_CURVE_KEY_TOKEN, self.block_id, gridpoint)
+
+            LOGGER.debug("Loading hazard curve %s at %s, %s" % (
+                        hazard_curve, site.latitude,  site.longitude))
+
+            success = self.memcache_client.set(memcache_key_hazard, 
+                hazard_curve.to_json())
+
+            if not success:
+                raise ValueError(
+                    "jobber: cannot write hazard curve to memcache")
+
+    def compute_risk(self, block_id, conditional_loss_poe=None, **kwargs):
+        """This task computes risk for a block of sites. It requires to have
+        pre-initialized in memcache:
+         1) list of sites
+         2) hazard curves
+         3) exposure portfolio (=assets)
+         4) vulnerability
+
+        TODO(fab): make conditional_loss_poe (set of probabilities of exceedance
+        for which the loss computation is done) a list of floats, and read it from
+        the job configuration.
+        """
+
+        if conditional_loss_poe is None:
+            conditional_loss_poe = DEFAULT_CONDITIONAL_LOSS_POE
+
+        risk_engine = engines.ClassicalPSHABasedLossRatioCalculator(job_id,
+            block_id)
+
+        # TODO(jmc): DONT assumes that hazard, assets, and output risk grid are
+        # the same (no nearest-neighbour search to find hazard)
+        block = job.Block.from_kvs(block_id)
+        sites_list = block.sites
+
+        LOGGER.debug("sites list for job_id %s, block_id %s:\n%s" % (
+            job_id, block_id, sites_list))
+
+        for (gridpoint, site) in sites_list:
+
+            logger.debug("processing gridpoint %s, site %s" % (gridpoint, site))
+            loss_ratio_curve = risk_engine.compute_loss_ratio_curve(gridpoint)
+
+            if loss_ratio_curve is not None:
+
+                # write to memcache: loss_ratio
+                key = kvs.generate_product_key(job_id,
+                    risk.LOSS_RATIO_CURVE_KEY_TOKEN, block_id, gridpoint)
+
+                logger.debug("RESULT: loss ratio curve is %s, write to key %s" 
+                     % (loss_ratio_curve, key))
+                memcache_client.set(key, loss_ratio_curve)
+            
+                # compute loss curve
+                loss_curve = risk_engine.compute_loss_curve(gridpoint, 
+                                                            loss_ratio_curve)
+                key = kvs.generate_product_key(job_id, 
+                    risk.LOSS_CURVE_KEY_TOKEN, block_id, gridpoint)
+
+                logger.debug("RESULT: loss curve is %s, write to key %s" % (
+                    loss_curve, key))
+                memcache_client.set(key, loss_curve)
+            
+                # compute conditional loss
+                loss_conditional = engines.compute_loss(loss_curve, 
+                                                        conditional_loss_poe)
+                key = kvs.generate_product_key(job_id, 
+                    risk.loss_token(conditional_loss_poe), block_id, gridpoint)
+
+                logger.debug("RESULT: conditional loss is %s, write to key %s"
+                    % (loss_conditional, key))
+                memcache_client.set(key, loss_conditional)
+
+        # assembling final product needs to be done by jobber, collecting the
+        # results from all tasks
+        return True
