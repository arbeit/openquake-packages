Description: Upstream changes introduced in version 0.4.3-1
 This patch has been created by dpkg-source during the package build.
 Here's the last changelog entry, hopefully it gives details on why
 those changes were made:
 .
 java-oq (0.4.3-1) natty; urgency=low
 .
   * Upstream release
 .
 The person named in the Author field signed this changelog entry.
Author: Muharem Hrnjadovic <mh@foldr3.com>

---
The information above should follow the Patch Tagging Guidelines, please
checkout http://dep.debian.net/deps/dep3/ to learn about the format. Here
are templates for supplementary fields that you might want to add:

Origin: <vendor|upstream|other>, <url of original patch>
Bug: <url in upstream bugtracker>
Bug-Debian: http://bugs.debian.org/<bugnumber>
Bug-Ubuntu: https://launchpad.net/bugs/<bugnumber>
Forwarded: <no|not-needed|url proving that it has been forwarded>
Reviewed-By: <name and email of someone who approved the patch>
Last-Update: <YYYY-MM-DD>

--- /dev/null
+++ java-oq-0.4.3/src/README
@@ -0,0 +1,4 @@
+java files go under this directory
+To run the tests, try this:
+> ant junit-tests-with-coverage
+
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/LocationListFormatter.java
@@ -0,0 +1,50 @@
+package org.gem;
+
+import org.opensha.commons.geo.Location;
+import org.opensha.commons.geo.LocationList;
+
+public class LocationListFormatter
+{
+
+    private final LocationList locations;
+
+    public LocationListFormatter(LocationList locations)
+    {
+        this.locations = locations;
+    }
+
+    public String format()
+    {
+        StringBuilder result = new StringBuilder();
+
+        for (int i = 0; i < locations.size(); i++)
+        {
+            Location current = locations.get(i);
+            result.append(format(current));
+
+            if (notLast(i))
+            {
+                result.append(", ");
+            }
+        }
+
+        return result.toString();
+    }
+
+    private boolean notLast(int index)
+    {
+        return !(index == locations.size() - 1);
+    }
+
+    private String format(Location location)
+    {
+        StringBuilder result = new StringBuilder();
+
+        result.append(location.getLongitude()).append(" ");
+        result.append(location.getLatitude()).append(" ");
+        result.append(location.getDepth());
+
+        return result.toString();
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/ScalarIntensityMeasureRelationshipApiDeserializer.java
@@ -0,0 +1,54 @@
+package org.gem;
+
+import java.lang.reflect.Constructor;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Type;
+
+import org.opensha.commons.param.event.ParameterChangeWarningEvent;
+import org.opensha.commons.param.event.ParameterChangeWarningListener;
+import org.opensha.sha.imr.ScalarIntensityMeasureRelationshipAPI;
+
+import com.google.gson.JsonDeserializationContext;
+import com.google.gson.JsonDeserializer;
+import com.google.gson.JsonElement;
+
+public class ScalarIntensityMeasureRelationshipApiDeserializer implements
+        JsonDeserializer<ScalarIntensityMeasureRelationshipAPI> {
+
+    ParameterChangeWarningEvent event = null;
+
+    @Override
+    public ScalarIntensityMeasureRelationshipAPI deserialize(JsonElement json,
+            Type typeOfT, JsonDeserializationContext context) {
+        ScalarIntensityMeasureRelationshipAPI ar = null;
+        try {
+            Class cl = Class.forName(json.getAsString());
+            Constructor cstr =
+                    cl.getConstructor(new Class[] { ParameterChangeWarningListener.class });
+            ar =
+                    (ScalarIntensityMeasureRelationshipAPI) cstr
+                            .newInstance(ParameterChangeWarningListener(event));
+        } catch (ClassNotFoundException e) {
+            e.printStackTrace();
+        } catch (SecurityException e) {
+            e.printStackTrace();
+        } catch (NoSuchMethodException e) {
+            e.printStackTrace();
+        } catch (IllegalArgumentException e) {
+            e.printStackTrace();
+        } catch (InstantiationException e) {
+            e.printStackTrace();
+        } catch (IllegalAccessException e) {
+            e.printStackTrace();
+        } catch (InvocationTargetException e) {
+            e.printStackTrace();
+        }
+        return ar;
+    }
+
+    private static ParameterChangeWarningListener
+            ParameterChangeWarningListener(ParameterChangeWarningEvent event) {
+        return null;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/PythonOutputStream.java
@@ -0,0 +1,23 @@
+package org.gem;
+
+import java.io.IOException;
+import java.io.OutputStream;
+
+public class PythonOutputStream extends OutputStream {
+    private IPythonPipe thispipe;
+    private StringBuilder buffer;
+
+    public void setPythonStdout(IPythonPipe mypipe) {
+        thispipe = mypipe;
+        buffer = new StringBuilder();
+    }
+
+    @Override
+    public void write(int arg0) throws IOException {
+        buffer.append((char) arg0);
+        if (arg0 == '\n') {
+            thispipe.write(buffer.toString());
+            buffer = new StringBuilder();
+        }
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/CalculationSettings.java
@@ -0,0 +1,334 @@
+package org.gem;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+
+import org.gem.params.CpuParams;
+import org.gem.params.DistanceParams;
+import org.gem.params.IMLListParams;
+import org.gem.params.IntensityMeasureParams;
+import org.gem.params.SiteParams;
+import org.gem.params.SourceType;
+import org.opensha.commons.calc.magScalingRelations.MagAreaRelationship;
+import org.opensha.commons.calc.magScalingRelations.magScalingRelImpl.WC1994_MagAreaRelationship;
+import org.opensha.commons.data.Site;
+import org.opensha.commons.data.TimeSpan;
+import org.opensha.commons.data.function.ArbitrarilyDiscretizedFunc;
+import org.opensha.sha.imr.attenRelImpl.BA_2008_AttenRel;
+import org.opensha.sha.imr.attenRelImpl.CB_2008_AttenRel;
+import org.opensha.sha.imr.attenRelImpl.CY_2008_AttenRel;
+import org.opensha.sha.imr.attenRelImpl.McVerryetal_2000_AttenRel;
+import org.opensha.sha.imr.param.IntensityMeasureParams.PGA_Param;
+import org.opensha.sha.imr.param.OtherParams.ComponentParam;
+import org.opensha.sha.imr.param.OtherParams.SigmaTruncLevelParam;
+import org.opensha.sha.imr.param.OtherParams.SigmaTruncTypeParam;
+import org.opensha.sha.imr.param.OtherParams.StdDevTypeParam;
+import org.opensha.sha.imr.param.SiteParams.Vs30_Param;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.GEM1ERF;
+
+public class CalculationSettings {
+
+    // ERF
+    private HashMap<SourceType, HashMap<String, Object>> Erf;
+    // GMPE
+    private HashMap<String, HashMap<String, Object>> Gmpe;
+    // Output
+    private HashMap<String, Object> Out;
+
+    // default parameters for area sources
+    private static double area_grid_spacing = 0.1;
+    private static String area_rupture_type = GEM1ERF.AREA_SRC_RUP_TYPE_POINT;
+    private static double area_lower_seismogenic_depth = 14.0;
+    private static MagAreaRelationship area_mag_area_rel =
+            new WC1994_MagAreaRelationship();
+
+    // default parameters for point sources
+    private static String point_rupture_type =
+            GEM1ERF.GRIDDED_SEIS_RUP_TYPE_POINT;
+    private static double point_lower_seismogenic_depth = 14.0;
+    private static MagAreaRelationship point_mag_area_rel =
+            new WC1994_MagAreaRelationship();
+
+    // default parameters for fault sources
+    private static double fault_grid_spacing = 1.0;
+    private static double fault_rupt_offset = 5.0;
+    private static MagAreaRelationship fault_mag_area_rel =
+            new WC1994_MagAreaRelationship();
+    private static double fault_magAreaStd = 0.0;
+    private static double fault_rupt_aspect_ratio = 1.0;
+    private static String fault_rupt_floating_type =
+            GEM1ERF.FLOATER_TYPE_ALONG_STRIKE_AND_DOWNDIP;
+
+    // default parameters for subduction fault sources
+    private static double sub_fault_grid_spacing = 10.0;
+    private static double sub_fault_rupt_offset = 10.0;
+    private static MagAreaRelationship sub_fault_mag_area_rel =
+            new WC1994_MagAreaRelationship();
+    private static double sub_fault_magAreaStd = 0.0;
+    private static double sub_fault_rupt_aspect_ratio = 1.0;
+    private static String sub_fault_rupt_floating_type =
+            GEM1ERF.SUB_FLOATER_TYPE_ALONG_STRIKE_AND_DOWNDIP;
+
+    // default parameters for all Gmpes
+    private static String intensity_meas_type = PGA_Param.NAME.toString();
+    private static String std_dev_type = StdDevTypeParam.STD_DEV_TYPE_TOTAL;
+    private static String sigma_trunc_type =
+            SigmaTruncTypeParam.SIGMA_TRUNC_TYPE_2SIDED;
+    private static double sigma_trunc_level = 3.0;
+
+    // default parameters for NGA Gmpes
+    // private static String intensity_meas_typeNGA = PGA_Param.NAME.toString();
+    // private static String std_dev_typeNGA =
+    // StdDevTypeParam.STD_DEV_TYPE_TOTAL;
+    // private static String sigma_trunc_typeNGA=
+    // SigmaTruncTypeParam.SIGMA_TRUNC_TYPE_2SIDED;
+    // private static double sigma_trunc_levelNGA = 3.0;
+    private static String componentNGA = ComponentParam.COMPONENT_GMRotI50;
+    private static double vs30NGA = 760.0;
+
+    // default parameters for McVerry et al.
+    // private static String intensity_meas_typeMcVerry =
+    // PGA_Param.NAME.toString();
+    // private static String std_dev_typeMcVerry =
+    // StdDevTypeParam.STD_DEV_TYPE_TOTAL;
+    // private static String sigma_trunc_typeMcVerry=
+    // SigmaTruncTypeParam.SIGMA_TRUNC_TYPE_2SIDED;
+    // private static double sigma_trunc_levelMcVerry = 3.0;
+    private static String componentMcVerry = ComponentParam.COMPONENT_AVE_HORZ;
+
+    // Zhao et al.
+    private static String componentZhao = ComponentParam.COMPONENT_AVE_HORZ;
+
+    // Atkinson and Boore 2006
+    private static String componentAtkBoo = ComponentParam.COMPONENT_GMRotI50;
+    private static double vs30AtkBoo = 760.0;
+
+    // intensity measure level list
+    private ArbitrarilyDiscretizedFunc imlList;
+
+    // default time span duration
+    private static double TimeSpanDuration = 50.0;
+
+    // default number of cpus to be used for calculation
+    private static int ncpu = 1;
+
+    // default minimum distance to source
+    private static double max_dist_source = 200.0;
+
+    // boolean to cache sources
+    private static boolean sourceCache = false;
+
+    public static ArbitrarilyDiscretizedFunc getDefaultIMLVals() {
+        ArbitrarilyDiscretizedFunc imlList = new ArbitrarilyDiscretizedFunc();
+        imlList.set(0.005, 1.0);
+        imlList.set(0.007, 1.0);
+        imlList.set(0.0098, 1.0);
+        imlList.set(0.0137, 1.0);
+        imlList.set(0.0192, 1.0);
+        imlList.set(0.0269, 1.0);
+        imlList.set(0.0376, 1.0);
+        imlList.set(0.0527, 1.0);
+        imlList.set(0.0738, 1.0);
+        imlList.set(0.103, 1.0);
+        imlList.set(0.145, 1.0);
+        imlList.set(0.203, 1.0);
+        imlList.set(0.284, 1.0);
+        imlList.set(0.397, 1.0);
+        imlList.set(0.556, 1.0);
+        imlList.set(0.778, 1.0);
+        imlList.set(1.09, 1.0);
+        imlList.set(1.52, 1.0);
+        imlList.set(2.13, 1.0);
+        return imlList;
+    }
+
+    public static ArbitrarilyDiscretizedFunc getDefaultLogIMLVals() {
+        ArbitrarilyDiscretizedFunc imlList = getDefaultIMLVals();
+        ArbitrarilyDiscretizedFunc imlLogList =
+                new ArbitrarilyDiscretizedFunc();
+        for (int i = 0; i < imlList.getNum(); i++) {
+            imlLogList.set(Math.log(imlList.getX(i)), 1.0);
+        }
+        return imlLogList;
+    }
+
+    // the constructor set default values
+    public CalculationSettings() {
+
+        Erf = new HashMap<SourceType, HashMap<String, Object>>();
+        Gmpe = new HashMap<String, HashMap<String, Object>>();
+        Out = new HashMap<String, Object>();
+
+        // set intentity measure level list
+        imlList = getDefaultLogIMLVals();
+
+        // *********** calculation settings for ERF instantiation **********//
+
+        // hashmap for area sources
+        HashMap<String, Object> areaSourceCalcSet =
+                new HashMap<String, Object>();
+        // source modeling type
+        areaSourceCalcSet
+                .put(GEM1ERF.AREA_SRC_RUP_TYPE_NAME, area_rupture_type);
+        // lower seismogenic depth
+        areaSourceCalcSet.put(GEM1ERF.AREA_SRC_LOWER_SEIS_DEPTH_PARAM_NAME,
+                area_lower_seismogenic_depth);
+        // source discretization
+        areaSourceCalcSet.put(GEM1ERF.AREA_SRC_DISCR_PARAM_NAME,
+                area_grid_spacing);
+        // magnitude scaling relationship
+        areaSourceCalcSet.put(GEM1ERF.AREA_SRC_MAG_SCALING_REL_PARAM_NAME,
+                area_mag_area_rel.getName());
+        Erf.put(SourceType.AREA_SOURCE, areaSourceCalcSet);
+
+        // hashmap for point sources
+        HashMap<String, Object> gridSourceCalcSet =
+                new HashMap<String, Object>();
+        // source modeling type
+        gridSourceCalcSet.put(GEM1ERF.GRIDDED_SEIS_RUP_TYPE_NAME,
+                point_rupture_type);
+        // lower seismogenic depth
+        gridSourceCalcSet.put(GEM1ERF.GRIDDED_SEIS_LOWER_SEIS_DEPTH_PARAM_NAME,
+                point_lower_seismogenic_depth);
+        // mag scaling relationship
+        gridSourceCalcSet.put(GEM1ERF.GRIDDED_SEIS_MAG_SCALING_REL_PARAM_NAME,
+                point_mag_area_rel.getName());
+        Erf.put(SourceType.GRID_SOURCE, gridSourceCalcSet);
+
+        // hashmap for fault source calculation settings
+        HashMap<String, Object> faultSourceCalcSet =
+                new HashMap<String, Object>();
+        // rupture offset
+        faultSourceCalcSet.put(GEM1ERF.FAULT_RUP_OFFSET_PARAM_NAME,
+                fault_rupt_offset);
+        // fault discretization
+        faultSourceCalcSet.put(GEM1ERF.FAULT_DISCR_PARAM_NAME,
+                fault_grid_spacing);
+        // mag scaling relationship
+        faultSourceCalcSet.put(GEM1ERF.FAULT_MAG_SCALING_REL_PARAM_NAME,
+                fault_mag_area_rel.getName());
+        // standard deviation
+        faultSourceCalcSet.put(GEM1ERF.FAULT_SCALING_SIGMA_PARAM_NAME,
+                fault_magAreaStd);
+        // rupture aspect ratio
+        faultSourceCalcSet.put(GEM1ERF.FAULT_RUP_ASPECT_RATIO_PARAM_NAME,
+                fault_rupt_aspect_ratio);
+        // rupture floating type
+        faultSourceCalcSet.put(GEM1ERF.FAULT_FLOATER_TYPE_PARAM_NAME,
+                fault_rupt_floating_type);
+        Erf.put(SourceType.FAULT_SOURCE, faultSourceCalcSet);
+
+        // hashmap for fault source calculation settings
+        HashMap<String, Object> subFaultSourceCalcSet =
+                new HashMap<String, Object>();
+        // rupture offset
+        subFaultSourceCalcSet.put(GEM1ERF.SUB_RUP_OFFSET_PARAM_NAME,
+                sub_fault_rupt_offset);
+        // fault discretization
+        subFaultSourceCalcSet.put(GEM1ERF.SUB_DISCR_PARAM_NAME,
+                sub_fault_grid_spacing);
+        // mag scaling relationship
+        subFaultSourceCalcSet.put(GEM1ERF.SUB_MAG_SCALING_REL_PARAM_NAME,
+                sub_fault_mag_area_rel.getName());
+        // standard deviation
+        subFaultSourceCalcSet.put(GEM1ERF.SUB_SCALING_SIGMA_PARAM_NAME,
+                sub_fault_magAreaStd);
+        // rupture aspect ratio
+        subFaultSourceCalcSet.put(GEM1ERF.SUB_RUP_ASPECT_RATIO_PARAM_NAME,
+                sub_fault_rupt_aspect_ratio);
+        // rupture floating type
+        subFaultSourceCalcSet.put(GEM1ERF.SUB_FLOATER_TYPE_PARAM_NAME,
+                sub_fault_rupt_floating_type);
+        Erf.put(SourceType.SUBDUCTION_FAULT_SOURCE, subFaultSourceCalcSet);
+
+        // ********* calculation settings for output ********//
+
+        Out.put(TimeSpan.DURATION, TimeSpanDuration);
+        Out.put(IMLListParams.IML_LIST.toString(), imlList);
+        Out.put(SiteParams.SITE_LIST.toString(), new ArrayList<Site>());
+        Out.put(DistanceParams.MAX_DIST_SOURCE.toString(), max_dist_source);
+        Out.put(CpuParams.CPU_NUMBER.toString(), ncpu);
+
+        Out.put(IntensityMeasureParams.INTENSITY_MEAS_TYPE.toString(),
+                intensity_meas_type);
+        Out.put(StdDevTypeParam.NAME, std_dev_type);
+        Out.put(SigmaTruncTypeParam.NAME, sigma_trunc_type);
+        Out.put(SigmaTruncLevelParam.NAME, sigma_trunc_level);
+
+        // ********* calculation settings for GMPE instantiation ***********//
+        // calculation parameters for NGA relationships: B&A, C&B, C&Y 2008
+        HashMap<String, Object> gmpeCalcSetNGA = new HashMap<String, Object>();
+        // gmpeCalcSetNGA.put(IntensityMeasureParams.INTENSITY_MEAS_TYPE.toString(),
+        // intensity_meas_typeNGA);
+        // gmpeCalcSetNGA.put(StdDevTypeParam.NAME, std_dev_typeNGA);
+        // gmpeCalcSetNGA.put(SigmaTruncTypeParam.NAME,sigma_trunc_typeNGA);
+        // gmpeCalcSetNGA.put(SigmaTruncLevelParam.NAME, sigma_trunc_levelNGA);
+        gmpeCalcSetNGA.put(ComponentParam.NAME, componentNGA);
+        gmpeCalcSetNGA.put(Vs30_Param.NAME, vs30NGA);
+        // B&A 2008
+        Gmpe.put(BA_2008_AttenRel.SHORT_NAME, gmpeCalcSetNGA);
+        // C&B 2008
+        Gmpe.put(CB_2008_AttenRel.SHORT_NAME, gmpeCalcSetNGA);
+        // C&Y 2008
+        Gmpe.put(CY_2008_AttenRel.SHORT_NAME, gmpeCalcSetNGA);
+
+        // calculation parameters for McVerry et al. 2000
+        HashMap<String, Object> gmpeCalcSetMcVerry2000 =
+                new HashMap<String, Object>();
+        // gmpeCalcSetMcVerry2000.put(IntensityMeasureParams.INTENSITY_MEAS_TYPE.toString(),
+        // intensity_meas_typeMcVerry);
+        // gmpeCalcSetMcVerry2000.put(StdDevTypeParam.NAME,
+        // std_dev_typeMcVerry);
+        // gmpeCalcSetMcVerry2000.put(SigmaTruncTypeParam.NAME,sigma_trunc_typeMcVerry);
+        // gmpeCalcSetMcVerry2000.put(SigmaTruncLevelParam.NAME,
+        // sigma_trunc_levelMcVerry);
+        gmpeCalcSetMcVerry2000.put(ComponentParam.NAME, componentMcVerry);
+        // McVerry 2000 et al.
+        Gmpe.put(McVerryetal_2000_AttenRel.SHORT_NAME, gmpeCalcSetMcVerry2000);
+
+        // Calculation Parameters for Zhao et al. (2000)
+//        HashMap<String, Object> gmpeCalcSetZhao2000 =
+//                new HashMap<String, Object>();
+//        gmpeCalcSetZhao2000.put(ComponentParam.NAME, componentZhao);
+//        Gmpe.put(ZhaoEtAl_2006_AttenRel.SHORT_NAME, gmpeCalcSetZhao2000);
+//
+//        // Calculation Parameters for Atkinson and Boore 2006
+//        HashMap<String, Object> gmpeCalcSetAB2006 =
+//                new HashMap<String, Object>();
+//        gmpeCalcSetAB2006.put(ComponentParam.NAME, componentAtkBoo);
+//        gmpeCalcSetAB2006.put(Vs30_Param.NAME, vs30AtkBoo);
+//        Gmpe.put(AtkBoo_2006_AttenRel.SHORT_NAME, gmpeCalcSetAB2006);
+    }
+
+    public HashMap<SourceType, HashMap<String, Object>> getErf() {
+        return Erf;
+    }
+
+    public void setErf(HashMap<SourceType, HashMap<String, Object>> erf) {
+        Erf = erf;
+    }
+
+    public HashMap<String, HashMap<String, Object>> getGmpe() {
+        return Gmpe;
+    }
+
+    public void setGmpe(HashMap<String, HashMap<String, Object>> gmpe) {
+        Gmpe = gmpe;
+    }
+
+    public HashMap<String, Object> getOut() {
+        return Out;
+    }
+
+    public void setOut(HashMap<String, Object> out) {
+        Out = out;
+    }
+
+    public boolean isSourceCache() {
+        return sourceCache;
+    }
+
+    public void setSourceCache(boolean cache) {
+        sourceCache = cache;
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/IPythonPipe.java
@@ -0,0 +1,7 @@
+package org.gem;
+
+public interface IPythonPipe {
+    // public void write(char output);
+
+    public void write(String output);
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/UnoptimizedDeepCopy.java
@@ -0,0 +1,49 @@
+package org.gem;
+
+import java.io.IOException;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectOutputStream;
+import java.io.ObjectInputStream;
+
+/**
+ * Utility for making deep copies (vs. clone()'s shallow copies) of objects.
+ * Objects are first serialized and then deserialized. Error checking is fairly
+ * minimal in this implementation. If an object is encountered that cannot be
+ * serialized (or that references an object that cannot be serialized) an error
+ * is printed to System.err and null is returned. Depending on your specific
+ * application, it might make more sense to have copy(...) re-throw the
+ * exception.
+ * 
+ * A later version of this class includes some minor optimizations.
+ */
+public class UnoptimizedDeepCopy {
+
+    /**
+     * Returns a copy of the object, or null if the object cannot be serialized.
+     */
+    public static Object copy(Object orig) {
+        Object obj = null;
+        try {
+            // Write the object out to a byte array
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            ObjectOutputStream out = new ObjectOutputStream(bos);
+            out.writeObject(orig);
+            out.flush();
+            out.close();
+
+            // Make an input stream from the byte array and read
+            // a copy of the object back in.
+            ObjectInputStream in =
+                    new ObjectInputStream(new ByteArrayInputStream(
+                            bos.toByteArray()));
+            obj = in.readObject();
+        } catch (IOException e) {
+            e.printStackTrace();
+        } catch (ClassNotFoundException cnfe) {
+            cnfe.printStackTrace();
+        }
+        return obj;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/TectonicRegionTypeDeserializer.java
@@ -0,0 +1,21 @@
+package org.gem;
+
+import java.lang.reflect.Type;
+
+import org.opensha.sha.util.TectonicRegionType;
+
+import com.google.gson.JsonDeserializationContext;
+import com.google.gson.JsonDeserializer;
+import com.google.gson.JsonElement;
+import com.google.gson.JsonParseException;
+
+public class TectonicRegionTypeDeserializer implements
+        JsonDeserializer<TectonicRegionType> {
+
+    @Override
+    public TectonicRegionType deserialize(JsonElement arg0, Type arg1,
+            JsonDeserializationContext arg2) throws JsonParseException {
+        return TectonicRegionType.getTypeForName(arg0.getAsString());
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/SourceDataDeserializer.java
@@ -0,0 +1,233 @@
+package org.gem;
+
+import java.lang.reflect.Type;
+
+import org.opensha.commons.data.function.ArbitrarilyDiscretizedFunc;
+import org.opensha.commons.geo.BorderType;
+import org.opensha.commons.geo.Location;
+import org.opensha.commons.geo.LocationList;
+import org.opensha.commons.geo.Region;
+import org.opensha.sha.earthquake.FocalMechanism;
+import org.opensha.sha.earthquake.griddedForecast.HypoMagFreqDistAtLoc;
+import org.opensha.sha.earthquake.griddedForecast.MagFreqDistsForFocalMechs;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMAreaSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMFaultSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMPointSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMSubductionFaultSourceData;
+import org.opensha.sha.faultSurface.FaultTrace;
+import org.opensha.sha.magdist.IncrementalMagFreqDist;
+import org.opensha.sha.util.TectonicRegionType;
+
+import com.google.gson.JsonArray;
+import com.google.gson.JsonDeserializationContext;
+import com.google.gson.JsonDeserializer;
+import com.google.gson.JsonElement;
+import com.google.gson.JsonObject;
+import com.google.gson.JsonParseException;
+
+public class SourceDataDeserializer implements JsonDeserializer<GEMSourceData> {
+
+    @Override
+    public GEMSourceData deserialize(JsonElement arg0, Type arg1,
+            JsonDeserializationContext arg2) throws JsonParseException {
+        GEMSourceData sourceData = null;
+        JsonObject obj = arg0.getAsJsonObject();
+        // common data
+        String id = obj.get("id").toString().replace("\"", "");
+        String name = obj.get("name").toString().replace("\"", "");
+        String trtName = obj.get("tectReg").toString().replace("\"", "");
+        TectonicRegionType trt = getTectonicRegionType(trtName);
+
+        if (obj.has("reg")) { // area source
+            Region reg = getRegion(obj);
+            MagFreqDistsForFocalMechs magfreqDistFocMech =
+                    getMagFreqDistsForFocalMechs(obj);
+            ArbitrarilyDiscretizedFunc aveRupTopVsMag = getAveRupTopVsMag(obj);
+            double aveHypoDepth = obj.get("aveHypoDepth").getAsDouble();
+            sourceData =
+                    new GEMAreaSourceData(id, name, trt, reg,
+                            magfreqDistFocMech, aveRupTopVsMag, aveHypoDepth);
+        } else if (obj.has("hypoMagFreqDistAtLoc")) { // point source
+            HypoMagFreqDistAtLoc hypoMagFreqDistAtLoc =
+                    getHypoMagFreqDistAtLoc(obj);
+            ArbitrarilyDiscretizedFunc aveRupTopVsMag = getAveRupTopVsMag(obj);
+            double aveHypoDepth = obj.get("aveHypoDepth").getAsDouble();
+            sourceData =
+                    new GEMPointSourceData(id, name, trt, hypoMagFreqDistAtLoc,
+                            aveRupTopVsMag, aveHypoDepth);
+        }
+        if (obj.has("trace")) { // fault source
+            JsonArray faultTrace = obj.get("trace").getAsJsonArray();
+            FaultTrace trace = getFaultTrace(faultTrace);
+            double dip = obj.get("dip").getAsDouble();
+            double rake = obj.get("rake").getAsDouble();
+            double seismDepthLow = obj.get("seismDepthLow").getAsDouble();
+            double seismDepthUpp = obj.get("seismDepthUpp").getAsDouble();
+            IncrementalMagFreqDist mfd =
+                    getMagFreqDist(obj.get("mfd").getAsJsonObject());
+            Boolean floatRuptureFlag =
+                    obj.get("floatRuptureFlag").getAsBoolean();
+            sourceData =
+                    new GEMFaultSourceData(id, name, trt, mfd, trace, dip,
+                            rake, seismDepthLow, seismDepthUpp,
+                            floatRuptureFlag);
+        }
+        if (obj.has("topTrace")) { // subduction source
+            JsonArray faultTrace = obj.get("topTrace").getAsJsonArray();
+            FaultTrace topTrace = getFaultTrace(faultTrace);
+            faultTrace = obj.get("bottomTrace").getAsJsonArray();
+            FaultTrace bottomTrace = getFaultTrace(faultTrace);
+            double rake = obj.get("rake").getAsDouble();
+            IncrementalMagFreqDist mfd =
+                    getMagFreqDist(obj.get("mfd").getAsJsonObject());
+            Boolean floatRuptureFlag =
+                    obj.get("floatRuptureFlag").getAsBoolean();
+            sourceData =
+                    new GEMSubductionFaultSourceData(id, name, trt, topTrace,
+                            bottomTrace, rake, mfd, floatRuptureFlag);
+        }
+        return sourceData;
+    }
+
+    private FaultTrace getFaultTrace(JsonArray faultTrace) {
+        FaultTrace trace = new FaultTrace("");
+        for (int i = 0; i < faultTrace.size(); i++) {
+            Location loc =
+                    locationFromJson(faultTrace.get(i).getAsJsonObject());
+            trace.add(loc);
+        }
+        return trace;
+    }
+
+    private Location locationFromJson(JsonObject obj) {
+        return new Location(obj.get("lat").getAsDouble() * (180 / Math.PI), obj
+                .get("lon").getAsDouble() * (180 / Math.PI), obj.get("depth")
+                .getAsDouble());
+    }
+
+    private HypoMagFreqDistAtLoc getHypoMagFreqDistAtLoc(JsonObject obj) {
+        HypoMagFreqDistAtLoc hypoMagFreqDistAtLoc = null;
+        JsonObject location =
+                obj.get("hypoMagFreqDistAtLoc").getAsJsonObject()
+                        .get("location").getAsJsonObject();
+        Location loc = locationFromJson(location);
+        JsonArray mfdArray =
+                obj.get("hypoMagFreqDistAtLoc").getAsJsonObject()
+                        .get("magFreqDist").getAsJsonArray();
+        JsonArray fmArray =
+                obj.get("hypoMagFreqDistAtLoc").getAsJsonObject()
+                        .get("focalMechanism").getAsJsonArray();
+        IncrementalMagFreqDist[] magFreqDistArray =
+                new IncrementalMagFreqDist[mfdArray.size()];
+        FocalMechanism[] focMechArray = new FocalMechanism[mfdArray.size()];
+        for (int i = 0; i < mfdArray.size(); i++) {
+            JsonObject mfd = mfdArray.get(i).getAsJsonObject();
+            JsonObject fm = fmArray.get(i).getAsJsonObject();
+            magFreqDistArray[i] = getMagFreqDist(mfd);
+            focMechArray[i] = getFocalMechanism(fm);
+        }
+        hypoMagFreqDistAtLoc =
+                new HypoMagFreqDistAtLoc(magFreqDistArray, loc, focMechArray);
+        return hypoMagFreqDistAtLoc;
+    }
+
+    private ArbitrarilyDiscretizedFunc getAveRupTopVsMag(JsonObject obj) {
+        ArbitrarilyDiscretizedFunc aveRupTopVsMag =
+                new ArbitrarilyDiscretizedFunc();
+        JsonArray aveRupTopDepthVsMag =
+                obj.get("aveRupTopVsMag").getAsJsonArray();
+        for (int i = 0; i < aveRupTopDepthVsMag.size(); i++) {
+            double x =
+                    aveRupTopDepthVsMag.get(i).getAsJsonArray().get(0)
+                            .getAsDouble();
+            double y =
+                    aveRupTopDepthVsMag.get(i).getAsJsonArray().get(1)
+                            .getAsDouble();
+            aveRupTopVsMag.set(x, y);
+        }
+        return aveRupTopVsMag;
+    }
+
+    private MagFreqDistsForFocalMechs getMagFreqDistsForFocalMechs(
+            JsonObject obj) {
+        JsonArray mfdList =
+                obj.get("magfreqDistFocMech").getAsJsonObject()
+                        .get("magFreqDist").getAsJsonArray();
+        JsonArray fmList =
+                obj.get("magfreqDistFocMech").getAsJsonObject()
+                        .get("focalMechanism").getAsJsonArray();
+        JsonObject mfd = null;
+        JsonObject fm = null;
+        IncrementalMagFreqDist[] mfdArray =
+                new IncrementalMagFreqDist[mfdList.size()];
+        FocalMechanism[] fmArray = new FocalMechanism[mfdList.size()];
+        for (int i = 0; i < mfdList.size(); i++) {
+            // magnitude frequency distribution
+            mfd = mfdList.get(i).getAsJsonObject();
+            IncrementalMagFreqDist magFreqDist = getMagFreqDist(mfd);
+            mfdArray[i] = magFreqDist;
+            // focal mechanism
+            fm = fmList.get(i).getAsJsonObject();
+            FocalMechanism focMech = getFocalMechanism(fm);
+            fmArray[i] = focMech;
+        }
+        MagFreqDistsForFocalMechs magfreqDistFocMech =
+                new MagFreqDistsForFocalMechs(mfdArray, fmArray);
+        return magfreqDistFocMech;
+    }
+
+    private FocalMechanism getFocalMechanism(JsonObject fm) {
+        double strike = fm.get("strike").getAsDouble();
+        double dip = fm.get("dip").getAsDouble();
+        double rake = fm.get("rake").getAsDouble();
+        FocalMechanism focMech = new FocalMechanism(strike, dip, rake);
+        return focMech;
+    }
+
+    private IncrementalMagFreqDist getMagFreqDist(JsonObject mfd) {
+        double minX = mfd.get("minX").getAsDouble();
+        double maxX = mfd.get("maxX").getAsDouble();
+        int num = mfd.get("num").getAsInt();
+        JsonArray mfdValues = mfd.get("points").getAsJsonArray();
+        IncrementalMagFreqDist magFreqDist =
+                new IncrementalMagFreqDist(minX, maxX, num);
+        for (int j = 0; j < mfdValues.size(); j++) {
+            magFreqDist.set(j, mfdValues.get(j).getAsDouble());
+        }
+        return magFreqDist;
+    }
+
+    private Region getRegion(JsonObject obj) {
+        JsonArray border =
+                obj.get("reg").getAsJsonObject().get("border").getAsJsonArray();
+        LocationList borderLocs = new LocationList();
+        for (int i = 0; i < border.size(); i++) {
+            Location loc =
+                    new Location(border.get(i).getAsJsonObject().get("lat")
+                            .getAsDouble()
+                            * (180 / Math.PI), border.get(i).getAsJsonObject()
+                            .get("lon").getAsDouble()
+                            * (180 / Math.PI));
+            borderLocs.add(loc);
+        }
+        Region reg = new Region(borderLocs, BorderType.MERCATOR_LINEAR);
+        return reg;
+    }
+
+    private TectonicRegionType getTectonicRegionType(String trtName) {
+        TectonicRegionType trt = null;
+        if (trtName.equalsIgnoreCase("ACTIVE_SHALLOW")) {
+            trt = TectonicRegionType.ACTIVE_SHALLOW;
+        } else if (trtName.equalsIgnoreCase("STABLE_SHALLOW")) {
+            trt = TectonicRegionType.STABLE_SHALLOW;
+        } else if (trtName.equalsIgnoreCase("SUBDUCTION_INTERFACE")) {
+            trt = TectonicRegionType.SUBDUCTION_INTERFACE;
+        } else if (trtName.equalsIgnoreCase("SUBDUCTION_SLAB")) {
+            trt = TectonicRegionType.SUBDUCTION_SLAB;
+        } else if (trtName.equalsIgnoreCase("VOLCANIC")) {
+            trt = TectonicRegionType.VOLCANIC;
+        }
+        return trt;
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/ScalarIMRJsonAdapter.java
@@ -0,0 +1,21 @@
+package org.gem;
+
+import java.lang.reflect.Type;
+
+import org.opensha.sha.imr.ScalarIntensityMeasureRelationshipAPI;
+
+import com.google.gson.JsonElement;
+import com.google.gson.JsonPrimitive;
+import com.google.gson.JsonSerializationContext;
+import com.google.gson.JsonSerializer;
+
+public class ScalarIMRJsonAdapter implements
+        JsonSerializer<ScalarIntensityMeasureRelationshipAPI> {
+
+    @Override
+    public JsonElement serialize(ScalarIntensityMeasureRelationshipAPI src,
+            Type typeOfSrc, JsonSerializationContext context) {
+        return new JsonPrimitive(src.getClass().getCanonicalName());
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/JsonSerializer.java
@@ -0,0 +1,144 @@
+package org.gem;
+
+import java.lang.reflect.Type;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+
+import org.gem.engine.hazard.redis.Cache;
+import org.opensha.commons.data.DataPoint2D;
+import org.opensha.commons.data.Site;
+import org.opensha.commons.data.function.DiscretizedFuncAPI;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMSourceData;
+import org.opensha.sha.imr.ScalarIntensityMeasureRelationshipAPI;
+import org.opensha.sha.util.TectonicRegionType;
+
+import com.google.gson.Gson;
+import com.google.gson.GsonBuilder;
+import com.google.gson.JsonElement;
+import com.google.gson.JsonObject;
+import com.google.gson.reflect.TypeToken;
+
+public class JsonSerializer {
+
+    /* Hazard Curve to JSON stuff */
+
+    /**
+     * Type definition for converting a hazard curve to JSON.
+     */
+    private static final Type CURVE_TYPE =
+            new TypeToken<List<Double>>() {
+            }.getType();
+
+    /**
+     * Type definition for converting a site to JSON.
+     */
+    private static final Type SITE_TYPE = new TypeToken<Map<String, String>>() {
+    }.getType();
+
+    private static final String SITE_LON = "site_lon";
+    private static final String SITE_LAT = "site_lat";
+    private static final String X = "x";
+    private static final String Y = "y";
+    private static final String POES = "poes";
+
+    /* End Hazard Curve to JSON stuff */
+
+    /**
+     * Serializes and array list of GEMSourceData
+     *
+     * @param sourceList
+     * @return
+     */
+    public static String getJsonSourceList(ArrayList<GEMSourceData> sourceList) {
+        String json = new Gson().toJson(sourceList);
+        return json;
+    }
+
+    public static void serializeSourceList(Cache cache, String key,
+            ArrayList<GEMSourceData> sources) {
+        cache.set(key, getJsonSourceList(sources));
+    }
+
+    public static List<GEMSourceData> getSourceListFromCache(Cache cache,
+            String key) {
+        GsonBuilder gson = new GsonBuilder();
+        gson.registerTypeAdapter(GEMSourceData.class,
+                new SourceDataDeserializer());
+        Type listType = new TypeToken<ArrayList<GEMSourceData>>() {
+        }.getType();
+        // At least up to gson 1.6 what we get is a LinkedList<GEMSourceData>
+        // while GEM1ERF.GEM1ERF is expecting ArrayList<GEMSourceData>.
+        List<GEMSourceData> result =
+            gson.create().fromJson((String) cache.get(key), listType);
+        return new ArrayList<GEMSourceData>(result);
+    }
+
+    public static HashMap<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> getGmpeMapFromCache(
+            Cache cache, String key) {
+
+        GsonBuilder gson = new GsonBuilder();
+        gson.registerTypeAdapter(ScalarIntensityMeasureRelationshipAPI.class,
+                new ScalarIntensityMeasureRelationshipApiDeserializer());
+        gson.registerTypeAdapter(TectonicRegionType.class,
+                new TectonicRegionTypeDeserializer());
+
+        Type hashType =
+                new TypeToken<HashMap<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI>>() {
+                }.getType();
+
+        return gson.create().fromJson((String) cache.get(key), hashType);
+    }
+
+    public static void serializeConfigurationFile(Cache cache, String key,
+            Properties configProperties) {
+        String json = new Gson().toJson(configProperties, Properties.class);
+        cache.set(key, json);
+    }
+
+    public static Properties getConfigurationPropertiesFromCache(Cache cache,
+            String key) {
+        return new Gson().fromJson((String) cache.get(key), Properties.class);
+    }
+
+    /**
+     * Convert the input Map into a List of JSON Strings.
+     *
+     * <p>
+     * <b>The order in which the results are returned is based on the order of
+     * the site list.</b>
+     * </p>
+     *
+     * @param hazCurves
+     * @return List of JSON Strings
+     */
+    public static List<String> hazardCurvesToJson(
+            Map<Site, DiscretizedFuncAPI> hazCurves, List<Site> siteList) {
+        List<String> result = new ArrayList<String>();
+        Gson gson = new Gson();
+        for (Site site : siteList) {
+            result.add(ordinatesToJsonElement(hazCurves.get(site), gson).toString());
+        }
+        return result;
+    }
+
+    /**
+     * Convert a hazard curve to a JSON list of ordinates.
+     *
+     * @param func
+     * @return
+     */
+    public static JsonElement ordinatesToJsonElement(DiscretizedFuncAPI func,
+            Gson gson) {
+        List<Double> curve = new ArrayList<Double>();
+        Iterator<DataPoint2D> ptIter = func.getPointsIterator();
+        while (ptIter.hasNext()) {
+            DataPoint2D point = ptIter.next();
+            curve.add(point.getY());
+        }
+        return gson.toJsonTree(curve, CURVE_TYPE);
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/calc/ProbabilityMassFunctionCalc.java
@@ -0,0 +1,187 @@
+package org.gem.calc;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.ListIterator;
+
+import org.opensha.commons.data.function.ArbitrarilyDiscretizedFunc;
+import org.opensha.commons.data.function.DiscretizedFunc;
+import org.opensha.commons.data.function.DiscretizedFuncAPI;
+import org.opensha.commons.data.function.EvenlyDiscretizedFunc;
+
+/**
+ * This class provide methods for calculating a probability mass function (PMF)
+ * from a probability of exceedance function (POE). The class is declared final
+ * so that it can be made inline and therefore be faster when executing. This
+ * can be important because this class can be called many times when calculating
+ * PMFs for many hazard curves.
+ * 
+ * @author damianomonelli
+ * 
+ */
+
+public final class ProbabilityMassFunctionCalc {
+    /**
+     * Return the appropriate type of PMF, based on the type of the input POE
+     * (even or arbitrarily discretized).
+     */
+    public static DiscretizedFunc getPMF(DiscretizedFunc poe) {
+        if (poe instanceof EvenlyDiscretizedFunc) {
+            return getPMF((EvenlyDiscretizedFunc) poe);
+        }
+        if (poe instanceof ArbitrarilyDiscretizedFunc) {
+            return getPMF((ArbitrarilyDiscretizedFunc) poe);
+        }
+        throw new IllegalArgumentException(
+                "Poe must be either evenly or arbitrarily discretized");
+    }
+
+    /**
+     * This method compute a PMF from a POE function assuming both to be evenly
+     * discretized.
+     * 
+     * @param poe
+     *            EvenlyDiscretizedFunc POE function
+     * @return EvenlyDiscretizedFunc PMF. PMF values refer to middle points of
+     *         POEs bins.
+     * @exception IllegalArgumentException
+     *                poe is null, poe contains less than 2 values (NOTE: In
+     *                theory the only situation we should avoid is
+     *                poe.getNum()==1 (because in this case the formula cannot
+     *                be applied). The case poe.getNum()==2 is avoided here
+     *                because the resulting PMF would have only one value, and
+     *                when an EvenlyDiscretizedFunc is defined with only one
+     *                value, than the delta parameter is overwritten in the
+     *                constructor and set to 0. This can cause some problems
+     *                later on, for instance when the PMF object is asked for
+     *                the delta value (for instance when saving the pmf in a XML
+     *                file). From the practical point of view this may not be a
+     *                big issue because we can expect that POE functions are
+     *                usually defined for more than 2 values), poe values are
+     *                not in the range [0,1], and poe values are not in
+     *                descending order.
+     */
+    public static EvenlyDiscretizedFunc getPMF(EvenlyDiscretizedFunc poe) {
+        validatePOE(poe);
+
+        // Number of values == number of bins' middle points, e.g.
+        // the number of values in the POE but decreased by 1
+        int numVal = poe.getNum() - 1;
+
+        // bin width (same as POE given that the POE is evenly spaced)
+        double binWidth = poe.getDelta();
+
+        // minimum value (the middle point of the first bin)
+        double minVal = poe.getX(0) + binWidth / 2;
+
+        EvenlyDiscretizedFunc pmf =
+                new EvenlyDiscretizedFunc(minVal, numVal, binWidth);
+        for (int i = 0; i < numVal; i++) {
+            double val = poe.getY(i) - poe.getY(i + 1);
+            pmf.set(i, val);
+        }
+        return pmf;
+    }
+
+    /**
+     * This method calculate probability mass function (PMF) values from an
+     * arbitrarily discretized probability of exceedence (POE) function.
+     * 
+     * @param poe
+     *            ArbitrarilyDiscretizedFunc containing POE values
+     * @return ArbitrarilyDiscretizedFunc containing PMF values. PMF values
+     *         refer to the middle points of the POE bins.
+     * @exception IllegalArgumentException
+     *                poe is null, poe contains less than 2 values (NOTE: In
+     *                theory the only situation we should avoid is
+     *                poe.getNum()==1 (because in this case the formula cannot
+     *                be applied). The case poe.getNum()==2 is avoided here
+     *                because the resulting PMF would have only one value, and
+     *                when an EvenlyDiscretizedFunc is defined with only one
+     *                value, than the delta parameter is overwritten in the
+     *                constructor and set to 0. This can cause some problems
+     *                later on, for instance when the PMF object is asked for
+     *                the delta value (for instance when saving the pmf in a XML
+     *                file). From the practical point of view this may not be a
+     *                big issue because we can expect that POE functions are
+     *                usually defined for more than 2 values), poe values are
+     *                not in the range [0,1], and poe values are not in
+     *                descending order.
+     */
+    public static ArbitrarilyDiscretizedFunc getPMF(
+            ArbitrarilyDiscretizedFunc poe) {
+
+        validatePOE(poe);
+
+        ArbitrarilyDiscretizedFunc pmf = new ArbitrarilyDiscretizedFunc();
+        for (int i = 0; i < poe.getNum() - 1; i++) {
+            double x1 = poe.getX(i);
+            double x2 = poe.getX(i + 1);
+            double xMean = (x1 + x2) / 2;
+            double val = poe.getY(i) - poe.getY(i + 1);
+            pmf.set(xMean, val);
+        }
+        return pmf;
+    }
+
+    /**
+     * Sanity check the incoming POE object for valid PMF output
+     * 
+     * @param poe
+     * @return Boolean
+     */
+    private static Boolean validatePOE(DiscretizedFuncAPI poe) {
+
+        if (poe == null) {
+            throw new IllegalArgumentException("POE function cannot be null");
+        }
+
+        if (poe.getNum() <= 2) {
+            throw new IllegalArgumentException(
+                    "POE function must contain >2 values");
+        }
+
+        if (poeValuesAreBetween0and1(poe) == false) {
+            throw new IllegalArgumentException(
+                    "POE function values must be (0 <= values <= 1)");
+        }
+
+        if (poeValuesAreDescending(poe) == false) {
+            throw new IllegalArgumentException(
+                    "POE function values must be in descending order");
+        }
+        return true;
+    }
+
+    /**
+     * This method checks if POE values are in descending order
+     * 
+     * @param poe
+     * @return Boolean
+     */
+    private static Boolean poeValuesAreDescending(DiscretizedFuncAPI poe) {
+        for (int i = 0; i < poe.getNum() - 1; i++) {
+            if (poe.getY(i + 1) > poe.getY(i))
+                return false;
+        }
+        return true;
+    }
+
+    /**
+     * This method checks that POE values are between 0 and 1
+     * 
+     * @param poe
+     * @return Boolean
+     */
+    private static Boolean poeValuesAreBetween0and1(DiscretizedFuncAPI poe) {
+        ListIterator<Double> valIter = poe.getYValuesIterator();
+        while (valIter.hasNext()) {
+            double val = valIter.next();
+            if (val < 0 || val > 1)
+                return false;
+        }
+        return true;
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/calc/HazardCalculator.java
@@ -0,0 +1,355 @@
+package org.gem.calc;
+
+import java.rmi.RemoteException;
+import java.text.DecimalFormat;
+import java.text.DecimalFormatSymbols;
+import java.util.HashMap;
+import java.util.List;
+import java.util.ListIterator;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.gem.JsonSerializer;
+import org.gem.engine.hazard.redis.Cache;
+import org.opensha.commons.data.Site;
+import org.opensha.commons.data.function.ArbitrarilyDiscretizedFunc;
+import org.opensha.commons.data.function.DiscretizedFuncAPI;
+import org.opensha.sha.calc.HazardCurveCalculator;
+import org.opensha.sha.earthquake.EqkRupForecastAPI;
+import org.opensha.sha.earthquake.EqkRupture;
+import org.opensha.sha.imr.ScalarIntensityMeasureRelationshipAPI;
+import org.opensha.sha.util.TectonicRegionType;
+
+import com.google.gson.Gson;
+
+/**
+ * This class provides methods for hazard calculations.
+ *
+ * @author damianomonelli
+ *
+ */
+
+public class HazardCalculator {
+
+    private static Log logger = LogFactory.getLog(HazardCalculator.class);
+
+    /**
+     * Calculate hazard curves for a set of sites from an earthquake rupture
+     * forecast using the classical PSHA approach
+     *
+     * @param siteList
+     *            : list of sites ({@link Site}) where to compute hazard curves
+     * @param erf
+     *            : earthquake rupture forecast {@link EqkRupForecastAPI}
+     * @param gmpeMap
+     *            : map associating tectonic region types (
+     *            {@link TectonicRegionType}) with attenuation relationships (
+     *            {@link ScalarIntensityMeasureRelationshipAPI})
+     * @param imlVals
+     *            : intensity measure levels (double[]) for which calculating
+     *            probabilities of exceedence
+     * @param integrationDistance
+     *            : maximum distance used for integration
+     * @return
+     */
+    public static
+            Map<Site, DiscretizedFuncAPI>
+            getHazardCurves(
+                    List<Site> siteList,
+                    EqkRupForecastAPI erf,
+                    Map<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> gmpeMap,
+                    List<Double> imlVals, double integrationDistance) {
+        validateInput(siteList, erf, gmpeMap);
+        if (imlVals == null) {
+            String msg = "Array of intensity measure levels cannot be null";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        if (imlVals.isEmpty()) {
+            String msg =
+                    "Array of intensity measure levels must"
+                            + " contain at least one value";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        Map<Site, DiscretizedFuncAPI> results =
+                new HashMap<Site, DiscretizedFuncAPI>();
+        HazardCurveCalculator curveCalculator = null;
+        try {
+            curveCalculator = new HazardCurveCalculator();
+            curveCalculator.setMaxSourceDistance(integrationDistance);
+            for (Site site : siteList) {
+                DiscretizedFuncAPI hazardCurve =
+                        new ArbitrarilyDiscretizedFunc();
+                for (double val : imlVals)
+                    hazardCurve.set(val, 1.0);
+                curveCalculator.getHazardCurve(hazardCurve, site, gmpeMap, erf);
+                results.put(site, hazardCurve);
+            }
+        } catch (RemoteException e) {
+            logger.error(e);
+            throw new RuntimeException(e);
+        }
+        return results;
+    }
+
+    /**
+     * Get the site/hazard curve pairs as a list of JSON Strings.
+     *
+     * @param siteList
+     * @param erf
+     * @param gmpeMap
+     * @param imlVals
+     * @param integrationDistance
+     * @return
+     */
+    public static
+            String[]
+            getHazardCurvesAsJson(
+                    List<Site> siteList,
+                    EqkRupForecastAPI erf,
+                    Map<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> gmpeMap,
+                    List<Double> imlVals, double integrationDistance) {
+        Map<Site, DiscretizedFuncAPI> curves =
+                getHazardCurves(siteList, erf, gmpeMap, imlVals,
+                        integrationDistance);
+        List<String> returnCurves =
+                JsonSerializer.hazardCurvesToJson(curves, siteList);
+        return returnCurves.toArray(new String[returnCurves.size()]);
+    }
+
+    /**
+     * Calculate ground motion fields (correlated or uncorrelated) from a
+     * stochastic event set generated through random sampling of an earthquake
+     * rupture forecast
+     *
+     * @param siteList
+     *            : list of sites ({@link Site}) where to compute ground motion
+     *            values
+     * @param erf
+     *            : earthquake rupture forecast {@link EqkRupForecastAPI}
+     * @param gmpeMap
+     *            : map associating tectonic region types (
+     *            {@link TectonicRegionType}) with attenuation relationships (
+     *            {@link ScalarIntensityMeasureRelationshipAPI})
+     * @param rn
+     *            : random ({@link Random}) number generator
+     * @param : correlation flag, if true compute correlated ground motion
+     *        fields using Jayaram and Baker (2009) correlation model
+     *        considering no Vs30 clustering; if false compute uncorrelated
+     *        ground motion fields
+     * @return
+     */
+    public static
+            Map<EqkRupture, Map<Site, Double>>
+            getGroundMotionFields(
+                    List<Site> siteList,
+                    EqkRupForecastAPI erf,
+                    Map<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> gmpeMap,
+                    Random rn, boolean correlation) {
+        validateInput(siteList, erf, gmpeMap);
+        if (rn == null) {
+            String msg = "Random number generator cannot be null";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        Map<EqkRupture, Map<Site, Double>> groundMotionFields =
+                new HashMap<EqkRupture, Map<Site, Double>>();
+        List<EqkRupture> eqkRupList =
+                StochasticEventSetGenerator
+                        .getStochasticEventSetFromPoissonianERF(erf, rn);
+        for (EqkRupture rup : eqkRupList) {
+            logger.debug("rupture mag is " + rup.getMag());
+            GroundMotionFieldCalculator gmfCalc =
+                new GroundMotionFieldCalculator(
+                        gmpeMap.get(rup.getTectRegType()),rup,siteList);
+            if (correlation == true) {
+                groundMotionFields.put(rup, gmfCalc
+                        .getCorrelatedGroundMotionField_JB2009(
+                        		rn));
+            } else {
+                groundMotionFields.put(rup, gmfCalc
+                        .getUncorrelatedGroundMotionField(rn));
+            }
+        }
+        return groundMotionFields;
+    }
+
+    public static
+            void
+            generateAndSaveGMFs(
+                    Cache cache,
+                    String key,
+                    String gmf_id,
+                    List<Site> siteList,
+                    EqkRupForecastAPI erf,
+                    Map<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> gmpeMap,
+                    Random rn, boolean correlation) {
+        Map<EqkRupture, Map<Site, Double>> gmfs =
+                getGroundMotionFields(siteList, erf, gmpeMap, rn, correlation);
+
+        String[] site_ids = new String[siteList.size()];
+        ListIterator<Site> sites = siteList.listIterator();
+        while (sites.hasNext()) {
+            sites.next();
+            site_ids[sites.nextIndex() - 1] =
+                    Integer.toString(sites.nextIndex() - 1);
+        }
+        String[] rupture_ids = new String[gmfs.keySet().size()];
+        for (int x = 0; x < gmfs.keySet().size(); x++) {
+            rupture_ids[x] = Integer.toString(x);
+        }
+        gmfToMemcache(cache, key, gmf_id, rupture_ids, site_ids, gmfs);
+    }
+
+    public static
+            Boolean
+            validateInput(
+                    List<Site> siteList,
+                    EqkRupForecastAPI erf,
+                    Map<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> gmpeMap) {
+        if (siteList == null) {
+            String msg = "List of sites cannot be null";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        if (siteList.isEmpty()) {
+            String msg = "List of sites must contain at least one site";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        if (erf == null) {
+            String msg = "Earthquake rupture forecast cannot be null";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        if (gmpeMap == null) {
+            String msg = "Gmpe map cannot be null";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        if (gmpeMap.isEmpty()) {
+            String msg = "Gmpe map must contain at least one gmpe";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        return true;
+    }
+
+    /**
+     * From a ground motion field this method serializes only the data that is
+     * needed to a json string.<br>
+     * The suggested format for a jsonized GMF is<br>
+     * {'gmf_id' : { 'eqkrupture_id' : { 'site_id' : {'lat' : lat_val, 'lon' :
+     * lon_val, 'mag' : double_val}}, { 'site_id' : { ...}} , {...} }}
+     *
+     * From identifiers.py, these are what the expected keys look like (this
+     * makes no expectation of the values), the keys are after the colon.
+     *
+     * sites: job_id!block_id!!sites gmf: job_id!block_id!!gmf gmf:
+     * job_id!block_id!site!gmf
+     *
+     * @return
+     */
+    protected static String gmfToJson(String gmfId, String[] eqkRuptureIds,
+            String[] siteIds,
+            Map<EqkRupture, Map<Site, Double>> groundMotionFields) {
+        StringBuilder result = new StringBuilder();
+        Gson gson = new Gson();
+        result.append("{");
+        result.append(gson.toJson(gmfId));
+        result.append(":{");
+        // TODO:
+        // The EqkRupture memcache keys must be known here.
+        // For now behave, as if the map object is ordered.
+        //
+        Set<EqkRupture> groundMotionFieldsKeys = groundMotionFields.keySet();
+        int indexEqkRupture = 0;
+        for (EqkRupture eqkRupture : groundMotionFieldsKeys) {
+            if (indexEqkRupture > 0) {
+                result.append(",");
+            }
+            result.append(gson.toJson(eqkRuptureIds[indexEqkRupture++]));
+            // start the eqk json object
+            result.append(":");
+            Map<Site, Double> groundMotionField =
+                    groundMotionFields.get(eqkRupture);
+            // TODO:
+            // The sites' memcache keys must be known here.
+            // For now behave, as if the map object is ordered.
+            Set<Site> groundMotionFieldKeys = groundMotionField.keySet();
+            int indexSite = 0;
+            StringBuilder siteListString = new StringBuilder();
+            siteListString.append("{");
+            // must instantiate the DecimalFormat object with a locale
+            // that uses the dot as decimal separator (such as the US locale)
+            DecimalFormatSymbols dfs = new DecimalFormatSymbols(Locale.US);
+            DecimalFormat df = new DecimalFormat("0.########E0", dfs);
+            for (Site s : groundMotionFieldKeys) {
+                if (indexSite > 0) {
+                    siteListString.append(",");
+                }
+                StringBuilder siteString = new StringBuilder();
+                siteString.append(gson.toJson(siteIds[indexSite++]));
+                // start the json site's value object
+                siteString.append(":{");
+                siteString.append(gson.toJson("lat") + ":"
+                        + df.format(s.getLocation().getLatitude()));
+                siteString.append(",");
+                siteString.append(gson.toJson("lon") + ":"
+                        + df.format(s.getLocation().getLongitude()));
+                siteString.append(",");
+                siteString.append(gson.toJson("mag") + ":"
+                        + df.format(groundMotionField.get(s)));
+                // close the the json site's value object and the site json
+                // object
+                siteString.append("}");
+                siteListString.append(siteString);
+            } // for
+            siteListString.append("}");
+            // close the eqk json object
+            result.append(siteListString);
+        } // for
+        result.append("}}");
+        return result.toString();
+    }
+
+    /**
+     * Saves a ground motion map to a Cache object.<br>
+     * <br>
+     * 1) Converts the <code>groundMotionFields</code> into json format.<br>
+     * E.g.<br>
+     * {"gmf_id":<br>
+     * {"eqkRupture_id_0":<br>
+     * {"site_id_0":{"lat":35.0,"lon":37.6,"mag":-4.7}},
+     * {"site_id_1":{"lat":37.5,"lon":35.6,"mag":-2.8}},...
+     *
+     * 2) Saves the json string to memCache.
+     *
+     * @param memCacheKey
+     * @param gmfId
+     *            The "json key" for the GMF (ground motion field)
+     * @param eqkRuptureIds
+     *            The "json key" for the the ruptures contained in
+     *            groundMotionFields
+     * @param siteIds
+     *            The "json key" for all the sites contained in
+     *            groundMotionFields
+     * @param groundMotionFields
+     *            The GMF to be saved to memcache
+     * @param cache
+     *            The memcache
+     */
+    protected static void gmfToMemcache(Cache cache, String memCacheKey,
+            String gmfId, String[] eqkRuptureIds, String[] siteIds,
+            Map<EqkRupture, Map<Site, Double>> groundMotionFields) {
+        String json =
+                gmfToJson(gmfId, eqkRuptureIds, siteIds, groundMotionFields);
+        logger.debug("Saving GMF to " + memCacheKey);
+        cache.set(memCacheKey, json);
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/calc/GroundMotionFieldCalculator.java
@@ -0,0 +1,497 @@
+package org.gem.calc;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.math.linear.CholeskyDecompositionImpl;
+import org.apache.commons.math.linear.OpenMapRealMatrix;
+import org.apache.commons.math.linear.RealMatrix;
+import org.opensha.commons.data.Site;
+import org.opensha.commons.geo.LocationUtils;
+import org.opensha.sha.earthquake.EqkRupture;
+import org.opensha.sha.imr.ScalarIntensityMeasureRelationshipAPI;
+import org.opensha.sha.imr.param.IntensityMeasureParams.PeriodParam;
+import org.opensha.sha.imr.param.IntensityMeasureParams.SA_Param;
+import org.opensha.sha.imr.param.OtherParams.SigmaTruncLevelParam;
+import org.opensha.sha.imr.param.OtherParams.SigmaTruncTypeParam;
+import org.opensha.sha.imr.param.OtherParams.StdDevTypeParam;
+
+/**
+ * Class providing methods for ground motion field calculation.
+ */
+public class GroundMotionFieldCalculator {
+
+	private static Log logger = LogFactory
+			.getLog(GroundMotionFieldCalculator.class);
+
+	/**
+	 * attenuation relationship for computing ground motion field
+	 */
+	private ScalarIntensityMeasureRelationshipAPI attenRel;
+	/**
+	 * earthquake rupture generating ground motion field
+	 */
+	private EqkRupture rup;
+	/**
+	 * list of sites where ground motion field is computed
+	 */
+	private List<Site> sites;
+	/**
+	 * lower triangular matrix obtained from cholesky decomposition of
+	 * covariance matrix (used for correlated ground motion field calculations)
+	 */
+	private RealMatrix lowerTriangularCovarianceMatrix;
+
+	/**
+	 * Jayaram and Baker 2009 Vs30 cluster parameter. The default is false (no
+	 * clustering in Vs30 distribution)
+	 */
+	private boolean JB2009_Vs30ClusterParam = false;
+
+	/**
+	 * if true compute correlated ground motion field using both inter- and
+	 * intra-event residuals, if false use only intra-event residuals (NOTE:
+	 * this option has been done mostly for testing purposes, some tests put
+	 * this flag to false to check that the correlation in the intra-event
+	 * residuals in the ground motion fiels is correclty computed). Default is
+	 * true.
+	 */
+	private boolean interEvent = true;
+
+	/**
+	 * Defines truncation level for covariance matrix calculation. If distance
+	 * between sites is greater than correlationTruncationLevel*correlationRange
+	 * then automatically set to zero the correlation value
+	 */
+	private double correlationTruncationLevel = 2.0;
+
+	/**
+	 * Defines a ground motion field calculator
+	 *
+	 * @param attenRel
+	 *            : {@link ScalarIntensityMeasureRelationshipAPI} attenuation
+	 *            relationship used for ground motion field calculation
+	 * @param rup
+	 *            : {@link EqkRupture} earthquake rupture generating the ground
+	 *            motion field
+	 * @param sites
+	 *            : array list of {@link Site} where ground motion values have
+	 *            to be computed
+	 */
+	public GroundMotionFieldCalculator(
+			ScalarIntensityMeasureRelationshipAPI attenRel, EqkRupture rup,
+			List<Site> sites) {
+		validateInput(attenRel, rup, sites);
+		this.attenRel = attenRel;
+		this.rup = rup;
+		this.sites = sites;
+		// the lower triangular matrix coming from cholesky decomposition of
+		// covariance matrix is set to null and calculated only if correlated
+		// ground motion calculation is requested
+		lowerTriangularCovarianceMatrix = null;
+	}
+
+	/**
+	 * Computes mean ground motion for a list of sites.
+	 *
+	 * @return : {@link Map} associating sites ({@link Site}) and ground motion
+	 *         values {@link Double}
+	 */
+	public Map<Site, Double> getMeanGroundMotionField() {
+
+		logger.debug("Computing mean ground motion field...");
+		// get current time
+		long start = System.currentTimeMillis();
+
+		Map<Site, Double> groundMotionMap = new HashMap<Site, Double>();
+		attenRel.setEqkRupture(rup);
+		for (Site site : sites) {
+			attenRel.setSite(site);
+			groundMotionMap.put(site, new Double(attenRel.getMean()));
+
+		}
+
+		getAndPrintElapsedTime(start);
+
+		return groundMotionMap;
+	}
+
+	/**
+	 * Computes uncorrelated ground motion field by adding to the mean ground
+	 * motion field Gaussian deviates which takes into account the truncation
+	 * level and the truncation type. If the attenuation relationship supports
+	 * inter and intra event standard deviations, the method computes ground
+	 * motion field by first generating the inter-event residual (same for all
+	 * the sites) and then sum the intra-event residuals (different for each
+	 * site), otherwise generate residuals for each site according to the total
+	 * standard deviation
+	 *
+	 * @param rn
+	 *            : {@link Random} random number generator for Gaussian deviate
+	 *            calculation
+	 * @return: {@link Map} associating sites ({@link Site}) and ground motion
+	 *          values {@link Double}
+	 */
+	public Map<Site, Double> getUncorrelatedGroundMotionField(Random rn) {
+
+		logger.debug("Computing uncorrelated ground motion field...");
+		// get current time
+		long start = System.currentTimeMillis();
+
+		checkRandomNumberIsNotNull(rn);
+
+		Map<Site, Double> groundMotionField = getMeanGroundMotionField();
+
+		if (attenRel.getParameter(StdDevTypeParam.NAME).getConstraint()
+				.isAllowed(StdDevTypeParam.STD_DEV_TYPE_INTER)
+				&& attenRel.getParameter(StdDevTypeParam.NAME).getConstraint()
+						.isAllowed(StdDevTypeParam.STD_DEV_TYPE_INTRA)) {
+			computeAndAddInterEventResidual(rn, groundMotionField);
+			computeAndAddSiteDependentResidual(rn, groundMotionField,
+					StdDevTypeParam.STD_DEV_TYPE_INTRA);
+		} else {
+			computeAndAddSiteDependentResidual(rn, groundMotionField,
+					StdDevTypeParam.STD_DEV_TYPE_TOTAL);
+		}
+
+		getAndPrintElapsedTime(start);
+
+		return groundMotionField;
+	}
+
+	/**
+	 * Compute ground motion field with spatial correlation using correlation
+	 * model from Jayamram & Baker (2009):
+	 * "Correlation model for spatially distributed ground-motion intensities"
+	 * Nirmal Jayaram and Jack W. Baker, Earthquake Engng. Struct. Dyn (2009)
+	 * The algorithm is structured according to the following steps: 1) Compute
+	 * mean ground motion values, 2) Stochastically generate inter-event
+	 * residual (which follow a univariate normal distribution), 3)
+	 * Stochastically generate intra-event residuals (following the proposed
+	 * correlation model) 4) Combine the three terms generated in steps 1-3.
+	 *
+	 * Intra-event residuals are calculated by generating Gaussian deviates from
+	 * a multivariate normal distribution using Cholesky factorization
+	 * (decompose covariance matrix, take lower triangular and multiply by a
+	 * vector of uncorrelated, standard Gaussian variables)
+	 *
+	 * @param rn
+	 *            : {@link Random} random number generator
+	 * @return: {@link Map} associating sites ({@link Site}) and ground motion
+	 *          values {@link Double}
+	 */
+	public Map<Site, Double> getCorrelatedGroundMotionField_JB2009(Random rn) {
+
+		logger.debug("Computing correlated (JB2009) ground motion field...");
+		// get current time
+		long start = System.currentTimeMillis();
+
+		checkRandomNumberIsNotNull(rn);
+		validateInputCorrelatedGmfCalc(attenRel);
+
+		// covariance matrix and cholesky decompositions are computed only once.
+		// If multiple ground motion fields are needed for
+		// the same rupture, these calculations are not redone.
+		if (lowerTriangularCovarianceMatrix == null) {
+			CholeskyDecompositionImpl cholDecomp = null;
+			try {
+				cholDecomp = new CholeskyDecompositionImpl(
+						getCovarianceMatrix_JB2009());
+			} catch (Exception e) {
+				String msg = "Unexpected exception: " + e.getMessage();
+				logger.error(msg);
+				throw new RuntimeException(e);
+			}
+			lowerTriangularCovarianceMatrix = cholDecomp.getL();
+		}
+
+		Map<Site, Double> groundMotionField = getMeanGroundMotionField();
+
+		if (interEvent == true) {
+			computeAndAddInterEventResidual(rn, groundMotionField);
+		}
+
+		computeAndAddCorrelatedIntraEventResidual(rn, groundMotionField);
+
+		getAndPrintElapsedTime(start);
+
+		return groundMotionField;
+	}
+
+	/**
+	 * Set GMPE standard deviation to inter-event, then stochastically generate
+	 * a single inter-event residual, and add this value to the already computed
+	 * ground motion values
+	 */
+	private void computeAndAddInterEventResidual(Random rn,
+			Map<Site, Double> groundMotionField) {
+
+		logger.debug("Computing and adding inter event residual...");
+		// get current time
+		long start = System.currentTimeMillis();
+
+		attenRel.getParameter(StdDevTypeParam.NAME).setValue(
+				StdDevTypeParam.STD_DEV_TYPE_INTER);
+		double interEventResidual = getGaussianDeviate(attenRel.getStdDev(),
+				(Double) attenRel.getParameter(SigmaTruncLevelParam.NAME)
+						.getValue(),
+				(String) attenRel.getParameter(SigmaTruncTypeParam.NAME)
+						.getValue(), rn);
+		for (Site site : sites) {
+			double val = groundMotionField.get(site);
+			groundMotionField.put(site, val + interEventResidual);
+		}
+
+		getAndPrintElapsedTime(start);
+	}
+
+	/**
+	 * For each site stochastically generate deviate according to GMPE standard
+	 * deviation type (the site and earthquake information are also set because
+	 * the standard deviation may depend on the site-rupture distance, rupture
+	 * magnitude,..), and add to the already computed ground motion value
+	 */
+	private void computeAndAddSiteDependentResidual(Random rn,
+			Map<Site, Double> groundMotionField, String stdType) {
+
+		logger.debug("Computing and adding " + stdType + " residual...");
+		// get current time
+		long start = System.currentTimeMillis();
+
+		attenRel.getParameter(StdDevTypeParam.NAME).setValue(stdType);
+		attenRel.setEqkRupture(rup);
+		for (Site site : sites) {
+			attenRel.setSite(site);
+			Double val = groundMotionField.get(site);
+			double deviate = getGaussianDeviate(attenRel.getStdDev(),
+					(Double) attenRel.getParameter(SigmaTruncLevelParam.NAME)
+							.getValue(),
+					(String) attenRel.getParameter(SigmaTruncTypeParam.NAME)
+							.getValue(), rn);
+			val = val + deviate;
+			groundMotionField.put(site, val);
+		}
+		getAndPrintElapsedTime(start);
+	}
+
+	/**
+	 * Compute intra-event residuals, by multiplying the lower triangular matrix
+	 * with a vector of univariate Gaussian deviates
+	 */
+	private void computeAndAddCorrelatedIntraEventResidual(Random rn,
+			Map<Site, Double> groundMotionField) {
+
+		logger.debug("Compute and add correlated and intra-event residuals...");
+		// get current time
+		long start = System.currentTimeMillis();
+
+		int numberOfSites = sites.size();
+		double[] gaussianDeviates = new double[numberOfSites];
+		for (int i = 0; i < numberOfSites; i++) {
+			gaussianDeviates[i] = getGaussianDeviate(1.0, (Double) attenRel
+					.getParameter(SigmaTruncLevelParam.NAME).getValue(),
+					(String) attenRel.getParameter(SigmaTruncTypeParam.NAME)
+							.getValue(), rn);
+		}
+
+		double[] intraEventResiduals = lowerTriangularCovarianceMatrix
+				.operate(gaussianDeviates);
+
+		int indexSite = 0;
+		for (Site site : sites) {
+			double val = groundMotionField.get(site);
+			groundMotionField.put(site, val + intraEventResiduals[indexSite]);
+			indexSite = indexSite + 1;
+		}
+
+		getAndPrintElapsedTime(start);
+	}
+
+	private void validateInputCorrelatedGmfCalc(
+			ScalarIntensityMeasureRelationshipAPI attenRel) {
+		if (attenRel.getParameter(StdDevTypeParam.NAME).getConstraint()
+				.isAllowed(StdDevTypeParam.STD_DEV_TYPE_INTER) == false) {
+			throw new IllegalArgumentException(
+					"The specified attenuation relationship does not provide"
+							+ " inter-event standard deviation");
+		}
+		if (attenRel.getParameter(StdDevTypeParam.NAME).getConstraint()
+				.isAllowed(StdDevTypeParam.STD_DEV_TYPE_INTRA) == false) {
+			throw new IllegalArgumentException(
+					"The specified attenuation relationship does not provide"
+							+ " intra-event standard deviation");
+		}
+	}
+
+	private void checkRandomNumberIsNotNull(Random rn) {
+		if (rn == null) {
+			throw new IllegalArgumentException(
+					"Random number generator cannot be null");
+		}
+	}
+
+	/**
+	 * Generate Gaussian deviate (mean zero, standard deviation =
+	 * standardDeviation)
+	 *
+	 * @param standardDeviation
+	 *            : double standard deviation
+	 * @param truncationLevel
+	 *            : double truncation level (in units of standard deviation)
+	 * @param truncationType
+	 *            : String type of truncation defined by the
+	 *            {@link SigmaTruncTypeParam}
+	 * @param rn
+	 *            : random number generator
+	 * @return : double
+	 */
+	private double getGaussianDeviate(double standardDeviation,
+			double truncationLevel, String truncationType, Random rn) {
+		double dev = rn.nextGaussian();
+		if (truncationType
+				.equalsIgnoreCase(SigmaTruncTypeParam.SIGMA_TRUNC_TYPE_2SIDED)) {
+			while (dev < -truncationLevel || dev > truncationLevel) {
+				dev = rn.nextGaussian();
+			}
+		} else if (truncationType
+				.equalsIgnoreCase(SigmaTruncTypeParam.SIGMA_TRUNC_TYPE_1SIDED)) {
+			while (dev > truncationLevel) {
+				dev = rn.nextGaussian();
+			}
+		}
+		return dev * standardDeviation;
+	}
+
+	/**
+	 * Calculates covariance matrix for intra-event residuals using correlation
+	 * model of Jayamram & Baker (2009):
+	 * "Correlation model for spatially distributed ground-motion intensities"
+	 * Nirmal Jayaram and Jack W. Baker, Earthquake Engng. Struct. Dyn (2009)
+	 *
+	 * @return covariance matrix as {@link OpenMapRealMatrix}
+	 */
+	private OpenMapRealMatrix getCovarianceMatrix_JB2009() {
+
+		logger.debug("Compute covariance matrix...");
+		// get current time
+		long start = System.currentTimeMillis();
+
+		int numberOfSites = sites.size();
+		OpenMapRealMatrix covarianceMatrix = new OpenMapRealMatrix(
+				numberOfSites, numberOfSites);
+
+		attenRel.setEqkRupture(rup);
+		attenRel.getParameter(StdDevTypeParam.NAME).setValue(
+				StdDevTypeParam.STD_DEV_TYPE_INTRA);
+
+		// default value for period is zero. Only if spectral acceleration
+		// calculation is requested, the value of the period variable is
+		// obtained from the attenRel object
+		double period = 0.0;
+		if (attenRel.getIntensityMeasure().getName()
+				.equalsIgnoreCase(SA_Param.NAME)) {
+			period = (Double) attenRel.getParameter(PeriodParam.NAME)
+					.getValue();
+		}
+
+		double correlationRange = Double.NaN;
+		if (period < 1 && JB2009_Vs30ClusterParam == false)
+			correlationRange = 8.5 + 17.2 * period;
+		else if (period < 1 && JB2009_Vs30ClusterParam == true)
+			correlationRange = 40.7 - 15.0 * period;
+		else if (period >= 1)
+			correlationRange = 22.0 + 3.7 * period;
+		double intraEventStd_i = Double.NaN;
+		double intraEventStd_j = Double.NaN;
+		double distance = Double.NaN;
+		double covarianceValue = Double.NaN;
+		for (int i = 0; i < numberOfSites; i++) {
+			Site site_i = sites.get(i);
+			attenRel.setSite(site_i);
+			intraEventStd_i = attenRel.getStdDev();
+			logger.debug("Covariance matrix row: " + (i + 1) + " of "
+					+ numberOfSites);
+			for (int j = i; j < numberOfSites; j++) {
+				Site site_j = sites.get(j);
+				distance = LocationUtils.horzDistance(site_i.getLocation(),
+						site_j.getLocation());
+				if (distance > correlationTruncationLevel * correlationRange) {
+					continue;
+				}
+				attenRel.setSite(site_j);
+				intraEventStd_j = attenRel.getStdDev();
+				covarianceValue = intraEventStd_i * intraEventStd_j
+						* Math.exp(-3 * (distance / correlationRange));
+				covarianceMatrix.setEntry(i, j, covarianceValue);
+				covarianceMatrix.setEntry(j, i, covarianceValue);
+			}
+		}
+		return covarianceMatrix;
+	}
+
+	private static Boolean validateInput(
+			ScalarIntensityMeasureRelationshipAPI attenRel, EqkRupture rup,
+			List<Site> sites) {
+		if (attenRel == null) {
+			throw new IllegalArgumentException(
+					"Attenuation relationship cannot be null");
+		}
+
+		if (rup == null) {
+			throw new IllegalArgumentException(
+					"Earthquake rupture cannot be null");
+		}
+
+		if (sites == null) {
+			throw new IllegalArgumentException(
+					"Array list of sites cannot be null");
+		}
+
+		if (sites.isEmpty()) {
+			throw new IllegalArgumentException(
+					"Array list of sites must contain at least one site");
+		}
+
+		return true;
+	}
+
+	public boolean isJB2009_Vs30ClusterParam() {
+		return JB2009_Vs30ClusterParam;
+	}
+
+	public void setJB2009_Vs30ClusterParam(boolean jB2009_Vs30ClusterParam) {
+		JB2009_Vs30ClusterParam = jB2009_Vs30ClusterParam;
+	}
+
+	public boolean isInterEvent() {
+		return interEvent;
+	}
+
+	public void setInterEvent(boolean interEvent) {
+		this.interEvent = interEvent;
+	}
+
+	public double getCorrelationTruncationLevel() {
+		return correlationTruncationLevel;
+	}
+
+	public void setCorrelationTruncationLevel(double correlationTruncationLevel) {
+		this.correlationTruncationLevel = correlationTruncationLevel;
+	}
+
+	private void getAndPrintElapsedTime(long start) {
+		// get elapsed time in milliseconds
+		long elapsedTimeMillis = System.currentTimeMillis() - start;
+		// get elapsed time in seconds
+		float elapsedTimeSec = elapsedTimeMillis / 1000F;
+		// get elapsed time in minutes
+		float elapsedTimeMin = elapsedTimeMillis / (60 * 1000F);
+		logger.debug("Elapsed time (s): " + elapsedTimeSec);
+		logger.debug("Elapsed time (min): " + elapsedTimeMin + "\n");
+	}
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/calc/StochasticEventSetGenerator.java
@@ -0,0 +1,138 @@
+package org.gem.calc;
+
+import java.util.ArrayList;
+import java.util.Random;
+
+import org.opensha.sha.earthquake.EqkRupForecast;
+import org.opensha.sha.earthquake.EqkRupForecastAPI;
+import org.opensha.sha.earthquake.EqkRupture;
+import org.opensha.sha.earthquake.ProbEqkRupture;
+import org.opensha.sha.earthquake.ProbEqkSource;
+import org.opensha.sha.util.TectonicRegionType;
+
+/**
+ * 
+ * This class provide methods for the creation of stochastic event sets (each
+ * given as an array list of EqkRupture objects) representative of a given
+ * Earthquake Rupture Forecast.
+ * 
+ * @author Damiano Monelli
+ * 
+ */
+public class StochasticEventSetGenerator {
+
+    private static boolean D = true;
+
+    /**
+     * Generate a stochastic event set from a Poissonian ERF. Sampling of the
+     * Poissonian pdf is based on the inverse transform method as described in
+     * "Computational Statistics Handbook with Matlab", Martinez & Martinez, Ed.
+     * Champman & Hall, pag. 103
+     * 
+     * @param erf
+     *            {@link EqkRupForecast} earthquake rupture forecast
+     * @param rn
+     *            {@link Random} random number generator
+     * @return: {@link ArrayList} of {@link EqkRupture} representing the sampled
+     *          events.
+     */
+    public static ArrayList<EqkRupture> getStochasticEventSetFromPoissonianERF(
+            EqkRupForecastAPI erf, Random rn) {
+
+        validateInput(erf, rn);
+
+        ArrayList<EqkRupture> stochasticEventSet = new ArrayList<EqkRupture>();
+        TectonicRegionType tectonicRegionType = null;
+        for (int sourceIdx = 0; sourceIdx < erf.getNumSources(); sourceIdx++) {
+            ProbEqkSource src = erf.getSource(sourceIdx);
+            tectonicRegionType = src.getTectonicRegionType();
+            for (int ruptureIdx = 0; ruptureIdx < src.getNumRuptures(); ruptureIdx++) {
+                ProbEqkRupture rup = src.getRupture(ruptureIdx);
+                double numExpectedRup = -Math.log(1 - rup.getProbability());
+                EqkRupture eqk =
+                        new EqkRupture(rup.getMag(), rup.getAveRake(),
+                                rup.getRuptureSurface(),
+                                rup.getHypocenterLocation());
+                eqk.setTectRegType(tectonicRegionType);
+                // sample Poisson distribution using inverse transfom method
+                // p is the Poisson probability
+                // F is the cumulative distribution function
+                // get number of rupture realizations (nRup) given
+                // number of expected ruptures (numExpectedRup). nRup copies of
+                // the same rupture are then added to the stochastic event set.
+                int nRup = 0;
+                boolean flag = true;
+                double u = rn.nextDouble();
+                int i = 0;
+                double p = Math.exp(-numExpectedRup);
+                double F = p;
+                while (flag == true) {
+                    if (u <= F) {
+                        nRup = i;
+                        flag = false;
+                    } else {
+                        p = numExpectedRup * p / (i + 1);
+                        i = i + 1;
+                        F = F + p;
+                    }
+                }
+                for (int j = 0; j < nRup; j++)
+                    stochasticEventSet.add(eqk);
+            }
+        }
+        return stochasticEventSet;
+    }
+
+    /**
+     * Generate multiple stochastic event sets by calling the
+     * getStochasticEvenSetFromPoissonianERF method.
+     * 
+     * @param erf
+     *            {@link EqkRupForecast} earthquake rupture forecast
+     * @param num
+     *            number of stochastic event sets
+     * @param rn
+     *            {@link Random} random number generator
+     * @return {@link ArrayList} of {@link ArrayList} of {@link EqkRupture}.
+     */
+    public static ArrayList<ArrayList<EqkRupture>>
+            getMultipleStochasticEventSetsFromPoissonianERF(EqkRupForecast erf,
+                    int num, Random rn) {
+
+        ArrayList<ArrayList<EqkRupture>> multiStocEventSet =
+                new ArrayList<ArrayList<EqkRupture>>();
+        for (int i = 0; i < num; i++) {
+            multiStocEventSet.add(getStochasticEventSetFromPoissonianERF(erf,
+                    rn));
+        }
+        return multiStocEventSet;
+    }
+
+    /**
+     * Check if the ERF contains only Poissonian sources
+     * 
+     * @param erf
+     */
+    private static Boolean ensurePoissonian(EqkRupForecastAPI erf) {
+        for (ProbEqkSource src : (ArrayList<ProbEqkSource>) erf.getSourceList())
+            if (src.isSourcePoissonian() == false)
+                throw new IllegalArgumentException("Sources must be Poissonian");
+        return true;
+    }
+
+    private static Boolean validateInput(EqkRupForecastAPI erf, Random rn) {
+        if (erf == null) {
+            throw new IllegalArgumentException(
+                    "Earthquake rupture forecast cannot be null");
+        }
+
+        if (rn == null) {
+            throw new IllegalArgumentException(
+                    "Random number generator cannot be null");
+        }
+
+        ensurePoissonian(erf);
+
+        return true;
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/XMLValidationError.java
@@ -0,0 +1,22 @@
+package org.gem.engine;
+
+import org.dom4j.DocumentException;
+
+/**
+ * Trivial Exception subclass with additional information for
+ * error reporting.
+ */
+public class XMLValidationError extends RuntimeException {
+    private String fileName;
+
+    public XMLValidationError(String fileName, DocumentException cause) {
+        super(cause);
+
+        this.fileName = fileName;
+    }
+
+    /** The full path of the invalid file */
+    public String getFileName() {
+        return fileName;
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/CalculatorConfigHelper.java
@@ -0,0 +1,232 @@
+package org.gem.engine;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.commons.configuration.Configuration;
+import org.opensha.commons.data.function.ArbitrarilyDiscretizedFunc;
+import org.opensha.commons.geo.Location;
+import org.opensha.commons.geo.LocationList;
+
+public class CalculatorConfigHelper {
+
+    /**
+     * This enum type defines all valid values for the intensity measure type
+     * configuration item. enum is used rather than defining all valid values as
+     * static members: <code></br>
+     * private final static String INTENSITY_MEASURE_CODE_PGA = "pga";</br>
+     * private final static String INTENSITY_MEASURE_CODE_MMI = "mmi";</br>
+     * </code></br>
+     * 
+     * @author roland
+     * 
+     */
+    public static enum IntensityMeasure {
+        PGA("pga"), MMI("mmi"), PGV("pgv"), PGD("pgd"), SA("sa");
+        private static HashMap<String, IntensityMeasure> intensityMeasures =
+                null;
+        private final String type;
+
+        IntensityMeasure(String name) {
+            type = name;
+        }
+
+        /**
+         * This method aims to a user friendly (i.e. readable) configuration
+         * value.</br> To override the enum's toString() method is not typically
+         * desirable. That method aims to a programmer friendly name.
+         * 
+         * @return the user friendly configuration value
+         */
+        public String type() {
+            return type;
+        }
+
+        /**
+         * Retrieves the enum objects from a internally stored HashMap. (Least
+         * access costs.)
+         * 
+         * @param key
+         *            The enum's name as returned by the enum's name() method.
+         * @return The enum object to the corresponding key.
+         */
+        public static IntensityMeasure get(String key) {
+            if (intensityMeasures == null) {
+                intensityMeasures = new HashMap<String, IntensityMeasure>();
+                for (IntensityMeasure im : values()) {
+                    intensityMeasures.put(im.name(), im);
+                }
+            }
+            return intensityMeasures.get(key);
+        } // get
+    } // enum IntensityMeasure
+
+    /**
+     * This enum type defines all valid values for the calculation modes
+     * configuration item. enum is used rather than defining all valid values as
+     * static members: <code></br>
+     * private final static String INTENSITY_MEASURE_CODE_PGA = "pga";</br>
+     * private final static String INTENSITY_MEASURE_CODE_MMI = "mmi";</br>
+     * </code></br>
+     * 
+     * @author roland
+     * 
+     */
+    public static enum CalculationMode {
+        FULL("Full Calculation"), EVENT_BASED("Event Based");
+        private static HashMap<String, CalculationMode> calculationModes = null;
+        private final String value;
+
+        CalculationMode(String name) {
+            value = name;
+        }
+
+        /**
+         * This method aims to a user friendly (i.e. readable) configuration
+         * value.</br> To override the enum's toString() method is not typically
+         * desirable. That method aims to a programmer friendly name.
+         * 
+         * @return the user friendly configuration value
+         */
+        public String value() {
+            return value;
+        }
+
+        /**
+         * Retrieves the enum objects from a internally stored HashMap. (Least
+         * access costs.)
+         * 
+         * @param key
+         *            The enum's name as returned by the enum's name() method.
+         * @return The enum object to the corresponding key.
+         */
+        public static CalculationMode get(String key) {
+            if (calculationModes == null) {
+                calculationModes = new HashMap<String, CalculationMode>();
+                for (CalculationMode cm : values()) {
+                    calculationModes.put(cm.name(), cm);
+                }
+            }
+            return calculationModes.get(key);
+        } // get
+    } // enum CalculationMode
+
+    // There may be additional/customizable properties
+    // -> does an enum type make sense? ...no.
+    // ...and yes: For the programmer to know at least how to access the
+    // defaults.
+    public static enum ConfigItems {
+        SOURCE_MODEL_LOGIC_TREE_FILE, GMPE_LOGIC_TREE_FILE, OUTPUT_DIR, SUBDUCTION_FAULT_SURFACE_DISCRETIZATION, MAXIMUM_DISTANCE, SUBDUCTION_FAULT_MAGNITUDE_SCALING_RELATIONSHIP, SUBDUCTION_RUPTURE_FLOATING_TYPE, INTENSITY_MEASURE_TYPE, FAULT_MAGNITUDE_SCALING_SIGMA, INCLUDE_GRID_SOURCES, PERIOD, INCLUDE_SUBDUCTION_FAULT_SOURCE, WIDTH_OF_MFD_BIN, MINIMUM_MAGNITUDE, SUBDUCTION_FAULT_MAGNITUDE_SCALING_SIGMA, DAMPING, NUMBER_OF_LOGIC_TREE_SAMPLES, NUMBER_OF_SEISMICITY_HISTORIES, AREA_SOURCE_MAGNITUDE_SCALING_RELATIONSHIP, INVESTIGATION_TIME, TREAT_GRID_SOURCE_AS, INCLUDE_AREA_SOURCES, FAULT_MAGNITUDE_SCALING_RELATIONSHIP, SUBDUCTION_RUPTURE_ASPECT_RATIO, TREAT_AREA_SOURCE_AS, REFERENCE_VS30_VALUE, REFERENCE_DEPTH_TO_2PT5KM_PER_SEC_PARAM, REGION_VERTEX, REGION_GRID_SPACING, CALCULATION_MODE, FAULT_SURFACE_DISCRETIZATION, COMPONENT, RUPTURE_ASPECT_RATIO, NUMBER_OF_PROCESSORS, INTENSITY_MEASURE_LEVELS, TRUNCATION_LEVEL, GMPE_TRUNCATION_TYPE, AREA_SOURCE_DISCRETIZATION, GRID_SOURCE_MAGNITUDE_SCALING_RELATIONSHIP, STANDARD_DEVIATION_TYPE, INCLUDE_FAULT_SOURCE, SUBDUCTION_FAULT_RUPTURE_OFFSET, FAULT_RUPTURE_OFFSET, RUPTURE_FLOATING_TYPE, SOURCE_MODEL_LT_RANDOM_SEED, GMPE_LT_RANDOM_SEED, SADIGH_SITE_TYPE, REGION, GROUND_MOTION_CORRELATION,
+    } // enum
+
+    /**
+     * This helper method is necessary if the REGION property is saved like
+     * this: REGION_VERTEX = -78.0, 0.0, -77.0, 0.0, -77.0, 1.0, -78.0, 1.0 ->
+     * comma separated list of doubles according to documentation of Apache
+     * commons configuration. -> implicitly they are pairs of coordinates, ->
+     * odd indexes mean latitude -> pair indexes mean longitude
+     * 
+     * @param calcConfig
+     * @return
+     */
+    public static LocationList makeRegionboundary(Configuration config) {
+        List<String> vertices =
+                config.getList(ConfigItems.REGION_VERTEX.name());
+        LocationList regionBoundary = new LocationList();
+        Location tmpLoc = null;
+        Iterator<String> iterator = vertices.iterator();
+        while (iterator.hasNext()) {
+            double lat = Double.parseDouble(iterator.next());
+            if (iterator.hasNext()) {
+                // if there is an odd number of coordinates, the last one is
+                // ignored
+                double lon = Double.parseDouble(iterator.next());
+                tmpLoc = new Location(lat, lon);
+                regionBoundary.add(tmpLoc);
+            }
+        } // while
+        return regionBoundary;
+    } // makeRegionBoundary()
+
+    /**
+     * This method retrieves the configuration items</br> 1)
+     * INTENSITY_MEASURE_TYPE</br> 2) INTENSITY_MEASURE_LEVELS</br>
+     * 
+     * Valid values for INTENSITY_MEASURE_TYPE are "PGA" and "MMI".</br>
+     * 
+     * The value of INTENSITY_MEASURE_LEVLES is a comma separated list of
+     * doubles or a multiple value property of doubles according to
+     * documentation of the org.apache.commons.configuration package.
+     * 
+     * From the Configuration object passed by as a parameter. This method is
+     * the same like "makeArbitrarilyDiscretizedFunc()". This method just exists
+     * to be consistent with terminology, i.e. the programmer can use
+     * "makeImlList()" instead of "makeArbitrarilyDiscretizedFunc()" which may
+     * the code make more understandable (to scientists).
+     * 
+     * @param calcConfig
+     *            A Configuration object, usually loaded from a config file.
+     * @return An arbitrarily discretized function.
+     */
+    public static ArbitrarilyDiscretizedFunc makeImlList(Configuration config) {
+        String intensityMeasureType =
+                config.getString(ConfigItems.INTENSITY_MEASURE_TYPE.name());
+        IntensityMeasure imt = IntensityMeasure.get(intensityMeasureType);
+        String[] imlArray =
+                config.getStringArray(ConfigItems.INTENSITY_MEASURE_LEVELS
+                        .name());
+        Double[] imls = StringArrToDoubleArr(imlArray);
+        return makeArbitrarilyDiscretizedFunc(imls, imt);
+    } // makeImlList()
+
+    public static List<Double> makeImlDoubleList(Configuration config) {
+        List<Double> imlDoubleList = new ArrayList<Double>();
+        ArbitrarilyDiscretizedFunc imlFunc = makeImlList(config);
+
+        for (int i = 0; i < imlFunc.getNum(); i++)
+            imlDoubleList.add(imlFunc.getX(i));
+        return imlDoubleList;
+    }
+
+    private static ArbitrarilyDiscretizedFunc makeArbitrarilyDiscretizedFunc(
+            Double[] intensityMeasureLevels, IntensityMeasure imt) {
+        ArbitrarilyDiscretizedFunc adf = new ArbitrarilyDiscretizedFunc();
+        for (double iml : intensityMeasureLevels) {
+            switch (imt) {
+            case PGA:
+                adf.set(Math.log(iml), 1.0);
+                break;
+            case MMI:
+                adf.set(iml, 1.0);
+                break;
+            case PGV:
+                adf.set(Math.log(iml), 1.0);
+                break;
+            case PGD:
+                adf.set(Math.log(iml), 1.0);
+                break;
+            case SA:
+                adf.set(Math.log(iml), 1.0);
+                break;
+            default:
+                throw new IllegalArgumentException(
+                        "Unknown intensity measure type : \"" + imt.toString()
+                                + "\"");
+            } // switch
+        } // for
+        return adf;
+    } // makeArbitrarilyDiscretizedFunc()
+
+    private static Double[] StringArrToDoubleArr(String[] strings) {
+        Double[] doubles = new Double[strings.length];
+        int index = 0;
+        for (String s : strings) {
+            doubles[index] = Double.parseDouble(s);
+            ++index;
+        }
+        return doubles;
+    } // StringArrToDoubleArr()
+
+} // class
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/GmpeLogicTreeData.java
@@ -0,0 +1,347 @@
+package org.gem.engine;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedReader;
+import java.io.ByteArrayInputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.lang.reflect.Constructor;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.StringTokenizer;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.gem.engine.hazard.redis.Cache;
+import org.gem.engine.logictree.LogicTree;
+import org.gem.engine.logictree.LogicTreeBranch;
+import org.gem.engine.logictree.LogicTreeBranchingLevel;
+import org.opensha.commons.param.event.ParameterChangeWarningEvent;
+import org.opensha.commons.param.event.ParameterChangeWarningListener;
+import org.opensha.sha.imr.AttenuationRelationship;
+import org.opensha.sha.imr.ScalarIntensityMeasureRelationshipAPI;
+import org.opensha.sha.imr.param.IntensityMeasureParams.DampingParam;
+import org.opensha.sha.imr.param.IntensityMeasureParams.PeriodParam;
+import org.opensha.sha.imr.param.IntensityMeasureParams.SA_Param;
+import org.opensha.sha.imr.param.OtherParams.ComponentParam;
+import org.opensha.sha.imr.param.OtherParams.SigmaTruncLevelParam;
+import org.opensha.sha.imr.param.OtherParams.SigmaTruncTypeParam;
+import org.opensha.sha.imr.param.OtherParams.StdDevTypeParam;
+import org.opensha.sha.util.TectonicRegionType;
+
+/**
+ * Instantiates {@link LogicTree} objects for ground motion prediction equations
+ * (GMPEs). Each {@link LogicTree} is associated to a {@link TectonicRegionType}
+ * and contains {@link AttenuationRelationship} objects. Logic tree data can be
+ * read from cache or file and are expected to be in nrML format. The specified
+ * GMPEs are expected to be the class names of the attenuation relationships
+ * defined in org.opensha.sha.imr.attenRelImpl package in OpenSHA-lite. From the
+ * GMPE name the corresponding {@link AttenuationRelationship} class is created
+ * through reflection. GMPE parameters are requested for setting
+ * {@link AttenuationRelationship} parameters.
+ */
+public class GmpeLogicTreeData {
+
+    private static Log logger = LogFactory.getLog(GmpeLogicTreeData.class);
+
+    private final ParameterChangeWarningListener warningListener = null;
+
+    private BufferedReader bufferedReader;
+
+    private HashMap<TectonicRegionType, LogicTree<ScalarIntensityMeasureRelationshipAPI>> gmpeLogicTreeHashMap;
+
+    private static final String packageName =
+            "org.opensha.sha.imr.attenRelImpl.";
+
+    /**
+     * Creates a new {@link GmpeLogicTreeData} given cache and key to read from.
+     */
+    public GmpeLogicTreeData(Cache cache, String key) throws IOException {
+
+        String source = (String) cache.get(key);
+        byte[] bytevals = source.getBytes();
+        InputStream byteis = new ByteArrayInputStream(bytevals);
+        BufferedInputStream oBIS = new BufferedInputStream(byteis);
+        this.bufferedReader = new BufferedReader(new InputStreamReader(oBIS));
+        gmpeLogicTreeHashMap =
+                new HashMap<TectonicRegionType, LogicTree<ScalarIntensityMeasureRelationshipAPI>>();
+    }
+
+    /**
+     * Creates a new {@link GmpeLogicTreeData} given the path of the file to
+     * read from.
+     */
+    public GmpeLogicTreeData(String path) throws FileNotFoundException {
+
+        File file = new File(path);
+        FileInputStream oFIS = new FileInputStream(file.getPath());
+        BufferedInputStream oBIS = new BufferedInputStream(oFIS);
+        this.bufferedReader = new BufferedReader(new InputStreamReader(oBIS));
+        gmpeLogicTreeHashMap =
+                new HashMap<TectonicRegionType, LogicTree<ScalarIntensityMeasureRelationshipAPI>>();
+    }
+
+    public GmpeLogicTreeData ()
+    {
+        
+    }
+    
+    /**
+     * Reads logic tree data and instantiates {@link LogicTree} objects for each
+     * {@link TectonicRegionType}. Each {@link LogicTree} contains
+     * {@link AttenuationRelationship} objects with the given parameters set up.
+     */
+    public void parse_tree(String component, String intensityMeasureType,
+            double period, double damping, String truncType, double truncLevel,
+            String stdType, double vs30) {
+
+        LogicTreeReader logicTreeReader = new LogicTreeReader(bufferedReader);
+
+        Map<String, LogicTree> logicTreeMap = logicTreeReader.read();
+
+        if (logicTreeMap.get(TectonicRegionType.ACTIVE_SHALLOW.toString()) != null) {
+            setGMPELogicTree(TectonicRegionType.ACTIVE_SHALLOW.toString(),
+                    component, intensityMeasureType, period, damping,
+                    truncType, truncLevel, stdType, vs30, logicTreeMap);
+        }
+
+        if (logicTreeMap.get(TectonicRegionType.STABLE_SHALLOW.toString()) != null) {
+            setGMPELogicTree(TectonicRegionType.STABLE_SHALLOW.toString(),
+                    component, intensityMeasureType, period, damping,
+                    truncType, truncLevel, stdType, vs30, logicTreeMap);
+        }
+
+        if (logicTreeMap
+                .get(TectonicRegionType.SUBDUCTION_INTERFACE.toString()) != null) {
+            setGMPELogicTree(
+                    TectonicRegionType.SUBDUCTION_INTERFACE.toString(),
+                    component, intensityMeasureType, period, damping,
+                    truncType, truncLevel, stdType, vs30, logicTreeMap);
+        }
+
+        if (logicTreeMap.get(TectonicRegionType.SUBDUCTION_SLAB.toString()) != null) {
+            setGMPELogicTree(TectonicRegionType.SUBDUCTION_SLAB.toString(),
+                    component, intensityMeasureType, period, damping,
+                    truncType, truncLevel, stdType, vs30, logicTreeMap);
+        }
+
+    }
+
+    /**
+     * Instantiates logic tree for a given tectonic region type
+     */
+    private void setGMPELogicTree(String tectReg, String component,
+            String intensityMeasureType, double period, double damping,
+            String truncType, double truncLevel, String stdType, double vs30,
+            Map<String, LogicTree> logicTreeMap) {
+        LogicTree logicTree = logicTreeMap.get(tectReg);
+        String gmpeNames = "";
+        String weights = "";
+        for (int i = 0; i < logicTree.getBranchingLevelAt(0).getBranchList()
+                .size(); i++) {
+            gmpeNames =
+                    gmpeNames
+                            + logicTree.getBranchingLevelAt(0).getBranch(i)
+                                    .getBranchingValue() + " ";
+            weights =
+                    weights
+                            + logicTree.getBranchingLevelAt(0).getBranch(i)
+                                    .getWeight() + " ";
+        }
+        gmpeNames = gmpeNames.trim();
+        weights = weights.trim();
+        gmpeLogicTreeHashMap.put(
+                TectonicRegionType.getTypeForName(tectReg),
+                createGmpeLogicTree(gmpeNames, weights, component,
+                        intensityMeasureType, period, damping, truncType,
+                        truncLevel, stdType, vs30));
+    }
+
+    /**
+     * create GMPE logic tree from string of names and string of weights
+     */
+    private LogicTree<ScalarIntensityMeasureRelationshipAPI>
+            createGmpeLogicTree(String gmpeNames, String gmpeWeights,
+                    String component, String intensityMeasureType,
+                    double period, double damping, String truncType,
+                    double truncLevel, String stdType, double vs30) {
+
+        ParameterChangeWarningEvent event = null;
+
+        StringTokenizer name = new StringTokenizer(gmpeNames);
+        StringTokenizer weight = new StringTokenizer(gmpeWeights);
+        if (name.countTokens() != weight.countTokens()) {
+            String msg =
+                    "Number of gmpes do not corresponds to number of weights!\n"
+                            + "Check your input!\n" + "Execution stopped!\n";
+            logger.fatal(msg);
+            throw new IllegalArgumentException(msg);
+        }
+
+        // create logic tree structure consisting of only one branching level
+        // the number of branches in the branching level corresponds to the
+        // number of gmpes defined
+        LogicTree<ScalarIntensityMeasureRelationshipAPI> gmpeLogicTree =
+                new LogicTree<ScalarIntensityMeasureRelationshipAPI>();
+        LogicTreeBranchingLevel branchingLevel =
+                new LogicTreeBranchingLevel(1, "", 0);
+        LogicTreeBranch branch = null;
+        int numBranch = name.countTokens();
+        for (int i = 0; i < numBranch; i++) {
+            String gmpeName = name.nextToken();
+            double gmpeWeight = Double.parseDouble(weight.nextToken());
+            branch = new LogicTreeBranch((i + 1), gmpeName, gmpeWeight);
+            branchingLevel.addBranch(branch);
+        }
+        gmpeLogicTree.appendBranchingLevel(branchingLevel);
+
+        // instantiate GMPE for each branch through reflection
+        for (int i = 0; i < numBranch; i++) {
+            String gmpeName =
+                    gmpeLogicTree.getBranchingLevelAt(0).getBranch(i)
+                            .getBranchingValue();
+            Class cl = null;
+            Constructor cstr = null;
+            AttenuationRelationship ar = null;
+            try {
+                cl = Class.forName(packageName + gmpeName);
+                cstr =
+                        cl.getConstructor(new Class[] { ParameterChangeWarningListener.class });
+                ar =
+                        (AttenuationRelationship) cstr
+                                .newInstance(warningListener);
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+            ar.setParamDefaults();
+            setGmpeParams(component, intensityMeasureType, period, damping,
+                    truncType, truncLevel, stdType, vs30, ar);
+            gmpeLogicTree.getEBMap().put(
+                    Integer.toString(gmpeLogicTree.getBranchingLevelAt(0)
+                            .getBranch(i).getRelativeID()), ar);
+        }
+        return gmpeLogicTree;
+    }
+
+    /**
+     * Set GMPE parameters
+     */
+    public void setGmpeParams(String component, String intensityMeasureType,
+            double period, double damping, String truncType, double truncLevel,
+            String stdType, double vs30, AttenuationRelationship ar) {
+        String gmpeName = ar.getClass().getCanonicalName();
+
+        ar.setComponentParameter(component, intensityMeasureType);
+        
+        if (ar.getSupportedIntensityMeasuresList().containsParameter(
+                intensityMeasureType)) {
+            ar.setIntensityMeasure(intensityMeasureType);
+        } else {
+            String msg =
+                    "The chosen intensity measure type: "
+                            + intensityMeasureType + " is not supported by "
+                            + gmpeName + "\n"
+                            + "The supported types are the following:\n"
+                            + ar.getSupportedIntensityMeasuresList().toString()
+                            + "\n" + "Check your input file!\n"
+                            + "Execution stopped.";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        if (intensityMeasureType.equalsIgnoreCase(SA_Param.NAME)) {
+            if (ar.getParameter(PeriodParam.NAME).isAllowed(period)) {
+                ar.getParameter(PeriodParam.NAME).setValue(period);
+            } else {
+                String msg =
+                        "The chosen period: "
+                                + period
+                                + " is not supported by "
+                                + gmpeName
+                                + "\n"
+                                + "The allowed values are the following:\n"
+                                + ar.getParameter(PeriodParam.NAME)
+                                        .getConstraint() + "\n"
+                                + "Check your input file\n"
+                                + "Execution stopped.";
+                logger.error(msg);
+                new IllegalArgumentException(msg);
+            }
+            if (ar.getParameter(DampingParam.NAME).isAllowed(damping)) {
+                ar.getParameter(DampingParam.NAME).setValue(damping);
+            } else {
+                String msg =
+                        "The chosen damping: "
+                                + damping
+                                + " is not supported by "
+                                + gmpeName
+                                + "\n"
+                                + "The allowed values are the following:\n"
+                                + ar.getParameter(DampingParam.NAME)
+                                        .getConstraint() + "\n"
+                                + "Check your input file\n"
+                                + "Execution stopped.";
+                logger.error(msg);
+                throw new IllegalArgumentException(msg);
+            }
+        }
+        if (ar.getParameter(SigmaTruncTypeParam.NAME).isAllowed(truncType)) {
+            ar.getParameter(SigmaTruncTypeParam.NAME).setValue(truncType);
+        } else {
+            String msg =
+                    "The chosen truncation type: "
+                            + truncType
+                            + " is not supported.\n"
+                            + "The allowed values are the following:\n"
+                            + ar.getParameter(SigmaTruncTypeParam.NAME)
+                                    .getConstraint() + "\n"
+                            + "Check your input file\n" + "Execution stopped.";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        if (ar.getParameter(SigmaTruncLevelParam.NAME).isAllowed(truncLevel)) {
+            ar.getParameter(SigmaTruncLevelParam.NAME).setValue(truncLevel);
+        } else {
+            String msg =
+                    "The chosen truncation level: "
+                            + truncLevel
+                            + " is not supported.\n"
+                            + "The allowed values are the following: \n"
+                            + ar.getParameter(SigmaTruncLevelParam.NAME)
+                                    .getConstraint() + "\n"
+                            + "Check your input file\n" + "Execution stopped.";
+            logger.error(msg);
+            throw new IllegalArgumentException(msg);
+        }
+        if (ar.getParameter(StdDevTypeParam.NAME).isAllowed(stdType)) {
+            ar.getParameter(StdDevTypeParam.NAME).setValue(stdType);
+        } else {
+            String msg =
+                    "The chosen standard deviation type: "
+                            + stdType
+                            + " is not supported by "
+                            + gmpeName
+                            + "\n"
+                            + "The allowed values are the following: \n"
+                            + ar.getParameter(StdDevTypeParam.NAME)
+                                    .getConstraint() + "\n"
+                            + "Check your input file\n" + "Execution stopped.";
+            logger.error(msg);
+            new IllegalArgumentException(msg);
+        }
+    }
+
+    /**
+     * Returns map of GMPE {@link LogicTree} with {@link TectonicRegionType} as
+     * keys
+     */
+    public
+            HashMap<TectonicRegionType, LogicTree<ScalarIntensityMeasureRelationshipAPI>>
+            getGmpeLogicTreeHashMap() {
+        return this.gmpeLogicTreeHashMap;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/LogicTreeProcessor.java
@@ -0,0 +1,950 @@
+package org.gem.engine;
+
+import java.io.IOException;
+import java.lang.reflect.Type;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Random;
+
+import org.apache.commons.configuration.AbstractFileConfiguration;
+import org.apache.commons.configuration.Configuration;
+import org.apache.commons.configuration.ConfigurationConverter;
+import org.apache.commons.configuration.ConfigurationException;
+import org.apache.commons.configuration.PropertiesConfiguration;
+import org.apache.commons.io.FilenameUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.gem.JsonSerializer;
+import org.gem.ScalarIMRJsonAdapter;
+import org.gem.engine.CalculatorConfigHelper.ConfigItems;
+import org.gem.engine.hazard.parsers.SourceModelReader;
+import org.gem.engine.hazard.redis.Cache;
+import org.gem.engine.logictree.LogicTree;
+import org.gem.engine.logictree.LogicTreeBranch;
+import org.gem.engine.logictree.LogicTreeRule;
+import org.gem.engine.logictree.LogicTreeRuleParam;
+import org.opensha.commons.data.TimeSpan;
+import org.opensha.commons.geo.Location;
+import org.opensha.sha.earthquake.EqkRupture;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.GEM1ERF;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMAreaSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMFaultSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMPointSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMSubductionFaultSourceData;
+import org.opensha.sha.faultSurface.EvenlyGriddedSurfaceAPI;
+import org.opensha.sha.imr.ScalarIntensityMeasureRelationshipAPI;
+import org.opensha.sha.magdist.GutenbergRichterMagFreqDist;
+import org.opensha.sha.magdist.IncrementalMagFreqDist;
+import org.opensha.sha.util.TectonicRegionType;
+
+import com.google.gson.Gson;
+import com.google.gson.GsonBuilder;
+import com.google.gson.reflect.TypeToken;
+
+public class LogicTreeProcessor {
+
+    /*
+     * Apache commons logging, not log4j specifically Note that for application
+     * code, declaring the log member as "static" is more efficient as one Log
+     * object is created per class, and is recommended. However this is not safe
+     * to do for a class which may be deployed via a "shared" classloader in a
+     * servlet or j2ee container or similar environment. If the class may end up
+     * invoked with different thread-context-classloader values set then the
+     * member must not be declared static. The use of "static" should therefore
+     * be avoided in code within any "library" type project.
+     */
+    private static Log logger = LogFactory.getLog(LogicTreeProcessor.class);
+
+    private final Configuration config;
+    private boolean hasPath;
+    private Cache kvs;
+
+    public LogicTreeProcessor(Properties p) {
+        config = ConfigurationConverter.getConfiguration(p);
+    }
+
+    public LogicTreeProcessor(String calcConfigFile)
+            throws ConfigurationException {
+        config = new PropertiesConfiguration();
+        ((PropertiesConfiguration) config).load(calcConfigFile);
+        logger.info(config);
+        hasPath = true;
+    }
+
+    /**
+     * Create LogicTreeProcessor by loading a job configuration from the
+     * available KVS. The configuration file is serialized as JSON.
+     *
+     * @param cache
+     *            - KVS connection
+     * @param key
+     *            - key used to retrieve the job config from the KVS
+     */
+    public LogicTreeProcessor(Cache cache, String key) {
+        kvs = cache;
+        Properties properties =
+                new Gson().fromJson((String) cache.get(key), Properties.class);
+
+        config = ConfigurationConverter.getConfiguration(properties);
+    }
+
+    private String configFilesPath() {
+        return FilenameUtils.getFullPath(((AbstractFileConfiguration) config)
+                .getPath());
+    }
+
+    private String getRelativePath(String key) {
+        return configFilesPath() + config.getString(key);
+    }
+
+    /**
+     * Two calculators are equal when have the same configuration.
+     *
+     * @param obj
+     *            the calculator to compare on
+     * @return true if the calculators are equal, false otherwise
+     */
+    @Override
+    public boolean equals(Object obj) {
+        if (!(obj instanceof LogicTreeProcessor)) {
+            return false;
+        }
+
+        LogicTreeProcessor other = (LogicTreeProcessor) obj;
+
+        Properties thisConfig = ConfigurationConverter.getProperties(config);
+
+        Properties otherConfig =
+                ConfigurationConverter.getProperties(other.config);
+
+        return thisConfig.equals(otherConfig);
+    }
+
+    /**
+     * Extracts from an EqkRupture object all data to be contained by a NRML
+     * instance. This is then converted to json format, and saved in the KVS
+     * with the passed key.
+     *
+     * @param rup
+     * @param key
+     * @param cache
+     */
+    public void serializeEqkRuptureToKvs(EqkRupture rup, String key, Cache cache) {
+        Gson g = new Gson();
+        String jsonData = g.toJson(new EqkRuptureDataForNrml(rup));
+        cache.set(key, jsonData);
+    }
+
+    /**
+     * A class ready to be converted by gson. This class extracts from an
+     * EqkRupture object all data to be contained by a NRML instance. That
+     * conversion by gson is supposed to result in a json String that is optimal
+     * to be read into a numpy array.
+     */
+    public class EqkRuptureDataForNrml {
+        private transient final String unknownTectonicRegionType = "Unknown";
+        private final double averageRake;
+        private String tectonicRegion;
+        private final double magRupture;
+        int numberOfColumns;
+        int numberOfRows;
+        private final double[] latGrid;
+        private final double[] lonGrid;
+        private final double[] depthGrid;
+
+        public EqkRuptureDataForNrml(EqkRupture rup) {
+            averageRake = rup.getAveRake();
+            if (rup.getTectRegType() != null) {
+                tectonicRegion = rup.getTectRegType().toString();
+            } else {
+                tectonicRegion = unknownTectonicRegionType;
+            }
+            magRupture = rup.getMag();
+            EvenlyGriddedSurfaceAPI grid = rup.getRuptureSurface();
+            /*
+             * the site data
+             */
+            numberOfColumns = grid.getNumCols();
+            numberOfRows = grid.getNumRows();
+            int countSites = numberOfColumns * numberOfRows;
+            latGrid = new double[countSites];
+            lonGrid = new double[countSites];
+            depthGrid = new double[countSites];
+            for (int row = 0; row < numberOfRows; row++) {
+                for (int col = 0; col < numberOfColumns; col++) {
+                    Location l = grid.get(row, col);
+                    int index = (row) * numberOfColumns + (col);
+                    latGrid[index] = l.getLatitude();
+                    lonGrid[index] = l.getLongitude();
+                    depthGrid[index] = l.getDepth();
+                } // for columns
+            } // for rows
+        } // constructor()
+
+        /**
+         * Getters and setters - may help, also when searching errors
+         */
+        public double[] getLatGrid() {
+            return latGrid;
+        }
+
+        /**
+         * Getters and setters - may help, also when searching errors
+         */
+        public double[] getLonGrid() {
+            return lonGrid;
+        }
+
+        /**
+         * Getters and setters - may help, also when searching errors
+         */
+        public double[] getDepthGrid() {
+            return depthGrid;
+        }
+    } // class EqkRuptureDataForKvs
+
+    /**
+     * Creates an ERF tree and writes it to the KVS, serialized as JSON.
+     *
+     * @param cache
+     *            - KVS
+     * @param key
+     *            - key of the data to be stored in the KVS
+     * @param seed
+     * @throws IOException
+     */
+    public void sampleAndSaveERFTree(Cache cache, String key, long seed)
+            throws IOException {
+        logger.warn("Random seed for ERFLT is " + Long.toString(seed));
+        ArrayList<GEMSourceData> arrayListSources =
+                new ArrayList(sampleSourceModelLogicTree(
+                        createErfLogicTreeData(), seed));
+        JsonSerializer.serializeSourceList(cache, key, arrayListSources);
+    }
+
+    public void sampleAndSaveGMPETree(Cache cache, String key, long seed)
+            throws IOException {
+        logger.warn("Random seed for GMPELT is " + Long.toString(seed));
+        HashMap<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> gmpe_map =
+                sampleGemLogicTreeGMPE(createGmpeLogicTreeData()
+                        .getGmpeLogicTreeHashMap(), seed);
+
+        GsonBuilder gson = new GsonBuilder();
+        gson.registerTypeAdapter(ScalarIntensityMeasureRelationshipAPI.class,
+                new ScalarIMRJsonAdapter());
+
+        Type hashType =
+                new TypeToken<HashMap<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI>>() {
+                }.getType();
+        logger.debug("GMPE HASHMAP: " + gmpe_map);
+        String json = gson.create().toJson(gmpe_map, hashType);
+        cache.set(key, json);
+    }
+
+    /**
+     * Generate N source models (each represented by an array list of
+     * GEMSourceData objects), by randomly sampling the source model logic tree.
+     *
+     * @param lt
+     *            : source model logic tree
+     * @param N
+     *            : number of models to be generated
+     * @param seed
+     *            : seed number for the random number generator
+     * @return
+     */
+    public List<GEMSourceData> sampleSourceModelLogicTree(
+            LogicTree<ArrayList<GEMSourceData>> lt, long seed) {
+
+        List<GEMSourceData> srcList = null;
+        Random rn = new Random(seed);
+
+        // sample first branching level to get the starting source model
+        int branchNumber = lt.sampleBranchingLevel(0, rn);
+        LogicTreeBranch branch =
+                lt.getBranchingLevelAt(0).getBranch(branchNumber - 1);
+        if (branch.getNameInputFile() != null) {
+            String sourceName = null;
+            if (hasPath) { // job from file
+                sourceName = configFilesPath() + branch.getNameInputFile();
+            } else { // job from kvs
+                sourceName =
+                        FilenameUtils.concat(config.getString("BASE_PATH"),
+                                branch.getNameInputFile());
+            }
+
+            SourceModelReader sourceModelReader =
+                    new SourceModelReader(sourceName, config
+                            .getDouble(ConfigItems.WIDTH_OF_MFD_BIN.name()));
+
+            // load sources
+            srcList = sourceModelReader.read();
+
+        } else {
+            String msg =
+                    "The first branching level of the ERF logic tree does"
+                            + " not contain a source model!!\n"
+                            + "Please correct your input!\n Execution stopped!";
+            logger.info(msg);
+            throw new IllegalArgumentException(msg);
+        }
+
+        // loop over sources
+        // for each source, loop over remaining branching levels and apply
+        // uncertainties
+        int numBranchingLevels = lt.getBranchingLevels().size();
+        int sourceIndex = 0;
+        for (GEMSourceData src : srcList) {
+            for (int i = 1; i < numBranchingLevels; i++) {
+                // sample the current branching level
+                branchNumber = lt.sampleBranchingLevel(i, rn);
+                // get the sampled branch
+                branch = lt.getBranchingLevelAt(i).getBranch(branchNumber - 1);
+                if (branch.getRule() != null) {
+                    // at the moment we apply rules to all source
+                    // typologies. In
+                    // the future we may want
+                    // to apply some filter (i.e. apply rule to this source
+                    // type
+                    // only...)
+                    // if area source
+                    if (src instanceof GEMAreaSourceData) {
+                        // replace the old source with the new source
+                        // accordingly to the rule
+                        srcList.set(sourceIndex, applyRuleToAreaSource(
+                                (GEMAreaSourceData) src, branch.getRule()));
+                    }
+                    // if point source
+                    if (src instanceof GEMPointSourceData) {
+                        // replace the old source with the new source
+                        // accordingly to the rule
+                        srcList.set(sourceIndex, applyRuleToPointSource(
+                                (GEMPointSourceData) src, branch.getRule()));
+                    }
+                    // if fault source
+                    if (src instanceof GEMFaultSourceData) {
+                        // replace the old source with the new source
+                        // accordingly to the rule
+                        srcList.set(sourceIndex, applyRuleToFaultSource(
+                                (GEMFaultSourceData) src, branch.getRule()));
+                    }
+                    // if subduction source
+                    if (src instanceof GEMSubductionFaultSourceData) {
+                        // replace the old source with the new source
+                        // accordingly to the rule
+                        srcList.set(sourceIndex,
+                                applyRuleToSubductionFaultSource(
+                                        (GEMSubductionFaultSourceData) src,
+                                        branch.getRule()));
+                    }
+                } else {
+                    // rule is not defined:
+                    String msg =
+                            "No rule is defined at branching level: " + i
+                                    + "\n" + "Please correct your input!\n"
+                                    + "Execution stopped!";
+                    logger.info(msg);
+                    throw new IllegalArgumentException(msg);
+                } // end if no rule is defined
+            } // end loop over branching levels
+            sourceIndex = sourceIndex + 1;
+        } // end loop over sources
+        return srcList;
+    }
+
+    /**
+     * This method applies an "uncertainty" rule to an area source data object
+     *
+     * @param areaSrc
+     *            : source data object subject to uncertainty
+     * @param rule
+     *            : GEMLogicTreeRule specifing parameter uncertainty
+     * @return: a new GEMAreaSourceData object with the parameter subject to the
+     *          uncertainty changed according to the rule. In case the rule is
+     *          not recognized an error is thrown and execution stops
+     */
+    private static GEMAreaSourceData applyRuleToAreaSource(
+            GEMAreaSourceData areaSrc, LogicTreeRule rule) {
+        // define new area source
+        GEMAreaSourceData newAreaSrc = areaSrc;
+        // if uncertainties on GR Mmax or GR b value
+        if (rule.getRuleName().toString().equalsIgnoreCase(
+                LogicTreeRuleParam.mMaxGRRelative.toString())
+                || rule.getRuleName().toString().equalsIgnoreCase(
+                        LogicTreeRuleParam.bGRRelative.toString())) {
+            // loop over mfds
+            // mfd index
+            int mfdIndex = 0;
+            for (IncrementalMagFreqDist mfd : areaSrc.getMagfreqDistFocMech()
+                    .getMagFreqDistList()) {
+                if (mfd instanceof GutenbergRichterMagFreqDist) {
+                    // new mfd
+                    GutenbergRichterMagFreqDist newMfdGr = null;
+                    if (rule.getRuleName().toString().equalsIgnoreCase(
+                            LogicTreeRuleParam.mMaxGRRelative.toString())) {
+                        // uncertainties on Mmax
+                        newMfdGr =
+                                applyMmaxGrRelative(
+                                        (GutenbergRichterMagFreqDist) mfd, rule
+                                                .getVal(), areaSrc.getName());
+                    } else if (rule.getRuleName().toString().equalsIgnoreCase(
+                            LogicTreeRuleParam.bGRRelative.toString())) {
+                        // uncertainties on b value
+                        newMfdGr =
+                                applybGrRelative(
+                                        (GutenbergRichterMagFreqDist) mfd, rule
+                                                .getVal(), areaSrc.getName());
+                    }
+                    // substitute old mfd with new mfd
+                    newAreaSrc.getMagfreqDistFocMech().getMagFreqDistList()[mfdIndex] =
+                            newMfdGr;
+                } // end if mfd is GR
+                mfdIndex = mfdIndex + 1;
+            } // for (loop over mfds)
+            // return new area source
+            return newAreaSrc;
+        } else {
+            // not(rule == mMaxGRRelative || == bGRRelative)
+            String msg =
+                    "Rule: " + rule.getRuleName().toString()
+                            + " not supported.\n"
+                            + "Check your input. Execution is stopped.";
+            logger.info(msg);
+            throw new IllegalArgumentException(msg);
+        }
+    } // applyRuleToAreaSource()
+
+    /**
+     * This method applies an "uncertainty" rule to a point source data object
+     *
+     * @param pntSrc
+     *            : source data object subject to uncertainty
+     * @param rule
+     *            : GEMLogicTreeRule specifing parameter uncertainty
+     * @return: a new GEMPointSourceData object with the parameter subject to
+     *          the uncertainty changed according to the rule. In case the rule
+     *          is not recognized an error is thrown and execution stops
+     */
+    private static GEMPointSourceData applyRuleToPointSource(
+            GEMPointSourceData pntSrc, LogicTreeRule rule) {
+        // new point source
+        GEMPointSourceData newPntSource = pntSrc;
+        // if uncertainties on GR Mmax or GR b value
+        if (rule.getRuleName().toString().equalsIgnoreCase(
+                LogicTreeRuleParam.mMaxGRRelative.toString())
+                || rule.getRuleName().toString().equalsIgnoreCase(
+                        LogicTreeRuleParam.bGRRelative.toString())) {
+            // loop over mfds
+            // mfd index
+            int mfdIndex = 0;
+            for (IncrementalMagFreqDist mfd : pntSrc.getHypoMagFreqDistAtLoc()
+                    .getMagFreqDistList()) {
+                if (mfd instanceof GutenbergRichterMagFreqDist) {
+                    GutenbergRichterMagFreqDist newMfdGr = null;
+                    // create new mfd by applying rule
+                    if (rule.getRuleName().toString().equalsIgnoreCase(
+                            LogicTreeRuleParam.mMaxGRRelative.toString())) {
+                        newMfdGr =
+                                applyMmaxGrRelative(
+                                        (GutenbergRichterMagFreqDist) mfd, rule
+                                                .getVal(), pntSrc.getName());
+                    } else if (rule.getRuleName().toString().equalsIgnoreCase(
+                            LogicTreeRuleParam.bGRRelative.toString())) {
+                        newMfdGr =
+                                applybGrRelative(
+                                        (GutenbergRichterMagFreqDist) mfd, rule
+                                                .getVal(), pntSrc.getName());
+                    }
+                    // substitute old mfd with new mfd
+                    newPntSource.getHypoMagFreqDistAtLoc().getMagFreqDistList()[mfdIndex] =
+                            newMfdGr;
+                } // if mfd is GR
+                mfdIndex = mfdIndex + 1;
+            } // for (loop over mfd)
+            return newPntSource;
+        } else {
+            // not(rule == mMaxGRRelative || == bGRRelative)
+            String msg =
+                    "Rule: " + rule.getRuleName().toString()
+                            + " not supported.\n"
+                            + "Check your input. Execution is stopped.";
+            logger.info(msg);
+            throw new IllegalArgumentException(msg);
+        }
+    } // applyRuleToPointSource()
+
+    /**
+     * This method applies an "uncertainty" rule to a fault source data object
+     *
+     * @param faultSrc
+     *            : source data object subject to uncertainty
+     * @param rule
+     *            : GEMLogicTreeRule specifing parameter uncertainty
+     * @return: a new GEMFaultSourceData object with the parameter subject to
+     *          the uncertainty changed according to the rule. In case the rule
+     *          is not recognized an error is thrown and execution stops
+     */
+    private static GEMFaultSourceData applyRuleToFaultSource(
+            GEMFaultSourceData faultSrc, LogicTreeRule rule) {
+        // if uncertainties on GR Mmax or GR b value
+        if (rule.getRuleName().toString().equalsIgnoreCase(
+                LogicTreeRuleParam.mMaxGRRelative.toString())
+                || rule.getRuleName().toString().equalsIgnoreCase(
+                        LogicTreeRuleParam.bGRRelative.toString())) {
+            // mfd
+            IncrementalMagFreqDist mfd = faultSrc.getMfd();
+            if (mfd instanceof GutenbergRichterMagFreqDist) {
+                GutenbergRichterMagFreqDist newMfdGr = null;
+                // create new mfd by applying rule
+                if (rule.getRuleName().toString().equalsIgnoreCase(
+                        LogicTreeRuleParam.mMaxGRRelative.toString())) {
+                    newMfdGr =
+                            applyMmaxGrRelative(
+                                    (GutenbergRichterMagFreqDist) mfd, rule
+                                            .getVal(), faultSrc.getName());
+                } else if (rule.getRuleName().toString().equalsIgnoreCase(
+                        LogicTreeRuleParam.bGRRelative.toString())) {
+                    newMfdGr =
+                            applybGrRelative((GutenbergRichterMagFreqDist) mfd,
+                                    rule.getVal(), faultSrc.getName());
+                }
+                // return new fault source with new mfd
+                return new GEMFaultSourceData(faultSrc.getID(), faultSrc
+                        .getName(), faultSrc.getTectReg(), newMfdGr, faultSrc
+                        .getTrace(), faultSrc.getDip(), faultSrc.getDip(),
+                        faultSrc.getSeismDepthLow(), faultSrc
+                                .getSeismDepthUpp(), faultSrc
+                                .getFloatRuptureFlag());
+            } else {
+                // mfd is not GR
+                // if the uncertainty do not apply return the unchanged object
+                return faultSrc;
+            }
+        } else {
+            // not(rule == mMaxGRRelative || == bGRRelative)
+            String msg =
+                    "Rule: " + rule.getRuleName().toString()
+                            + " not supported.\n"
+                            + "Check your input. Execution is stopped.";
+            logger.info(msg);
+            throw new IllegalArgumentException(msg);
+        }
+    } // applyRuleToFaultSource()
+
+    /**
+     * This method applies an "uncertainty" rule to a subduction source data
+     * object
+     *
+     * @param subFaultSrc
+     *            : source data object subject to uncertainty
+     * @param rule
+     *            : GEMLogicTreeRule specifing parameter uncertainty
+     * @return: a new GEMSubductionSourceData object with the parameter subject
+     *          to uncertainty changed according to the rule. In case the rule
+     *          is not recognized an error is thrown and execution stops
+     */
+    private static GEMSubductionFaultSourceData applyRuleToSubductionFaultSource(
+            GEMSubductionFaultSourceData subFaultSrc, LogicTreeRule rule) {
+
+        // if uncertainties on GR Mmax or GR b value
+        if (rule.getRuleName().toString().equalsIgnoreCase(
+                LogicTreeRuleParam.mMaxGRRelative.toString())
+                || rule.getRuleName().toString().equalsIgnoreCase(
+                        LogicTreeRuleParam.bGRRelative.toString())) {
+
+            // mfd
+            IncrementalMagFreqDist mfd = subFaultSrc.getMfd();
+
+            if (mfd instanceof GutenbergRichterMagFreqDist) {
+
+                GutenbergRichterMagFreqDist newMfdGr = null;
+
+                // create new mfd by applying rule
+                if (rule.getRuleName().toString().equalsIgnoreCase(
+                        LogicTreeRuleParam.mMaxGRRelative.toString())) {
+                    newMfdGr =
+                            applyMmaxGrRelative(
+                                    (GutenbergRichterMagFreqDist) mfd, rule
+                                            .getVal(), subFaultSrc.getName());
+                } else if (rule.getRuleName().toString().equalsIgnoreCase(
+                        LogicTreeRuleParam.bGRRelative.toString())) {
+                    newMfdGr =
+                            applybGrRelative((GutenbergRichterMagFreqDist) mfd,
+                                    rule.getVal(), subFaultSrc.getName());
+                }
+
+                // return new subduction fault source with the new mfd
+                return new GEMSubductionFaultSourceData(subFaultSrc.getID(),
+                        subFaultSrc.getName(), subFaultSrc.getTectReg(),
+                        subFaultSrc.getTopTrace(),
+                        subFaultSrc.getBottomTrace(), subFaultSrc.getRake(),
+                        newMfdGr, subFaultSrc.getFloatRuptureFlag());
+
+            } // end if mfd is GR
+            // if uncertainty does not apply return unchanged object
+            else {
+                return subFaultSrc;
+            }
+
+        }// end if rule == mMaxGRRelative || == bGRRelative
+        else {
+            String msg =
+                    "Rule: " + rule.getRuleName().toString()
+                            + " not supported.\n"
+                            + "Check your input. Execution is stopped.";
+            logger.info(msg);
+            throw new IllegalArgumentException(msg);
+        }
+    } // applyRuleToSubductionFaultSource()
+
+    /**
+     *
+     * @param mfdGR
+     *            : original magnitude frequency distribution
+     * @param deltaMmax
+     *            : uncertainty on maximum magnitude
+     * @param areaSrc
+     *            : source
+     * @return
+     */
+    private static GutenbergRichterMagFreqDist applyMmaxGrRelative(
+            GutenbergRichterMagFreqDist mfdGR, double deltaMmax,
+            String sourceName) {
+
+        // minimum magnitude
+        double mMin = mfdGR.getMagLower();
+        // b value
+        double bVal = mfdGR.get_bValue();
+        // total moment rate
+        double totMoRate = mfdGR.getTotalMomentRate();
+        // deltaM
+        double deltaM = mfdGR.getDelta();
+
+        // calculate new mMax value
+        // old mMax value
+        double mMax = mfdGR.getMagUpper();
+        // add uncertainty value (deltaM/2 is added because mMax
+        // refers to bin center
+        mMax = mMax + deltaM / 2 + deltaMmax;
+        // round mMax with respect to deltaM
+        mMax = Math.round(mMax / deltaM) * deltaM;
+        // move back to bin center
+        mMax = mMax - deltaM / 2;
+        // logger.info("New mMax: "+mMax);
+
+        if (mMax - mMin >= deltaM) {
+
+            // calculate number of magnitude values
+            int numVal = (int) Math.round((mMax - mMin) / deltaM + 1);
+
+            // create new GR mfd
+            GutenbergRichterMagFreqDist newMfdGr =
+                    new GutenbergRichterMagFreqDist(mMin, numVal, deltaM);
+            newMfdGr.setAllButTotCumRate(mMin, mMax, totMoRate, bVal);
+
+            // return new mfd
+            return newMfdGr;
+
+        } else {
+            // stop execution and return null
+            logger
+                    .info("Uncertaintiy value: "
+                            + deltaMmax
+                            + " on maximum magnitude for source: "
+                            + sourceName
+                            + " give maximum magnitude smaller than minimum magnitude!\n"
+                            + "Check your input. Execution stopped.");
+            // logger.info("Uncertaintiy value: " + deltaMmax +
+            // " on maximum magnitude for source: " + sourceName
+            // + " give maximum magnitude smaller than minimum magnitude!");
+            // logger.info("Check your input. Execution stopped.");
+            return null;
+        }
+
+    }
+
+    private static GutenbergRichterMagFreqDist applybGrRelative(
+            GutenbergRichterMagFreqDist mfdGR, double deltaB, String sourceName) {
+
+        // minimum magnitude
+        double mMin = mfdGR.getMagLower();
+        // maximum magnitude
+        double mMax = mfdGR.getMagUpper();
+        // b value
+        double bVal = mfdGR.get_bValue();
+        // total moment rate
+        double totMoRate = mfdGR.getTotalMomentRate();
+        // deltaM
+        double deltaM = mfdGR.getDelta();
+
+        // calculate new b value
+        bVal = bVal + deltaB;
+
+        if (bVal >= 0.0) {
+
+            // calculate number of magnitude values
+            int numVal = (int) Math.round((mMax - mMin) / deltaM + 1);
+
+            // create new GR mfd
+            GutenbergRichterMagFreqDist newMfdGr =
+                    new GutenbergRichterMagFreqDist(mMin, numVal, deltaM);
+            newMfdGr.setAllButTotCumRate(mMin, mMax, totMoRate, bVal);
+
+            // return new mfd
+            return newMfdGr;
+
+        } else {
+            String msg =
+                    "Uncertaintiy value: " + deltaB
+                            + " on b value for source: " + sourceName
+                            + " give b value smaller than 0!\n"
+                            + "Check your input. Execution stopped!";
+            logger.info(msg);
+            // logger.info("Uncertaintiy value: " + deltaB +
+            // " on b value for source: " + sourceName
+            // + " give b value smaller than 0!");
+            // logger.info("Check your input. Execution stopped!");
+            throw new IllegalArgumentException(msg);
+        }
+    } // applybGrRelative()
+
+    /**
+     * Set the GEM1ERF params given the parameters defined in
+     *
+     * @param erf
+     *            : erf for which parameters have to be set
+     * @param calcConfig
+     *            : calculator configuration obejct containing parameters for
+     *            the ERF
+     */
+    public void setGEM1ERFParams(GEM1ERF erf) {
+        // set minimum magnitude
+        /*
+         * xxr: TODO: !!!type safety!!! apache's Configuration interface handles
+         * a similar problem this way: Instead of defining one single method
+         * like public void setParameter(String key, Object value) {...} there
+         * is one method per type defined: setString(), setDouble(), setInt(),
+         * ...
+         */
+        erf.setParameter(GEM1ERF.MIN_MAG_NAME, config
+                .getDouble(ConfigItems.MINIMUM_MAGNITUDE.name()));
+        // set time span
+        TimeSpan timeSpan = new TimeSpan(TimeSpan.NONE, TimeSpan.YEARS);
+        timeSpan.setDuration(config.getDouble(ConfigItems.INVESTIGATION_TIME
+                .name()));
+        erf.setTimeSpan(timeSpan);
+
+        // params for area source
+        // set inclusion of area sources in the calculation
+        erf.setParameter(GEM1ERF.INCLUDE_AREA_SRC_PARAM_NAME, config
+                .getBoolean(ConfigItems.INCLUDE_AREA_SOURCES.name()));
+        // set rupture type ("area source rupture model /
+        // area_source_rupture_model / AreaSourceRuptureModel)
+        erf.setParameter(GEM1ERF.AREA_SRC_RUP_TYPE_NAME, config
+                .getString(ConfigItems.TREAT_AREA_SOURCE_AS.name()));
+        // set area discretization
+        erf.setParameter(GEM1ERF.AREA_SRC_DISCR_PARAM_NAME, config
+                .getDouble(ConfigItems.AREA_SOURCE_DISCRETIZATION.name()));
+        // set mag-scaling relationship
+        erf
+                .setParameter(
+                        GEM1ERF.AREA_SRC_MAG_SCALING_REL_PARAM_NAME,
+                        config
+                                .getString(ConfigItems.AREA_SOURCE_MAGNITUDE_SCALING_RELATIONSHIP
+                                        .name()));
+        // params for grid source
+        // inclusion of grid sources in the calculation
+        erf.setParameter(GEM1ERF.INCLUDE_GRIDDED_SEIS_PARAM_NAME, config
+                .getBoolean(ConfigItems.INCLUDE_GRID_SOURCES.name()));
+        // rupture model
+        erf.setParameter(GEM1ERF.GRIDDED_SEIS_RUP_TYPE_NAME, config
+                .getString(ConfigItems.TREAT_GRID_SOURCE_AS.name()));
+        // mag-scaling relationship
+        erf
+                .setParameter(
+                        GEM1ERF.GRIDDED_SEIS_MAG_SCALING_REL_PARAM_NAME,
+                        config
+                                .getString(ConfigItems.AREA_SOURCE_MAGNITUDE_SCALING_RELATIONSHIP
+                                        .name()));
+
+        // params for fault source
+        // inclusion of fault sources in the calculation
+        erf.setParameter(GEM1ERF.INCLUDE_FAULT_SOURCES_PARAM_NAME, config
+                .getBoolean(ConfigItems.INCLUDE_FAULT_SOURCE.name()));
+        // rupture offset
+        erf.setParameter(GEM1ERF.FAULT_RUP_OFFSET_PARAM_NAME, config
+                .getDouble(ConfigItems.FAULT_RUPTURE_OFFSET.name()));
+        // surface discretization
+        erf.setParameter(GEM1ERF.FAULT_DISCR_PARAM_NAME, config
+                .getDouble(ConfigItems.FAULT_SURFACE_DISCRETIZATION.name()));
+        // mag-scaling relationship
+        erf.setParameter(GEM1ERF.FAULT_MAG_SCALING_REL_PARAM_NAME, config
+                .getString(ConfigItems.FAULT_MAGNITUDE_SCALING_RELATIONSHIP
+                        .name()));
+
+        // mag-scaling sigma
+        erf.setParameter(GEM1ERF.FAULT_SCALING_SIGMA_PARAM_NAME, config
+                .getDouble(ConfigItems.FAULT_MAGNITUDE_SCALING_SIGMA.name()));
+        // rupture aspect ratio
+        erf.setParameter(GEM1ERF.FAULT_RUP_ASPECT_RATIO_PARAM_NAME, config
+                .getDouble(ConfigItems.RUPTURE_ASPECT_RATIO.name()));
+        // rupture floating type
+        erf.setParameter(GEM1ERF.FAULT_FLOATER_TYPE_PARAM_NAME, config
+                .getString(ConfigItems.RUPTURE_FLOATING_TYPE.name()));
+
+        // params for subduction fault
+        // inclusion of fault sources in the calculation
+        erf
+                .setParameter(
+                        GEM1ERF.INCLUDE_SUBDUCTION_SOURCES_PARAM_NAME,
+                        config
+                                .getBoolean(ConfigItems.INCLUDE_SUBDUCTION_FAULT_SOURCE
+                                        .name()));
+        // rupture offset
+        erf.setParameter(GEM1ERF.SUB_RUP_OFFSET_PARAM_NAME, config
+                .getDouble(ConfigItems.SUBDUCTION_FAULT_RUPTURE_OFFSET.name()));
+        // surface discretization
+        erf.setParameter(GEM1ERF.SUB_DISCR_PARAM_NAME, config
+                .getDouble(ConfigItems.SUBDUCTION_FAULT_SURFACE_DISCRETIZATION
+                        .name()));
+        // mag-scaling relationship
+        erf
+                .setParameter(
+                        GEM1ERF.SUB_MAG_SCALING_REL_PARAM_NAME,
+                        config
+                                .getString(ConfigItems.SUBDUCTION_FAULT_MAGNITUDE_SCALING_RELATIONSHIP
+                                        .name()));
+        // mag-scaling sigma
+        erf.setParameter(GEM1ERF.SUB_SCALING_SIGMA_PARAM_NAME, config
+                .getDouble(ConfigItems.SUBDUCTION_FAULT_MAGNITUDE_SCALING_SIGMA
+                        .name()));
+        // rupture aspect ratio
+        erf.setParameter(GEM1ERF.SUB_RUP_ASPECT_RATIO_PARAM_NAME, config
+                .getDouble(ConfigItems.SUBDUCTION_RUPTURE_ASPECT_RATIO.name()));
+        // rupture floating type
+        erf
+                .setParameter(GEM1ERF.SUB_FLOATER_TYPE_PARAM_NAME, config
+                        .getString(ConfigItems.SUBDUCTION_RUPTURE_FLOATING_TYPE
+                                .name()));
+
+        // update
+        erf.updateForecast();
+    } // setGEM1ERFParams()
+
+    public static HashMap<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> sampleGemLogicTreeGMPE(
+            HashMap<TectonicRegionType, LogicTree<ScalarIntensityMeasureRelationshipAPI>> listLtGMPE,
+            long seed) {
+
+        Random rn = null;
+        if (seed != 0) {
+            rn = new Random(seed);
+        } else {
+            rn = new Random();
+        }
+
+        HashMap<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI> hm =
+                new HashMap<TectonicRegionType, ScalarIntensityMeasureRelationshipAPI>();
+
+        // loop over tectonic regions
+        Iterator<TectonicRegionType> iter = listLtGMPE.keySet().iterator();
+        while (iter.hasNext()) {
+
+            // get tectonic region type
+            TectonicRegionType trt = iter.next();
+
+            // get corresponding logic tree
+            LogicTree<ScalarIntensityMeasureRelationshipAPI> ltGMPE =
+                    listLtGMPE.get(trt);
+
+            ltGMPE.toString();
+
+            // sample the first branching level
+            int branch = ltGMPE.sampleBranchingLevel(0, rn);
+
+            // select the corresponding gmpe from the end-branch mapping
+            ScalarIntensityMeasureRelationshipAPI gmpe =
+                    ltGMPE.getEBMap().get(Integer.toString(branch));
+
+            hm.put(trt, gmpe);
+        }
+
+        return hm;
+
+    }
+
+    /**
+     * Reads source model logic tree data and returns a {@link LogicTree} object
+     * representing epistemic uncertainties in the source model. Logic Tree data
+     * are expected in nrml format and read using a {@link LogicTreeReader}
+     * object. Logic Tree data can be read from file or kvs. The method assumes
+     * that only one source model logic tree is defined and therefore only the
+     * first logic tree read is returned.
+     */
+    public LogicTree createErfLogicTreeData() throws IOException {
+        // Distinguish between reading from file or from kvs. Use a
+        // LogicTreeReader object to read logic tree data and returns the first
+        // logic tree read (this because currently epistemic uncertainties in
+        // the source model are assumed to be described by only one logic tree).
+        if (hasPath == true) {
+            LogicTreeReader logicTreeReader =
+                    new LogicTreeReader(
+                            getRelativePath(ConfigItems.SOURCE_MODEL_LOGIC_TREE_FILE
+                                    .name()));
+            return logicTreeReader.read().get("1");
+        } else {
+            LogicTreeReader logicTreeReader =
+                    new LogicTreeReader(kvs, config
+                            .getString(ConfigItems.SOURCE_MODEL_LOGIC_TREE_FILE
+                                    .name()));
+            return logicTreeReader.read().get("1");
+        }
+    }
+
+    /**
+     * Reads GMPE logic tree data and returns a {@link GmpeLogicTreeData}
+     * containing {@link LogicTree} object(s) defining epistemic uncertainties
+     * on GMPES.
+     */
+    public GmpeLogicTreeData createGmpeLogicTreeData() throws IOException {
+        // read GMPE params from config file. Distinguish between reading from
+        // file or kvs. Then read and instantiate logic tree objects using a
+        // GmpeLogicTreeData object.
+        String component = config.getString(ConfigItems.COMPONENT.name());
+        String intensityMeasureType =
+                config.getString(ConfigItems.INTENSITY_MEASURE_TYPE.name());
+        Double period = config.getDouble(ConfigItems.PERIOD.name());
+        Double damping = config.getDouble(ConfigItems.DAMPING.name());
+        String gmpeTruncationType =
+                config.getString(ConfigItems.GMPE_TRUNCATION_TYPE.name());
+        Double truncationLevel =
+                config.getDouble(ConfigItems.TRUNCATION_LEVEL.name());
+        String standardDeviationType =
+                config.getString(ConfigItems.STANDARD_DEVIATION_TYPE.name());
+        Double referenceVs30Value =
+                config.getDouble(ConfigItems.REFERENCE_VS30_VALUE.name());
+        // instantiate eventually
+        GmpeLogicTreeData gmpeLogicTree = null;
+        if (hasPath == true) {
+            String relativePath =
+                    getRelativePath(ConfigItems.GMPE_LOGIC_TREE_FILE.name());
+            gmpeLogicTree = new GmpeLogicTreeData(relativePath);
+            gmpeLogicTree.parse_tree(component, intensityMeasureType, period,
+                    damping, gmpeTruncationType, truncationLevel,
+                    standardDeviationType, referenceVs30Value);
+        } else {
+            String gmpeSha =
+                    config.getString(ConfigItems.GMPE_LOGIC_TREE_FILE.name());
+            gmpeLogicTree = new GmpeLogicTreeData(kvs, gmpeSha);
+            gmpeLogicTree.parse_tree(component, intensityMeasureType, period,
+                    damping, gmpeTruncationType, truncationLevel,
+                    standardDeviationType, referenceVs30Value);
+        }
+
+        return gmpeLogicTree;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/XMLMismatchError.java
@@ -0,0 +1,32 @@
+package org.gem.engine;
+
+import org.dom4j.DocumentException;
+
+/**
+ * Unexpected document type during parsing
+ */
+public class XMLMismatchError extends RuntimeException {
+    private String fileName, actualTag, expectedTag;
+
+    public XMLMismatchError(String fileName, String actualTag,
+                            String expectedTag) {
+        this.fileName = fileName;
+        this.actualTag = actualTag;
+        this.expectedTag = expectedTag;
+    }
+
+    /** The full path of the invalid file */
+    public String getFileName() {
+        return fileName;
+    }
+
+    /** Main tag of the document (first child of the NRML tag) */
+    public String getActualTag() {
+        return actualTag;
+    }
+
+    /** Expected main tag of the document (first child of the NRML tag) */
+    public String getExpectedTag() {
+        return expectedTag;
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/LogicTreeReader.java
@@ -0,0 +1,257 @@
+package org.gem.engine;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedReader;
+import java.io.ByteArrayInputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+
+import org.dom4j.Document;
+import org.dom4j.Element;
+import org.dom4j.io.SAXReader;
+import org.gem.engine.hazard.redis.Cache;
+import org.gem.engine.logictree.LogicTree;
+import org.gem.engine.logictree.LogicTreeBranch;
+import org.gem.engine.logictree.LogicTreeBranchingLevel;
+import org.gem.engine.logictree.LogicTreeRule;
+import org.gem.engine.logictree.LogicTreeRuleParam;
+
+/**
+ * Class for reading logic tree data in a nrML format file.
+ *
+ */
+public class LogicTreeReader {
+
+    private final BufferedReader bufferedReader;
+
+    private final Map<String, LogicTree> logicTreeHashMap;
+
+    private static final String LOGIC_TREE_SET = "logicTreeSet";
+    private static final String BRANCHING_LEVEL = "branchingLevel";
+    private static final String TECTONIC_REGION = "tectonicRegion";
+    private static final String UNCERTAINTY_TYPE = "uncertaintyType";
+    private static final String UNCERTAINTY_MODEL = "uncertaintyModel";
+    private static final String UNCERTAINTY_WEIGHT = "uncertaintyWeight";
+    private static final String SOURCE_MODEL = "sourceModel";
+    private static final String MAX_MAGNITUDE_GUTENBERG_RICHTER_RELATIVE =
+            "maxMagnitudeGutenbergRichterRelative";
+    private static final String B_VALUE_GUTENBERG_RICHTER_RELATIVE =
+            "bValueGutenbergRichterRelative";
+
+    /**
+     * Creates a new LogicTreeReader given the path of the file to read from.
+     */
+    public LogicTreeReader(String path) {
+        File xml = new File(path);
+        FileInputStream fileInputStream;
+        try {
+            fileInputStream = new FileInputStream(xml.getPath());
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+        BufferedInputStream bufferedInputStream =
+                new BufferedInputStream(fileInputStream);
+        this.bufferedReader =
+                new BufferedReader(new InputStreamReader(bufferedInputStream));
+        logicTreeHashMap = new HashMap<String, LogicTree>();
+    }
+
+    /**
+     * Creates a new LogicTreeReader given cache and key to read from
+     */
+    public LogicTreeReader(Cache cache, String key) {
+        String source = (String) cache.get(key);
+        byte[] bytevals = source.getBytes();
+        InputStream byteis = new ByteArrayInputStream(bytevals);
+        BufferedInputStream bufferedInputStream =
+                new BufferedInputStream(byteis);
+        this.bufferedReader =
+                new BufferedReader(new InputStreamReader(bufferedInputStream));
+        logicTreeHashMap = new HashMap<String, LogicTree>();
+    }
+
+    /**
+     * Creates a new LogicTreeReader given BufferedReader to read from
+     */
+    public LogicTreeReader(BufferedReader bufferedReader) {
+        this.bufferedReader = bufferedReader;
+        logicTreeHashMap = new HashMap<String, LogicTree>();
+    }
+
+    /**
+     * Reads file and returns logic tree data. The method loops over the
+     * possible logic trees defined in the file. For each logic tree definition,
+     * it creates a corresponding {@link LogicTree} object and stores it in a
+     * map with a key that is the logic tree number or the tectonic region type
+     * (if defined in the file).
+     */
+    public Map<String, LogicTree> read() {
+        if (System.getProperty("openquake.nrml.schema") == null)
+            throw new RuntimeException("Set openquake.nrml.schema property to the NRML schema path");
+
+        SAXReader reader = new SAXReader(true);
+        Document doc = null;
+        try {
+            reader.setFeature("http://apache.org/xml/features/validation/schema", true);
+            reader.setProperty("http://java.sun.com/xml/jaxp/properties/schemaLanguage", "http://www.w3.org/2001/XMLSchema");
+            reader.setProperty("http://java.sun.com/xml/jaxp/properties/schemaSource", "file://" + System.getProperty("openquake.nrml.schema"));
+            doc = reader.read(this.bufferedReader);
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+        Element root = doc.getRootElement(); // <nrml> element
+
+        /**
+         * Makes a loop over the possible logic trees defined in the file. In
+         * case of the GMPE logic tree file, multiple logic trees are defined,
+         * one for each tectonic region implied by the source model. For the
+         * source model logic tree file, currently only one logic tree is
+         * defined. For each logic tree definition, creates a logic tree object.
+         * Depending on the uncertainty type, additional attributed of the
+         * branch class must be edited. In case of source model uncertainties, a
+         * source model file must be specified. In case of parameter
+         * uncertainties, a logic tree rule must be defined.
+         */
+        int indexLogicTree = 1;
+        Iterator i = root.elements().iterator();
+        while (i.hasNext()) {
+            Element logicTreeSetElem = (Element) i.next();
+            String localName = logicTreeSetElem.getQName().getName();
+            if (localName != LOGIC_TREE_SET)
+                throw new XMLMismatchError(null, localName, LOGIC_TREE_SET);
+
+            Map<String, LogicTree> logicTrees =
+                    parseLogicTreeSet(logicTreeSetElem, indexLogicTree);
+
+            for (String key : logicTrees.keySet()) {
+                logicTreeHashMap.put(key, logicTrees.get(key));
+            }
+
+            indexLogicTree++;
+        }
+        return logicTreeHashMap;
+    }
+
+    /**
+     * Parse child elements of a &lt;logicTreeSet&gt; element.
+     *
+     * @param logicTreeSet
+     * @param indexLogicTree
+     * @return Map of LogicTrees, keyed by tectonicRegion. If no tectonicRegion
+     *         is defined for a logicTree, keys will be "1" through "N", where N
+     *         is the total number of logicTree elements in the logicTreeSet.
+     */
+    private Map<String, LogicTree> parseLogicTreeSet(Element logicTreeSet,
+            int indexLogicTree) {
+
+        String key = Integer.toString(indexLogicTree);
+        Map<String, LogicTree> logicTrees = new HashMap<String, LogicTree>();
+        Iterator i = logicTreeSet.elementIterator();
+        while (i.hasNext()) {
+            Element elem = (Element) i.next();
+            LogicTree logicTree = new LogicTree();
+
+            // skip config for now
+            // TODO(LB): we might care about the <config> elem later
+            // at the time this was written, the example files did not
+            // include any config items
+            if (elem.getName().equals("config")) {
+                continue;
+            }
+
+            String tectonicRegion = parseLogicTree(elem, logicTree);
+            if (tectonicRegion != null) {
+                key = tectonicRegion;
+            }
+            logicTrees.put(key, logicTree);
+        }
+        return logicTrees;
+    }
+
+    /**
+     * Parse attributes and children of a &lt;logicTree&gt; element.
+     *
+     * @param logicTreeElem
+     * @param logicTree
+     * @return tectonicRegion of the logic tree (or null if none is defined)
+     */
+    private String parseLogicTree(Element logicTreeElem, LogicTree logicTree) {
+
+        String tectonicRegion = logicTreeElem.attributeValue(TECTONIC_REGION);
+
+        Iterator i = logicTreeElem.elementIterator();
+        while (i.hasNext()) {
+            Element branchSet = (Element) i.next();
+
+            parseLogicTreeBranchSet(branchSet, logicTree);
+
+        }
+        return tectonicRegion;
+    }
+
+    /**
+     * Parse attributes and children of a &lt;logicTreeBranchSet&gt; element.
+     *
+     * @param branchSet
+     * @param logicTree
+     */
+    private void parseLogicTreeBranchSet(Element branchSet, LogicTree logicTree) {
+
+        int indexBranchingLevel =
+                Integer.parseInt(branchSet.attributeValue(BRANCHING_LEVEL));
+
+        String uncertaintyType = branchSet.attributeValue(UNCERTAINTY_TYPE);
+        LogicTreeBranchingLevel branchingLevel =
+                new LogicTreeBranchingLevel(indexBranchingLevel, "", 0);
+
+        int indexBranch = 1;
+        Iterator i = branchSet.elementIterator();
+        while (i.hasNext()) {
+            Element logicTreeBranch = (Element) i.next();
+
+            parseLogicTreeBranch(logicTreeBranch, branchingLevel,
+                    uncertaintyType, indexBranch);
+
+            indexBranch++;
+        }
+        logicTree.appendBranchingLevel(branchingLevel);
+    }
+
+    /**
+     * Parse child elements of &lt;logicTreeBranch&gt; element.
+     *
+     * @param logicTreeBranch
+     * @param branchingLevel
+     * @param uncertaintyType
+     * @param indexBranch
+     */
+    private void parseLogicTreeBranch(Element logicTreeBranch,
+            LogicTreeBranchingLevel branchingLevel, String uncertaintyType,
+            int indexBranch) {
+
+        String uncertaintyModel =
+                (String) logicTreeBranch.element(UNCERTAINTY_MODEL).getData();
+        Double uncertaintyWeight =
+                Double.valueOf((String) logicTreeBranch.element(
+                        UNCERTAINTY_WEIGHT).getData());
+        LogicTreeBranch branch =
+                new LogicTreeBranch(indexBranch, uncertaintyModel,
+                        uncertaintyWeight);
+        if (uncertaintyType.equalsIgnoreCase(SOURCE_MODEL))
+            branch.setNameInputFile(uncertaintyModel);
+        else if (uncertaintyType
+                .equalsIgnoreCase(MAX_MAGNITUDE_GUTENBERG_RICHTER_RELATIVE))
+            branch.setRule(new LogicTreeRule(LogicTreeRuleParam.mMaxGRRelative,
+                    Double.valueOf(uncertaintyModel)));
+        else if (uncertaintyType
+                .equalsIgnoreCase(B_VALUE_GUTENBERG_RICHTER_RELATIVE))
+            branch.setRule(new LogicTreeRule(LogicTreeRuleParam.bGRRelative,
+                    Double.valueOf(uncertaintyModel)));
+        branchingLevel.addBranch(branch);
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/logictree/LogicTree.java
@@ -0,0 +1,240 @@
+package org.gem.engine.logictree;
+
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.gem.engine.LogicTreeProcessor;
+
+/**
+ * Class for logic tree definition.
+ * <p>
+ * A logic tree is defined in terms of a list of {@link LogicTreeBranchingLevel} objects (each containing one or more
+ * {@link LogicTreeBranch} objects (each defining an uncertainty model)) plus a map storing end-branch models
+ * (HashMap<String, Element>). A name can be also provided.
+ */
+public class LogicTree<Element> implements Iterable<Element>, Serializable
+{
+
+    private static final long serialVersionUID = -2656457282143245160L;
+    private static final Log logger = LogFactory.getLog(LogicTreeProcessor.class);
+
+    private String name;
+    private Map<String, Element> ebMap;
+    private final List<LogicTreeBranchingLevel> branchingLevels;
+
+    public LogicTree()
+    {
+        this.name = "";
+        this.ebMap = new HashMap<String, Element>();
+        this.branchingLevels = new ArrayList<LogicTreeBranchingLevel>();
+    }
+
+    public LogicTree(String name)
+    {
+        this.name = name;
+        this.ebMap = new HashMap<String, Element>();
+        this.branchingLevels = new ArrayList<LogicTreeBranchingLevel>();
+    }
+
+    public void appendBranchingLevel(LogicTreeBranchingLevel branchingLevel)
+    {
+        this.branchingLevels.add(branchingLevel);
+    }
+
+    public void addEBMapping(String label, Element obj)
+    {
+        this.ebMap.put(label, obj);
+    }
+
+    public List<LogicTreeBranchingLevel> getBranchingLevels()
+    {
+        return this.branchingLevels;
+    }
+
+    public LogicTreeBranchingLevel getBranchingLevelAt(int index)
+    {
+        return this.branchingLevels.get(index);
+    }
+
+    public String getModelName()
+    {
+        return this.name;
+    }
+
+    /**
+     * Returns weight for end-branch model specified by string lab.
+     */
+    public double getWeight(String lab)
+    {
+        String[] strarr = lab.split("_");
+        LogicTreeBranchingLevel brl = this.branchingLevels.get(strarr.length - 1);
+        return brl.getBranch(Integer.valueOf(strarr[strarr.length - 1]).intValue()).getWeight();
+    }
+
+    /**
+     * Returns total weight (product of weights) for end-branch model specified by string lab.
+     */
+    public double getTotWeight(String lab)
+    {
+        double weight = 1.0;
+        String[] strarr = lab.split("_");
+
+        for (int i = 0; i < strarr.length; i++)
+        {
+            LogicTreeBranchingLevel brl = this.branchingLevels.get(i);
+            LogicTreeBranch br = brl.getBranch(Integer.valueOf(strarr[i]).intValue() - 1);
+            weight = weight * br.getWeight();
+        }
+
+        return weight;
+    }
+
+    /**
+     * Returns the end-branch models map.
+     */
+    public Map<String, Element> getEBMap()
+    {
+        return ebMap;
+    }
+
+    /**
+     * Iterator over the end-branch models.
+     */
+    @Override
+    public Iterator<Element> iterator()
+    {
+        return ebMap.values().iterator();
+    }
+
+    @Override
+    public String toString()
+    {
+        StringBuilder asString = new StringBuilder();
+
+        asString.append("Logic tree name: " + this.name + "\n");
+        asString.append("Total number of branching levels in the logic tree: " + this.branchingLevels.size() + "\n");
+
+        for (int i = 0; i < this.branchingLevels.size(); i++)
+        {
+            LogicTreeBranchingLevel branchingLevel = getBranchingLevelAt(i);
+
+            asString.append("> Branching level: " + branchingLevel.getLevel());
+            asString.append(", label: " + branchingLevel.getBranchingLabel());
+            asString.append(", appliesTo: " + branchingLevel.getAppliesTo() + "\n");
+
+            int numBranches = branchingLevel.getBranchList().size();
+
+            // TODO: We should call toString() on the logicTreeBranch
+            for (int j = 0; j < numBranches; j++)
+            {
+                LogicTreeBranch branch = branchingLevel.getBranch(j);
+
+                asString.append(">> Branch number: " + branch.getRelativeID());
+                asString.append(", label: " + branch.getBranchingValue());
+                asString.append(", weight: " + branch.getWeight() + "\n");
+
+                asString.append(">>> Associated file: " + branch.getNameInputFile() + "\n");
+                asString.append(">>> Associated rule: " + branch.getRule().getRuleName() + "\n");
+                asString.append(">>> Associated uncertainty value: " + branch.getRule().getVal() + "\n");
+            }
+        }
+
+        return asString.toString();
+    }
+
+    /**
+     * This method samples a branching level and return the index of a branch.
+     * <p>
+     * The sampling is done using the inverse transform method. (For the algorithm description see
+     * "Computational Statistics Handbook with Matlab", Martinez & Martinez, Champman & all, pag.83).
+     */
+    public int sampleBranchingLevel(int branchingLevelIndex, Random rn)
+    {
+
+        int sample = -Integer.MIN_VALUE;
+
+        // get the corresponding branching level
+        LogicTreeBranchingLevel bl = this.branchingLevels.get(branchingLevelIndex);
+
+        // x values
+        int[] x = new int[bl.getBranchList().size()];
+        // p (probability values)
+
+        double[] p = new double[bl.getBranchList().size()];
+
+        // loop over branches
+        int i = 0;
+
+        for (LogicTreeBranch b : bl.getBranchList())
+        {
+            x[i] = b.getRelativeID();
+            p[i] = b.getWeight();
+
+            logger.debug("label, prob: " + x[i] + " " + p[i]);
+
+            i = i + 1;
+        }
+
+        // compute cumulative distribution
+        double[] cdf = new double[p.length];
+
+        // initialize to zero
+        for (i = 0; i < cdf.length; i++)
+        {
+            cdf[i] = 0.0;
+        }
+
+        for (i = 0; i < cdf.length; i++)
+        {
+            for (int j = 0; j <= i; j++)
+            {
+                cdf[i] = cdf[i] + p[j];
+            }
+        }
+
+        logger.debug("Cumulative distribution:");
+
+        for (i = 0; i < cdf.length; i++)
+        {
+            logger.debug(cdf[i]);
+        }
+
+        // generate uniform random number between 0 and 1
+        double rand = rn.nextDouble();
+
+        logger.debug("Random number: " + rand);
+
+        // loop over probabilities
+        for (int j = 0; j < p.length; j++)
+        {
+            if (rand <= cdf[j])
+            {
+                sample = x[j];
+                break;
+            }
+        }
+
+        return sample;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public boolean equals(Object obj)
+    {
+        if (!(obj instanceof LogicTree))
+        {
+            return false;
+        }
+
+        LogicTree other = (LogicTree) obj;
+        return branchingLevels.equals(other.branchingLevels);
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/logictree/LogicTreeRuleParam.java
@@ -0,0 +1,59 @@
+package org.gem.engine.logictree;
+
+public enum LogicTreeRuleParam {
+
+    /** uncertainties on maximum magnitude */
+    mMaxGRRelative("mMaxGRRelative"),
+
+    /** uncertainties on GR b value */
+    bGRRelative("bGRRelative"),
+
+    /** no uncertainties */
+    NONE("noUncertainties");
+
+    private String name;
+
+    private LogicTreeRuleParam(String name) {
+        this.name = name;
+    }
+
+    /**
+     * This gets the GemLogicTreeRule associated with the given string
+     * 
+     * @param name
+     * @return
+     */
+    public static LogicTreeRuleParam getTypeForName(String name) {
+        if (name == null)
+            throw new NullPointerException();
+        for (LogicTreeRuleParam trt : LogicTreeRuleParam.values()) {
+            if (trt.name.equals(name))
+                return trt;
+        }
+        throw new IllegalArgumentException("Rule parameter name: " + name
+                + " does not exist");
+    }
+
+    /**
+     * This check whether given string is a valid GemLogicTreeRule
+     * 
+     * @param name
+     * @return
+     */
+    public static boolean isValidType(String name) {
+        boolean answer = false;
+        for (LogicTreeRuleParam trt : LogicTreeRuleParam.values()) {
+            if (trt.name.equals(name)) {
+                answer = true;
+                break;
+            }
+        }
+        return answer;
+    }
+
+    @Override
+    public String toString() {
+        return name;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/logictree/LogicTreeBranchingLevel.java
@@ -0,0 +1,129 @@
+package org.gem.engine.logictree;
+
+import java.io.Serializable;
+import java.util.ArrayList;
+
+/**
+ * Class for defining a logic tree branching level. A branching level consists
+ * of a set of {@link LogicTreeBranch} objects (each defining an uncertainty
+ * model). A branching level is characterized by a branching level number
+ * (representing its position in the logic tree), a branching label (a
+ * descriptor), and by an appliesTo value indicating to which previous
+ * end-branch model the branching level applies to (NOTE: this feature has never
+ * been really implemented or used).
+ */
+public class LogicTreeBranchingLevel implements Serializable {
+
+    private ArrayList<LogicTreeBranch> treeBranchList;
+    private int level;
+    private String branchingLabel;
+    private int appliesTo;
+
+    /**
+     * Empty constructor initializing only list of {@link LogicTreeBranch}
+     */
+    public LogicTreeBranchingLevel() {
+
+        this.treeBranchList = new ArrayList<LogicTreeBranch>();
+    }
+
+    /**
+     * Creates new logic tree branching level, given the level number (level),
+     * descriptor (branchingLabel), and branch index the branching level applies
+     * to (appliesTo)
+     */
+    public LogicTreeBranchingLevel(int level, String branchingLabel,
+            int appliesTo) {
+
+        this.treeBranchList = new ArrayList<LogicTreeBranch>();
+        this.level = level;
+        this.branchingLabel = branchingLabel;
+        this.appliesTo = appliesTo;
+    }
+
+    /**
+     * Adds {@link LogicTreeBranch}
+     */
+    public void addBranch(LogicTreeBranch treeBranch) {
+        this.treeBranchList.add(treeBranch);
+    }
+
+    /**
+     * Gets {@link LogicTreeBranch} list
+     */
+    public ArrayList<LogicTreeBranch> getBranchList() {
+        return treeBranchList;
+    }
+
+    /**
+     * Sets {@link LogicTreeBranch} lists
+     */
+    public void setTreeBranchList(ArrayList<LogicTreeBranch> treeBranchList) {
+        this.treeBranchList = treeBranchList;
+    }
+
+    /**
+     * Gets branching level number
+     */
+    public int getLevel() {
+        return level;
+    }
+
+    /**
+     * Sets branching level number
+     */
+    public void setLevel(int level) {
+        this.level = level;
+    }
+
+    /**
+     * Gets branching label
+     */
+    public String getBranchingLabel() {
+        return branchingLabel;
+    }
+
+    /**
+     * Sets branching label
+     */
+    public void setBranchingLabel(String branchingLabel) {
+        this.branchingLabel = branchingLabel;
+    }
+
+    /**
+     * Gets applies to
+     */
+    public int getAppliesTo() {
+        return appliesTo;
+    }
+
+    /**
+     * Sets applies to
+     */
+    public void setAppliesTo(int appliesTo) {
+        // if (appliesTo >= 0){
+        this.appliesTo = appliesTo;
+        // }
+    }
+
+    /**
+     * Gets branch with index idx
+     */
+    public LogicTreeBranch getBranch(int idx) {
+        return this.treeBranchList.get(idx);
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (!(obj instanceof LogicTreeBranchingLevel)) {
+            return false;
+        }
+
+        LogicTreeBranchingLevel other = (LogicTreeBranchingLevel) obj;
+
+        return treeBranchList.equals(other.treeBranchList)
+                && level == other.level
+                && branchingLabel.equals(other.branchingLabel)
+                && appliesTo == other.appliesTo;
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/logictree/LogicTreeBranch.java
@@ -0,0 +1,122 @@
+package org.gem.engine.logictree;
+
+import java.io.Serializable;
+
+/**
+ * Class for logic tree branch definition. A logic tree branch is defined in
+ * terms of a branch number, a branch value (string describing an uncertainty
+ * model), and a weight (representing the likelihood associated to the
+ * uncertainty model). A logic tree branch can additionally contain a file name
+ * (representing an uncertainty model) or a logic tree rule (representing
+ * uncertainty on a particular model parameter)
+ */
+public class LogicTreeBranch implements Serializable {
+
+    private int relativeID;
+    private String branchingValue;
+    private double weight;
+    private String nameInputFile;
+    private LogicTreeRule rule;
+
+    public LogicTreeBranch() {
+
+    }
+
+    /**
+     * Creates a new Logic Tree branch, given the branch number (relativeID), an
+     * uncertainty model (branchingValue), and a weight. The input file field
+     */
+    public LogicTreeBranch(int relativeID, String branchingValue, double weight) {
+        this.relativeID = relativeID;
+        this.branchingValue = branchingValue;
+        this.weight = weight;
+        nameInputFile = "";
+        rule = new LogicTreeRule(LogicTreeRuleParam.NONE, 0.0);
+    }
+
+    /**
+     * Gets branch number
+     */
+    public int getRelativeID() {
+        return relativeID;
+    }
+
+    /**
+     * Sets branch number
+     */
+    public void setRelativeID(int relativeID) {
+        this.relativeID = relativeID;
+    }
+
+    /**
+     * Gets uncertainty model
+     */
+    public String getBranchingValue() {
+        return branchingValue;
+    }
+
+    /**
+     * Sets uncertainty model
+     */
+    public void setBranchingValue(String branchingValue) {
+        this.branchingValue = branchingValue;
+    }
+
+    /**
+     * Gets input file
+     */
+    public String getNameInputFile() {
+        return nameInputFile;
+    }
+
+    /**
+     * Sets input file
+     */
+    public void setNameInputFile(String nameInputFile) {
+        this.nameInputFile = nameInputFile;
+    }
+
+    /**
+     * Gets branch weight
+     */
+    public double getWeight() {
+        return weight;
+    }
+
+    /**
+     * Sets branch weight
+     */
+    public void setWeight(double weight) {
+        this.weight = weight;
+    }
+
+    /**
+     * Gets logic tree rule
+     */
+    public LogicTreeRule getRule() {
+        return rule;
+    }
+
+    /**
+     * Sets logic tree rule
+     */
+    public void setRule(LogicTreeRule rule) {
+        this.rule = rule;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (!(obj instanceof LogicTreeBranch)) {
+            return false;
+        }
+
+        LogicTreeBranch other = (LogicTreeBranch) obj;
+
+        return relativeID == other.relativeID
+                && branchingValue.equals(other.branchingValue)
+                && weight == other.weight
+                && nameInputFile.equals(other.nameInputFile)
+                && rule.equals(other.rule);
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/logictree/LogicTreeRule.java
@@ -0,0 +1,41 @@
+package org.gem.engine.logictree;
+
+import java.io.Serializable;
+
+/**
+ * Classes for defining uncertainty in terms of a rule
+ * {@link LogicTreeRuleParam}
+ */
+public class LogicTreeRule implements Serializable {
+
+    private final LogicTreeRuleParam rule;
+
+    private final double val;
+
+    public LogicTreeRule(LogicTreeRuleParam rule, double val) {
+
+        this.rule = rule;
+        this.val = val;
+
+    }
+
+    public LogicTreeRuleParam getRuleName() {
+        return rule;
+    }
+
+    public double getVal() {
+        return val;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (!(obj instanceof LogicTreeRule)) {
+            return false;
+        }
+
+        LogicTreeRule other = (LogicTreeRule) obj;
+
+        return val == other.val && rule.equals(other.rule);
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/hazard/redis/Cache.java
@@ -0,0 +1,106 @@
+package org.gem.engine.hazard.redis;
+
+import java.net.InetSocketAddress;
+
+import org.jredis.ClientRuntimeException;
+import org.jredis.RedisException;
+import org.jredis.connector.ConnectionSpec;
+import org.jredis.ri.alphazero.JRedisClient;
+import org.jredis.ri.alphazero.connection.DefaultConnectionSpec;
+
+/**
+ * Store stuff in Redis.
+ * 
+ * This wraps the jredis library.
+ * 
+ * @author Christopher MacGown
+ */
+public class Cache {
+    private JRedisClient client;
+
+    /**
+     * Default client constructor, defaults to database 0.
+     */
+    public Cache(String host, int port) {
+        try {
+            // Do the connection.
+            client = new JRedisClient(getConnectionSpec(host, port, 0));
+        } catch (ClientRuntimeException e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /**
+     * Constructor for specifying database.
+     */
+    public Cache(String host, int port, int db) {
+        try {
+            // Do the connection.
+            client = new JRedisClient(getConnectionSpec(host, port, db));
+        } catch (ClientRuntimeException e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /**
+     * Get the connection spec for the client connection
+     */
+    private ConnectionSpec getConnectionSpec(String host, int port, int db) {
+        ConnectionSpec connectionSpec = DefaultConnectionSpec.newSpec();
+        InetSocketAddress addr = new InetSocketAddress(host, port);
+
+        // Build the connection specification
+        connectionSpec.setAddress(addr.getAddress()).setPort(addr.getPort())
+                .setReconnectCnt(2) // # times to reconnect if we disconnected.
+                .setDatabase(db);
+
+        return connectionSpec;
+    }
+
+    /**
+     * Throw a fit and break everything.
+     */
+    public boolean set(String key, Object value) {
+        throw new RuntimeException("Lars frowns upon your shenanigans");
+    }
+
+    /**
+     * Given a key and a string, write that string to Redis.
+     * <p>
+     * 
+     * @param key
+     *            The key to use.
+     * @param value
+     *            The value to be written.
+     */
+    public void set(String key, String value) {
+        try {
+            client.set(key, value);
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /**
+     * Given a key, return the value from the client.
+     * <p>
+     * 
+     * @param key
+     *            The key to use.
+     */
+    public Object get(String key) {
+        try {
+            return new String(client.get(key));
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public void flush() {
+        try {
+            client.flushdb();
+        } catch (RedisException e) {
+            throw new RuntimeException(e);
+        }
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/hazard/parsers/SourceModelReader.java
@@ -0,0 +1,530 @@
+package org.gem.engine.hazard.parsers;
+
+import java.io.File;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.StringTokenizer;
+
+import org.dom4j.Document;
+import org.dom4j.DocumentException;
+import org.dom4j.Element;
+import org.dom4j.io.SAXReader;
+import org.opensha.commons.data.function.ArbitrarilyDiscretizedFunc;
+import org.opensha.commons.geo.BorderType;
+import org.opensha.commons.geo.Location;
+import org.opensha.commons.geo.LocationList;
+import org.opensha.commons.geo.Region;
+import org.opensha.sha.earthquake.FocalMechanism;
+import org.opensha.sha.earthquake.griddedForecast.HypoMagFreqDistAtLoc;
+import org.opensha.sha.earthquake.griddedForecast.MagFreqDistsForFocalMechs;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMAreaSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMFaultSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMPointSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMSourceData;
+import org.opensha.sha.earthquake.rupForecastImpl.GEM1.SourceData.GEMSubductionFaultSourceData;
+import org.opensha.sha.faultSurface.FaultTrace;
+import org.opensha.sha.magdist.GutenbergRichterMagFreqDist;
+import org.opensha.sha.magdist.IncrementalMagFreqDist;
+import org.opensha.sha.util.TectonicRegionType;
+import org.gem.engine.XMLMismatchError;
+import org.gem.engine.XMLValidationError;
+
+/**
+ * Class for reading source model data in a nrML format file. The constructor of
+ * this class takes the path of the file to read from.
+ *
+ */
+public class SourceModelReader {
+
+    private final List<GEMSourceData> sourceList;
+
+    private final String path;
+    private final double deltaMFD;
+
+    // border type for area source definition
+    private static BorderType borderType = BorderType.GREAT_CIRCLE;
+
+    private static final String SIMPLE_FAULT = "simpleFaultSource";
+    private static final String COMPLEX_FAULT = "complexFaultSource";
+    private static final String AREA = "areaSource";
+    private static final String POINT = "pointSource";
+
+    private static final String SOURCE_NAME = "name";
+    private static final String SOURCE_ID = "id";
+    private static final String TECTONIC_REGION = "tectonicRegion";
+
+    private static final String SIMPLE_FAULT_GEOMETRY = "simpleFaultGeometry";
+    private static final String FAULT_TRACE = "faultTrace";
+    private static final String UPPER_SEIMSMOGENIC_DEPTH =
+            "upperSeismogenicDepth";
+    private static final String LOWER_SEISMOGENIC_DEPTH =
+            "lowerSeismogenicDepth";
+
+    private static final String COMPLEX_FAULT_GEOMETRY = "complexFaultGeometry";
+    private static final String FAULT_TOP_EDGE = "faultTopEdge";
+    private static final String FAULT_BOTTOM_EDGE = "faultBottomEdge";
+
+    private static final String AREA_BOUNDARY = "areaBoundary";
+    private static final String EXTERIOR = "exterior";
+    private static final String LINEAR_RING = "LinearRing";
+    private static final String POS_LIST = "posList";
+
+    private static final String POINT_LOCATION = "Point";
+    private static final String POS = "pos";
+
+    private static final String RUPTURE_RATE_MODEL = "ruptureRateModel";
+
+    private static final String HYPOCENTRAL_DEPTH = "hypocentralDepth";
+
+    private static final String FOCAL_MECHANISM = "focalMechanism";
+    private static final String STRIKE = "strike";
+    private static final String DIP = "dip";
+    private static final String RAKE = "rake";
+
+    private static final String RUPTURE_DEPTH_DISTRIBUTION =
+            "ruptureDepthDistribution";
+
+    private static final String EVENLY_DISCRETIZED_MAG_FREQ_DIST =
+            "evenlyDiscretizedIncrementalMFD";
+    private static final String BIN_SIZE = "binSize";
+    private static final String MIN_VAL = "minVal";
+
+    private static final String TRUNCATED_GUTENBERG_RICHTER =
+            "truncatedGutenbergRichter";
+    private static final String a_VALUE_CUMULATIVE = "aValueCumulative";
+    private static final String b_VALUE = "bValue";
+    private static final String MIN_MAGNITUDE = "minMagnitude";
+    private static final String MAX_MAGNITUDE = "maxMagnitude";
+
+    private static final String NODAL_PLANE1 = "nodalPlane1";
+    private static final String NODAL_PLANES = "nodalPlanes";
+    private static final String VALUE = "value";
+    private static final String DEPTH = "depth";
+    private static final String MAGNITUDE = "magnitude";
+    private static final String LINE_STRING = "LineString";
+    private static final String FAULT_EDGES = "faultEdges";
+    private static final String POLYGON = "Polygon";
+    private static final String LOCATION = "location";
+    private static final String SOURCE_MODEL = "sourceModel";
+
+    /**
+     * Creates a new SourceModelReader given the path of the file to read from
+     * and the bin width for the magnitude frequency distribution definition
+     */
+    public SourceModelReader(String path, double deltaMFD) {
+        this.path = path;
+        this.sourceList = new ArrayList<GEMSourceData>();
+        this.deltaMFD = deltaMFD;
+    }
+
+    /**
+     * Reads file and returns source model data. For each source definition, a
+     * {@link GEMSourceData} is created and stored in a list.
+     */
+    @SuppressWarnings("unchecked")
+    public List<GEMSourceData> read() {
+        if (System.getProperty("openquake.nrml.schema") == null)
+            throw new RuntimeException("Set openquake.nrml.schema property  to the NRML schema path");
+
+        this.sourceList.clear();
+
+        File xml = new File(path);
+        SAXReader reader = new SAXReader(true);
+        Document doc = null;
+        try {
+            reader.setFeature("http://apache.org/xml/features/validation/schema", true);
+            reader.setProperty("http://java.sun.com/xml/jaxp/properties/schemaLanguage", "http://www.w3.org/2001/XMLSchema");
+            reader.setProperty("http://java.sun.com/xml/jaxp/properties/schemaSource", "file://" + System.getProperty("openquake.nrml.schema"));
+            doc = reader.read(xml);
+        } catch (DocumentException e) {
+            throw new XMLValidationError(xml.getAbsolutePath(), e);
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+
+        Element sourceModel = doc.getRootElement().element(SOURCE_MODEL);
+        if (sourceModel == null)
+        {
+            Element child = (Element) doc.getRootElement().elements().get(0);
+            String localName = child.getQName().getName();
+
+            throw new XMLMismatchError(null, localName, SOURCE_MODEL);
+        }
+
+        Iterator i = sourceModel.elements().iterator();
+
+        while (i.hasNext()) {
+            Element elem = (Element) i.next();
+            String elemName = elem.getName();
+            if (elemName.equalsIgnoreCase(SIMPLE_FAULT)) {
+                sourceList.add(getSimpleFaultSourceData(deltaMFD, elem));
+            } else if (elemName.equalsIgnoreCase(COMPLEX_FAULT)) {
+                sourceList.add(getComplexFaultSourceData(deltaMFD, elem));
+            } else if (elemName.equalsIgnoreCase(AREA)) {
+                sourceList.add(getAreaSourceData(deltaMFD, elem));
+            } else if (elemName.equalsIgnoreCase(POINT)) {
+                sourceList.add(getPointSourceData(deltaMFD, elem));
+            }
+        }
+        return sourceList;
+    }
+
+    /**
+     * Reads and return point source data
+     */
+    private GEMPointSourceData getPointSourceData(double deltaMFD, Element elem) {
+        Location pointLoc =
+                getPointLocation(elem.element(LOCATION).element(POINT_LOCATION));
+        MagFreqDistsForFocalMechs magfreqDistFocMech =
+                getMagFreqDistsForFocalMechs(deltaMFD, elem);
+        HypoMagFreqDistAtLoc hypoMagFreqDistAtLoc =
+                new HypoMagFreqDistAtLoc(magfreqDistFocMech
+                        .getMagFreqDistList(), pointLoc, magfreqDistFocMech
+                        .getFocalMechanismList());
+        ArbitrarilyDiscretizedFunc rupDepthDist =
+                getRuptureDepthDistribution(elem
+                        .element(RUPTURE_DEPTH_DISTRIBUTION));
+        double hypocentralDepth =
+                Double.valueOf((String) elem.element(HYPOCENTRAL_DEPTH)
+                        .getData());
+
+        return new GEMPointSourceData(extractIDFrom(elem),
+                extractNameFrom(elem), extractTectonicRegionFrom(elem),
+                hypoMagFreqDistAtLoc, rupDepthDist, hypocentralDepth);
+    }
+
+    /**
+     * Returns the type of the tectonic region of the given element.
+     */
+    private TectonicRegionType extractTectonicRegionFrom(Element elem) {
+        return TectonicRegionType.getTypeForName((String) elem.element(
+                TECTONIC_REGION).getData());
+    }
+
+    /**
+     * Returns the ID of the given element.
+     */
+    private String extractIDFrom(Element elem) {
+        return elem.attributeValue(SOURCE_ID);
+    }
+
+    /**
+     * Returns the name of the given element.
+     */
+    private String extractNameFrom(Element elem) {
+        return (String) elem.element(SOURCE_NAME).getData();
+    }
+
+    /**
+     * Reads and returns area source data
+     */
+    private GEMAreaSourceData getAreaSourceData(double deltaMFD, Element elem) {
+        Region reg =
+                new Region(get2DLocList(elem.element(AREA_BOUNDARY).element(
+                        POLYGON).element(EXTERIOR).element(LINEAR_RING)
+                        .element(POS_LIST)), borderType);
+        MagFreqDistsForFocalMechs magfreqDistFocMech =
+                getMagFreqDistsForFocalMechs(deltaMFD, elem);
+        ArbitrarilyDiscretizedFunc rupDepthDist =
+                getRuptureDepthDistribution(elem
+                        .element(RUPTURE_DEPTH_DISTRIBUTION));
+        double hypocentralDepth =
+                Double.valueOf((String) elem.element(HYPOCENTRAL_DEPTH)
+                        .getData());
+
+        return new GEMAreaSourceData(extractIDFrom(elem),
+                extractNameFrom(elem), extractTectonicRegionFrom(elem), reg,
+                magfreqDistFocMech, rupDepthDist, hypocentralDepth);
+    }
+
+    /**
+     * Reads and returns complex fault source data
+     */
+    private GEMSubductionFaultSourceData getComplexFaultSourceData(
+            double deltaMFD, Element elem) {
+        Element complexFaultGeometry = elem.element(COMPLEX_FAULT_GEOMETRY);
+
+        FaultTrace faultTopEdge =
+                getFaultTrace(complexFaultGeometry.element(FAULT_EDGES)
+                        .element(FAULT_TOP_EDGE).element(LINE_STRING).element(
+                                POS_LIST));
+        FaultTrace faultBottomEdge =
+                getFaultTrace(complexFaultGeometry.element(FAULT_EDGES)
+                        .element(FAULT_BOTTOM_EDGE).element(LINE_STRING)
+                        .element(POS_LIST));
+        double rake = Double.valueOf((String) elem.element(RAKE).getData());
+        IncrementalMagFreqDist magFreqDist = getMagFreqDist(deltaMFD, elem);
+
+        return new GEMSubductionFaultSourceData(extractIDFrom(elem),
+                extractNameFrom(elem), extractTectonicRegionFrom(elem),
+                faultTopEdge, faultBottomEdge, rake, magFreqDist, true);
+    }
+
+    /**
+     * Reads and return simple fault source data
+     */
+    private GEMFaultSourceData getSimpleFaultSourceData(double deltaMFD,
+            Element elem) {
+        Element simpleFaultGeometry = elem.element(SIMPLE_FAULT_GEOMETRY);
+        FaultTrace faultTrace =
+                getFaultTrace(simpleFaultGeometry.element(FAULT_TRACE).element(
+                        LINE_STRING).element(POS_LIST));
+        double dip =
+                Double.valueOf((String) simpleFaultGeometry.element(DIP)
+                        .getData());
+        double upperSeismogenicDepth =
+                Double.valueOf((String) simpleFaultGeometry.element(
+                        UPPER_SEIMSMOGENIC_DEPTH).getData());
+        double lowerSeismogenicDepth =
+                Double.valueOf((String) simpleFaultGeometry.element(
+                        LOWER_SEISMOGENIC_DEPTH).getData());
+        double rake = Double.valueOf((String) elem.element(RAKE).getData());
+
+        IncrementalMagFreqDist magFreqDist = getMagFreqDist(deltaMFD, elem);
+
+        return new GEMFaultSourceData(extractIDFrom(elem),
+                extractNameFrom(elem), extractTectonicRegionFrom(elem),
+                magFreqDist, faultTrace, dip, rake, lowerSeismogenicDepth,
+                upperSeismogenicDepth, true);
+    }
+
+    /**
+     * Reads fault trace coordinates (Each point being specified by a triplet
+     * (longitude, latitude, depth)) and returns {@link FaultTrace}
+     */
+    private FaultTrace getFaultTrace(Element trace) {
+        FaultTrace faultTrace = new FaultTrace("");
+        LocationList locList = get3DLocList(trace);
+        faultTrace.addAll(locList);
+        return faultTrace;
+    }
+
+    /**
+     * Reads magnitude frequency distribution/focal mechanism pairs and returns
+     * {@link MagFreqDistsForFocalMechs}
+     */
+    @SuppressWarnings("unchecked")
+    private MagFreqDistsForFocalMechs getMagFreqDistsForFocalMechs(
+            double deltaMFD, Element elem) {
+        List<IncrementalMagFreqDist> mfdList =
+                new ArrayList<IncrementalMagFreqDist>();
+        List<FocalMechanism> focMechList = new ArrayList<FocalMechanism>();
+        for (Iterator j = elem.elementIterator(RUPTURE_RATE_MODEL); j.hasNext();) {
+            Element e = (Element) j.next();
+            IncrementalMagFreqDist magFreqDist = getMagFreqDist(deltaMFD, e);
+            mfdList.add(magFreqDist);
+            FocalMechanism focMech =
+                    getFocalMechanism(e.element(FOCAL_MECHANISM));
+            focMechList.add(focMech);
+        }
+        IncrementalMagFreqDist[] mfdArray =
+                new IncrementalMagFreqDist[mfdList.size()];
+        FocalMechanism[] fmArray = new FocalMechanism[focMechList.size()];
+        for (int ii = 0; ii < mfdList.size(); ii++) {
+            mfdArray[ii] = mfdList.get(ii);
+            fmArray[ii] = focMechList.get(ii);
+        }
+        return new MagFreqDistsForFocalMechs(mfdArray, fmArray);
+    }
+
+    /**
+     * Reads magnitude frequency distribution data (truncated Gutenberg-Richter
+     * or incremental evenly discretized) and returns
+     * {@link IncrementalMagFreqDist}
+     */
+    private IncrementalMagFreqDist getMagFreqDist(double deltaMFD, Element elem) {
+        IncrementalMagFreqDist magFreqDist = null;
+        if (elem.element(TRUNCATED_GUTENBERG_RICHTER) != null) {
+            magFreqDist =
+                    getGutenbergRichterMagFreqDist(deltaMFD, elem
+                            .element(TRUNCATED_GUTENBERG_RICHTER));
+        } else if (elem.element(EVENLY_DISCRETIZED_MAG_FREQ_DIST) != null) {
+            magFreqDist =
+                    getEvenlyDiscretizedMagFreqDist(elem
+                            .element(EVENLY_DISCRETIZED_MAG_FREQ_DIST));
+        }
+        return magFreqDist;
+    }
+
+    /**
+     * Reads incremental evenly discretized magnitude frequency distribution
+     * data and returns {@link IncrementalMagFreqDist}
+     */
+    private IncrementalMagFreqDist getEvenlyDiscretizedMagFreqDist(
+            Element evenlyDiscretizedMagFreqDist) {
+        IncrementalMagFreqDist magFreqDist;
+        double binSize =
+                Double.valueOf(evenlyDiscretizedMagFreqDist
+                        .attributeValue(BIN_SIZE));
+        double minVal =
+                Double.valueOf(evenlyDiscretizedMagFreqDist
+                        .attributeValue(MIN_VAL));
+
+        StringTokenizer st =
+                new StringTokenizer((String) evenlyDiscretizedMagFreqDist
+                        .getData());
+
+        magFreqDist =
+                new IncrementalMagFreqDist(minVal, st.countTokens(), binSize);
+
+        int index = 0;
+
+        while (st.hasMoreTokens()) {
+            magFreqDist.add(index, Double.valueOf(st.nextToken()));
+            index++;
+        }
+
+        return magFreqDist;
+    }
+
+    /**
+     * Reads Gutenberg-Richter magnitude frequency distribution data and returns
+     * {@link GutenbergRichterMagFreqDist}
+     */
+    private GutenbergRichterMagFreqDist getGutenbergRichterMagFreqDist(
+            double deltaMFD, Element gutenbergRichter) {
+        GutenbergRichterMagFreqDist magFreqDist;
+        double aVal =
+                Double.valueOf((String) gutenbergRichter.element(
+                        a_VALUE_CUMULATIVE).getData());
+        double bVal =
+                Double.valueOf((String) gutenbergRichter.element(b_VALUE)
+                        .getData());
+        double minMag =
+                Double.valueOf((String) gutenbergRichter.element(MIN_MAGNITUDE)
+                        .getData());
+        double maxMag =
+                Double.valueOf((String) gutenbergRichter.element(MAX_MAGNITUDE)
+                        .getData());
+        magFreqDist = createGrMfd(aVal, bVal, minMag, maxMag, deltaMFD);
+        return magFreqDist;
+    }
+
+    /**
+     * Reads focal mechanism data (strike, dip and rake) and returns
+     * {@link FocalMechanism}
+     */
+    private FocalMechanism getFocalMechanism(Element focalMech) {
+        Element nodalPlane =
+                focalMech.element(NODAL_PLANES).element(NODAL_PLANE1);
+
+        double strike =
+                Double.valueOf((String) nodalPlane.element(STRIKE).element(
+                        VALUE).getData());
+        double dip =
+                Double.valueOf((String) nodalPlane.element(DIP).element(VALUE)
+                        .getData());
+        double rake =
+                Double.valueOf((String) nodalPlane.element(RAKE).element(VALUE)
+                        .getData());
+        return new FocalMechanism(strike, dip, rake);
+    }
+
+    /**
+     * Reads rupture depth distribution data (rupture magnitude vs. rupture
+     * depth) and returns an {@link ArbitrarilyDiscretizedFunc}
+     */
+    private ArbitrarilyDiscretizedFunc getRuptureDepthDistribution(
+            Element ruptureDepthDist) {
+        ArbitrarilyDiscretizedFunc rupDepthDist =
+                new ArbitrarilyDiscretizedFunc();
+        String xVals = (String) ruptureDepthDist.element(MAGNITUDE).getData();
+        String yVals = (String) ruptureDepthDist.element(DEPTH).getData();
+        StringTokenizer xVal = new StringTokenizer(xVals);
+        StringTokenizer yVal = new StringTokenizer(yVals);
+        while (xVal.hasMoreTokens())
+            rupDepthDist.set(Double.valueOf(xVal.nextToken()), Double
+                    .valueOf(yVal.nextToken()));
+        return rupDepthDist;
+    }
+
+    /**
+     * Reads location list (each location being specified by a triplet
+     * (longitude,latitude,depth)) and returns {@link LocationList}
+     */
+    private LocationList get3DLocList(Element posList) {
+        LocationList locList = new LocationList();
+        String positionList = ((String) posList.getData());
+        StringTokenizer st = new StringTokenizer(positionList);
+        while (st.hasMoreTokens()) {
+            double lon = Double.valueOf(st.nextToken());
+            double lat = Double.valueOf(st.nextToken());
+            double depth = Double.valueOf(st.nextToken());
+            locList.add(new Location(lat, lon, depth));
+        }
+        return locList;
+    }
+
+    /**
+     * Reads location list (each location being specified by a doublet
+     * (longitude,latitude)) and returns a {@link LocationList}
+     */
+    private LocationList get2DLocList(Element posList) {
+        LocationList locList = new LocationList();
+        String positionList = (String) posList.getData();
+        StringTokenizer st = new StringTokenizer(positionList);
+        while (st.hasMoreTokens()) {
+            double lon = Double.valueOf(st.nextToken());
+            double lat = Double.valueOf(st.nextToken());
+            locList.add(new Location(lat, lon));
+        }
+        return locList;
+    }
+
+    /**
+     * Reads single location (longitude,latitude). Returns {@link Location}
+     */
+    private Location getPointLocation(Element pointLocation) {
+        String pointCoords = (String) pointLocation.element(POS).getData();
+        StringTokenizer st = new StringTokenizer(pointCoords);
+        double longitude = Double.valueOf(st.nextToken());
+        double latitude = Double.valueOf(st.nextToken());
+        return new Location(latitude, longitude);
+    }
+
+    /**
+     * Defines truncated Gutenberg-Richter magnitude frequency distribution
+     *
+     * @param aVal
+     *            : cumulative a value
+     * @param bVal
+     *            : b value
+     * @param mMin
+     *            : minimum magnitude
+     * @param mMax
+     *            : maximum magnitude
+     * @param deltaMFD
+     *            : discretization interval
+     * @return {@link GutenbergRichterMagFreqDist}
+     */
+    private GutenbergRichterMagFreqDist createGrMfd(double aVal, double bVal,
+            double mMin, double mMax, double deltaMFD) {
+        GutenbergRichterMagFreqDist mfd = null;
+        // round mMin and mMax with respect to delta bin
+        mMin = Math.round(mMin / deltaMFD) * deltaMFD;
+        mMax = Math.round(mMax / deltaMFD) * deltaMFD;
+        // compute total cumulative rate between minimum and maximum magnitude
+        double totCumRate = Double.NaN;
+        if (mMin != mMax) {
+            totCumRate =
+                    Math.pow(10, aVal - bVal * mMin)
+                            - Math.pow(10, aVal - bVal * mMax);
+        } else {
+            // compute incremental a value and calculate rate corresponding to
+            // minimum magnitude
+            double aIncr = aVal + Math.log10(bVal * Math.log(10));
+            totCumRate = Math.pow(10, aIncr - bVal * mMin);
+        }
+        if (mMax != mMin) {
+            // shift to bin center
+            mMin = mMin + deltaMFD / 2;
+            mMax = mMax - deltaMFD / 2;
+        }
+        int numVal = (int) Math.round(((mMax - mMin) / deltaMFD + 1));
+        mfd =
+                new GutenbergRichterMagFreqDist(bVal, totCumRate, mMin, mMax,
+                        numVal);
+        return mfd;
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/engine/hazard/parsers/RuptureReader.java
@@ -0,0 +1,265 @@
+package org.gem.engine.hazard.parsers;
+
+import java.io.File;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.StringTokenizer;
+
+import org.dom4j.Document;
+import org.dom4j.XPath;
+import org.dom4j.io.SAXReader;
+import org.dom4j.xpath.DefaultXPath;
+import org.opensha.commons.geo.Location;
+import org.opensha.sha.earthquake.EqkRupture;
+import org.opensha.sha.faultSurface.ApproxEvenlyGriddedSurface;
+import org.opensha.sha.faultSurface.FaultTrace;
+import org.opensha.sha.faultSurface.PointSurface;
+import org.opensha.sha.faultSurface.StirlingGriddedSurface;
+import org.opensha.sha.util.TectonicRegionType;
+
+/**
+ * Reads NRML definitions of rupture elements.
+ */
+public class RuptureReader
+{
+
+    private final File file;
+    private Document document;
+    private final double gridSpacing;
+
+    private static final Map<String, String> namespaces = new HashMap<String, String>();
+
+    {
+        namespaces.put("gml", "http://www.opengis.net/gml");
+        namespaces.put("qml", "http://quakeml.org/xmlns/quakeml/1.1");
+        namespaces.put("nrml", "http://openquake.org/xmlns/nrml/0.2");
+    }
+
+    class InvalidFormatException extends RuntimeException
+    {
+
+        private static final long serialVersionUID = 4430401704068185529L;
+
+        public InvalidFormatException(String message)
+        {
+            super(message);
+        }
+
+    }
+
+    /**
+     * Parses the document and updates the rupture object.
+     */
+    interface RuptureParser
+    {
+
+        /**
+         * Updates the given rupture with the data found in the document.
+         */
+        void update(EqkRupture rupture);
+
+    }
+
+    class PointRuptureParser implements RuptureParser
+    {
+
+        private final Document document;
+
+        PointRuptureParser(Document document)
+        {
+            this.document = document;
+        }
+
+        @Override
+        public void update(EqkRupture rupture)
+        {
+            rupture.setHypocenterLocation(location());
+            rupture.setAveRake(asDouble("//qml:rake/qml:value"));
+
+            PointSurface surface = new PointSurface(location());
+            surface.setAveStrike(asDouble("//qml:strike/qml:value"));
+            surface.setAveDip(asDouble("//qml:dip/qml:value"));
+
+            rupture.setRuptureSurface(surface);
+        }
+
+        private Location location()
+        {
+            String pos = xpath("//gml:pos").selectSingleNode(document).getText();
+            StringTokenizer splitter = new StringTokenizer(pos);
+            return nextLocation(splitter);
+        }
+
+    }
+
+    class SimpleFaultRuptureParser implements RuptureParser
+    {
+
+        private final Document document;
+
+        SimpleFaultRuptureParser(Document document)
+        {
+            this.document = document;
+        }
+
+        @Override
+        public void update(EqkRupture rupture)
+        {
+            rupture.setAveRake(asDouble("//nrml:rake"));
+
+            double dip = asDouble("//nrml:dip");
+            double usd = asDouble("//nrml:upperSeismogenicDepth");
+            double lsd = asDouble("//nrml:lowerSeismogenicDepth");
+
+            StirlingGriddedSurface surface = new StirlingGriddedSurface(trace(), dip, usd, lsd, gridSpacing);
+            rupture.setRuptureSurface(surface);
+        }
+
+        private FaultTrace trace()
+        {
+            FaultTrace trace = new FaultTrace("");
+            String values = xpath("//gml:posList").selectSingleNode(document).getText();
+            StringTokenizer splitter = new StringTokenizer(values);
+
+            while (splitter.hasMoreTokens())
+            {
+                trace.add(nextLocation(splitter));
+            }
+
+            return trace;
+        }
+
+    }
+
+    class ComplexFaultRuptureParser implements RuptureParser
+    {
+
+        private final Document document;
+
+        ComplexFaultRuptureParser(Document document)
+        {
+            this.document = document;
+        }
+
+        @Override
+        public void update(EqkRupture rupture)
+        {
+            rupture.setAveRake(asDouble("//nrml:rake"));
+
+            String topFaultXPath = "//nrml:faultTopEdge/gml:LineString/gml:posList";
+            String bottomFaultXPath = "//nrml:faultBottomEdge/gml:LineString/gml:posList";
+
+            FaultTrace topFault = trace(topFaultXPath);
+            FaultTrace bottomFault = trace(bottomFaultXPath);
+            ApproxEvenlyGriddedSurface surface = new ApproxEvenlyGriddedSurface(topFault, bottomFault, gridSpacing);
+
+            rupture.setRuptureSurface(surface);
+        }
+
+        private FaultTrace trace(String xpath)
+        {
+            FaultTrace trace = new FaultTrace(null);
+            String values = xpath(xpath).selectSingleNode(document).getText();
+            StringTokenizer splitter = new StringTokenizer(values);
+
+            while (splitter.hasMoreTokens())
+            {
+                trace.add(nextLocation(splitter));
+            }
+
+            return trace;
+        }
+
+    }
+
+    public RuptureReader(File file, double gridSpacing)
+    {
+        if (!file.isFile())
+        {
+            throw new IllegalArgumentException("unknown file!");
+        }
+
+        this.file = file;
+        this.gridSpacing = gridSpacing;
+    }
+
+    public RuptureReader(String path, double gridSpacing)
+    {
+        this(new File(path), gridSpacing);
+    }
+
+    /**
+     * Reads the document and returns the rupture object.
+     *
+     * @return the rupture defined in the document
+     */
+    public EqkRupture read()
+    {
+        SAXReader reader = new SAXReader();
+
+        try
+        {
+            document = reader.read(this.file);
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+
+        EqkRupture rupture = new EqkRupture();
+
+        rupture.setMag(asDouble("//nrml:magnitude"));
+        rupture.setTectRegType(tectonicRegionType());
+
+        if (xpath("//nrml:pointRupture").selectSingleNode(document) != null)
+        {
+            new PointRuptureParser(document).update(rupture);
+        }
+        else if (xpath("//nrml:simpleFaultRupture").selectSingleNode(document) != null)
+        {
+            new SimpleFaultRuptureParser(document).update(rupture);
+        }
+        else if (xpath("//nrml:complexFaultRupture").selectSingleNode(document) != null)
+        {
+            new ComplexFaultRuptureParser(document).update(rupture);
+        }
+        else
+            throw new RuntimeException("'" + file + "' isn't a known rupture type");
+
+        return rupture;
+    }
+
+    private XPath xpath(String pattern)
+    {
+        XPath xpath = new DefaultXPath(pattern);
+        xpath.setNamespaceURIs(namespaces);
+
+        return xpath;
+    }
+
+    private TectonicRegionType tectonicRegionType()
+    {
+        String name = xpath("//nrml:tectonicRegion").selectSingleNode(document).getText();
+        return TectonicRegionType.getTypeForName(name);
+    }
+
+    private double asDouble(String pattern)
+    {
+        return Double.parseDouble(xpath(pattern).selectSingleNode(document).getText());
+    }
+
+    private Location nextLocation(StringTokenizer splitter)
+    {
+        if (splitter.countTokens() % 3 != 0)
+        {
+            throw new InvalidFormatException("longitude, latitude and depth must be always specified!");
+        }
+
+        double lon = Double.parseDouble(splitter.nextToken());
+        double lat = Double.parseDouble(splitter.nextToken());
+        double depth = Double.parseDouble(splitter.nextToken());
+
+        return new Location(lat, lon, depth);
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/params/IMLListParams.java
@@ -0,0 +1,51 @@
+package org.gem.params;
+
+public enum IMLListParams {
+
+    /** Intensity measure level list */
+    IML_LIST("Intensity measure level list");
+
+    private String name;
+
+    private IMLListParams(String name) {
+        this.name = name;
+    }
+
+    /**
+     * This gets the GemSourceType associated with the given string
+     * 
+     * @param name
+     * @return
+     */
+    public static IMLListParams getTypeForName(String name) {
+        if (name == null)
+            throw new NullPointerException();
+        for (IMLListParams trt : IMLListParams.values()) {
+            if (trt.name.equals(name))
+                return trt;
+        }
+        throw new IllegalArgumentException(
+                "Area source parameter name does not exist");
+    }
+
+    /**
+     * This check whether given string is a valid Gem source type
+     * 
+     * @param name
+     * @return
+     */
+    public static boolean isValidType(String name) {
+        boolean answer = false;
+        for (IMLListParams trt : IMLListParams.values()) {
+            if (trt.name.equals(name))
+                answer = true;
+        }
+        return answer;
+    }
+
+    @Override
+    public String toString() {
+        return name;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/params/CpuParams.java
@@ -0,0 +1,50 @@
+package org.gem.params;
+
+public enum CpuParams {
+
+    /** number of cpus used for calculation */
+    CPU_NUMBER("Number of cpus used for calculation");
+
+    private String name;
+
+    private CpuParams(String name) {
+        this.name = name;
+    }
+
+    /**
+     * This gets the GemSourceType associated with the given string
+     * 
+     * @param name
+     * @return
+     */
+    public static CpuParams getTypeForName(String name) {
+        if (name == null)
+            throw new NullPointerException();
+        for (CpuParams trt : CpuParams.values()) {
+            if (trt.name.equals(name))
+                return trt;
+        }
+        throw new IllegalArgumentException("Cpu parameter name does not exist");
+    }
+
+    /**
+     * This check whether given string is a valid Gem source type
+     * 
+     * @param name
+     * @return
+     */
+    public static boolean isValidType(String name) {
+        boolean answer = false;
+        for (CpuParams trt : CpuParams.values()) {
+            if (trt.name.equals(name))
+                answer = true;
+        }
+        return answer;
+    }
+
+    @Override
+    public String toString() {
+        return name;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/params/DistanceParams.java
@@ -0,0 +1,51 @@
+package org.gem.params;
+
+public enum DistanceParams {
+
+    /** maximum distance to source */
+    MAX_DIST_SOURCE("Maximum distance to source");
+
+    private String name;
+
+    private DistanceParams(String name) {
+        this.name = name;
+    }
+
+    /**
+     * This gets the GemSourceType associated with the given string
+     * 
+     * @param name
+     * @return
+     */
+    public static DistanceParams getTypeForName(String name) {
+        if (name == null)
+            throw new NullPointerException();
+        for (DistanceParams trt : DistanceParams.values()) {
+            if (trt.name.equals(name))
+                return trt;
+        }
+        throw new IllegalArgumentException(
+                "Area source parameter name does not exist");
+    }
+
+    /**
+     * This check whether given string is a valid Gem source type
+     * 
+     * @param name
+     * @return
+     */
+    public static boolean isValidType(String name) {
+        boolean answer = false;
+        for (DistanceParams trt : DistanceParams.values()) {
+            if (trt.name.equals(name))
+                answer = true;
+        }
+        return answer;
+    }
+
+    @Override
+    public String toString() {
+        return name;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/params/SourceType.java
@@ -0,0 +1,59 @@
+package org.gem.params;
+
+public enum SourceType {
+
+    /** Area source. */
+    AREA_SOURCE("Area Source"),
+
+    /** Grid source. */
+    GRID_SOURCE("Grid Source"),
+
+    /** Fault source. */
+    FAULT_SOURCE("Fault Source"),
+
+    /** Subduction fault source. */
+    SUBDUCTION_FAULT_SOURCE("Subduction Fault Source");
+
+    private String name;
+
+    private SourceType(String name) {
+        this.name = name;
+    }
+
+    /**
+     * This gets the GemSourceType associated with the given string
+     * 
+     * @param name
+     * @return
+     */
+    public static SourceType getTypeForName(String name) {
+        if (name == null)
+            throw new NullPointerException();
+        for (SourceType trt : SourceType.values()) {
+            if (trt.name.equals(name))
+                return trt;
+        }
+        throw new IllegalArgumentException("GEM source name does not exist");
+    }
+
+    /**
+     * This check whether given string is a valid Gem source type
+     * 
+     * @param name
+     * @return
+     */
+    public static boolean isValidType(String name) {
+        boolean answer = false;
+        for (SourceType trt : SourceType.values()) {
+            if (trt.name.equals(name))
+                answer = true;
+        }
+        return answer;
+    }
+
+    @Override
+    public String toString() {
+        return name;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/params/SiteParams.java
@@ -0,0 +1,51 @@
+package org.gem.params;
+
+public enum SiteParams {
+
+    /** Site list */
+    SITE_LIST("Site list");
+
+    private String name;
+
+    private SiteParams(String name) {
+        this.name = name;
+    }
+
+    /**
+     * This gets the GemSourceType associated with the given string
+     * 
+     * @param name
+     * @return
+     */
+    public static SiteParams getTypeForName(String name) {
+        if (name == null)
+            throw new NullPointerException();
+        for (SiteParams trt : SiteParams.values()) {
+            if (trt.name.equals(name))
+                return trt;
+        }
+        throw new IllegalArgumentException(
+                "Area source parameter name does not exist");
+    }
+
+    /**
+     * This check whether given string is a valid Gem source type
+     * 
+     * @param name
+     * @return
+     */
+    public static boolean isValidType(String name) {
+        boolean answer = false;
+        for (SiteParams trt : SiteParams.values()) {
+            if (trt.name.equals(name))
+                answer = true;
+        }
+        return answer;
+    }
+
+    @Override
+    public String toString() {
+        return name;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/params/IntensityMeasureParams.java
@@ -0,0 +1,53 @@
+package org.gem.params;
+
+import org.opensha.commons.param.StringParameter;
+
+public enum IntensityMeasureParams {
+
+    /** Intensity measure type */
+    INTENSITY_MEAS_TYPE("Intensity measure type");
+
+    private String name;
+
+    private IntensityMeasureParams(String name) {
+        this.name = name;
+    }
+
+    /**
+     * This gets the GemSourceType associated with the given string
+     * 
+     * @param name
+     * @return
+     */
+    public static IntensityMeasureParams getTypeForName(String name) {
+        if (name == null)
+            throw new NullPointerException();
+        for (IntensityMeasureParams trt : IntensityMeasureParams.values()) {
+            if (trt.name.equals(name))
+                return trt;
+        }
+        throw new IllegalArgumentException(
+                "Area source parameter name does not exist");
+    }
+
+    /**
+     * This check whether given string is a valid Gem source type
+     * 
+     * @param name
+     * @return
+     */
+    public static boolean isValidType(String name) {
+        boolean answer = false;
+        for (IntensityMeasureParams trt : IntensityMeasureParams.values()) {
+            if (trt.name.equals(name))
+                answer = true;
+        }
+        return answer;
+    }
+
+    @Override
+    public String toString() {
+        return name;
+    }
+
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/log/AMQPConnectionFactory.java
@@ -0,0 +1,11 @@
+package org.gem.log;
+
+import java.util.Date;
+
+/**
+ * Abstract AMQP connection factory interface
+ */
+public interface AMQPConnectionFactory {
+    /// Create a new AMQP connection
+    public AMQPConnection getConnection();
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/log/AMQPAppender.java
@@ -0,0 +1,189 @@
+package org.gem.log;
+
+import java.util.Calendar;
+import java.util.Date;
+
+import org.apache.log4j.AppenderSkeleton;
+import org.apache.log4j.Layout;
+import org.apache.log4j.PatternLayout;
+import org.apache.log4j.spi.LoggingEvent;
+
+/**
+ * Log4J appender class that sends log messages through RabbitMQ
+ *
+ * Example configuration:
+ *
+ * <pre><code>
+ *   log4j.appender.rabbit=org.gem.log.AMQPAppender
+ *   log4j.appender.rabbit.host=amqp.openquake.org
+ *   log4j.appender.rabbit.port=4004
+ *   log4j.appender.rabbit.username=looser
+ *   log4j.appender.rabbit.password=s3krit
+ *   log4j.appender.rabbit.virtualhost=job.oq.org/test
+ *   log4j.appender.rabbit.routingKeyPattern=log.%p
+ *   log4j.appender.rabbit.exchange=amq.topic
+ *   log4j.appender.rabbit.layout=org.apache.log4j.PatternLayout
+ *   log4j.appender.rabbit.layout.ConversionPattern=%d %-5p [%c] - %m%n
+ * </code></pre>
+ */
+public class AMQPAppender extends AppenderSkeleton {
+    /**
+     * Collects all the data for a logging event
+     */
+    protected class Event {
+        public Event(LoggingEvent event) {
+            this.event = event;
+            this.timestamp = Calendar.getInstance().getTime();
+
+            // Handle stack trace if required
+            if (layout.ignoresThrowable()) {
+                StringBuffer buffer = new StringBuffer(layout.format(event));
+                String[] stackTrace = event.getThrowableStrRep();
+
+                if (stackTrace != null)
+                    for (String frame : stackTrace)
+                        buffer.append(frame).append(Layout.LINE_SEP);
+
+                this.message = buffer.toString();
+            }
+            else {
+                this.message = layout.format(event);
+            }
+        }
+
+        /// The log message (including stack trace if present)
+        public String message;
+        /// Timestamp when the message has been added to the logger
+        public Date timestamp;
+        /// The original logging event
+        public LoggingEvent event;
+    }
+
+    // AMQP configuration
+    protected String host, virtualHost, username, password;
+    protected int port;
+    protected String exchange = "";
+    protected PatternLayout routingKeyPattern = null;
+
+    // AMQP communication
+    protected static AMQPConnectionFactory connectionFactory;
+    protected AMQPConnection connection;
+
+    // constructors
+
+    public AMQPAppender() {
+        host = "localhost";
+        port = 5672;
+        username = "guest";
+        password = "guest";
+        virtualHost = "/";
+
+        activateOptions();
+    }
+
+    public static void setConnectionFactory(AMQPConnectionFactory factory) {
+        connectionFactory = factory;
+    }
+
+    // configuration
+
+    /// Set the exchange name
+    public void setExchange(String exchange) {
+        this.exchange = exchange;
+    }
+
+    /// Set the pattern (in PatternLayout format) used to create the routing key
+    public void setRoutingKeyPattern(String routingKeyPattern) {
+        this.routingKeyPattern = new PatternLayout(routingKeyPattern);
+    }
+
+    /// Set the AMQP host
+    public void setHost(String host) {
+        this.host = host;
+    }
+
+    /// Set the AMQP port
+    public void setPort(int port) {
+        this.port = port;
+    }
+
+    /// Set the AMQP user name
+    public void setUsername(String username) {
+        this.username= username;
+    }
+
+    /// Set the AMQP password
+    public void setPassword(String password) {
+        this.password = password;
+    }
+
+    /// Set the AMQP virtualhost
+    public void setVirtualHost(String virtualHost) {
+        this.virtualHost = virtualHost;
+    }
+
+    // override/implement Appender methods
+
+    @Override
+    public void activateOptions() {
+        super.activateOptions();
+
+        if (connection == null)
+            return;
+
+        applyConnectionParameters();
+    }
+
+    // as per discussion on IRC, we do all the sending synchronously,
+    // assuming RabbitMQ is available and fast in handling messages
+    @Override
+    protected void append(LoggingEvent event) {
+        sendMessage(new Event(event));
+    }
+
+    @Override
+    public boolean requiresLayout() {
+        return true;
+    }
+
+    @Override
+    public void close() {
+        if (connection != null)
+            connection.close();
+    }
+
+    // implementation
+
+    /// Send the log message to the AMQP server
+    protected void sendMessage(Event event) {
+        String routingKey;
+        if (routingKeyPattern != null)
+            routingKey = routingKeyPattern.format(event.event).toLowerCase();
+        else
+            routingKey = "";
+
+        if (connection == null)
+            openConnection();
+
+        connection.publish(exchange, routingKey, event.timestamp,
+                           event.event.getLevel().toString(),
+                           event.message);
+    }
+
+    private void openConnection() {
+        connection = connectionFactory.getConnection();
+        applyConnectionParameters();
+    }
+
+    private void applyConnectionParameters() {
+        connection.setHost(host);
+        connection.setPort(port);
+        connection.setUsername(username);
+        connection.setPassword(password);
+        connection.setVirtualHost(virtualHost);
+
+        // force the connection close (so it will be reopened with the
+        // new parameters the next time a message is sent)
+        connection.close();
+    }
+}
--- /dev/null
+++ java-oq-0.4.3/src/org/gem/log/AMQPConnection.java
@@ -0,0 +1,30 @@
+package org.gem.log;
+
+import java.util.Date;
+
+/**
+ * Abstract AMQP connection interface
+ */
+public interface AMQPConnection {
+    /// Set the AMQP host
+    public void setHost(String host);
+
+    /// Set the AMQP port
+    public void setPort(int port);
+
+    /// Set the AMQP user name
+    public void setUsername(String username);
+
+    /// Set the AMQP password
+    public void setPassword(String password);
+
+    /// Set the AMQP virtualhost
+    public void setVirtualHost(String virtualHost);
+
+    /// Close the AMQP connection
+    public void close();
+
+    /// Send a new message to the queue
+    public void publish(String exchange, String routingKey, Date timestamp,
+                        String level, String message);
+}
