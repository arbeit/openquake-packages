--- a/openquake/hazard/opensha.py
+++ b/openquake/hazard/opensha.py
@@ -20,16 +20,17 @@
 Wrapper around the OpenSHA-lite java library.
 """
 
+
+from itertools import izip
+import functools
 import math
-import os
 import multiprocessing
+import os
 import random
-import functools
-
-from itertools import izip
+import time
 
-from openquake import java
 from openquake import kvs
+from openquake import java
 from openquake import logs
 from openquake import shapes
 from openquake import xml
@@ -98,8 +99,7 @@
         value = self["HAZARD_TASKS"]
         return 2 * multiprocessing.cpu_count() if value is None else int(value)
 
-    def do_curves(self, sites, realizations,
-                  serializer=None,
+    def do_curves(self, sites, realizations, serializer=None,
                   the_task=tasks.compute_hazard_curve):
         """Trigger the calculation of hazard curves, serialize as requested.
 
@@ -137,12 +137,9 @@
             self.store_gmpe_map(source_model_generator.getrandbits(32))
 
             utils_tasks.distribute(
-                self.number_of_tasks(), the_task, ("site_list", sites),
+                self.number_of_tasks(), the_task, ("sites", sites),
                 dict(job_id=self.job_id, realization=realization),
-                flatten_results=True)
-
-            if serializer:
-                serializer(sites, realization)
+                flatten_results=True, ath=serializer)
 
     # pylint: disable=R0913
     def do_means(self, sites, realizations,
@@ -185,12 +182,7 @@
         utils_tasks.distribute(
             self.number_of_tasks(), curve_task, ("sites", sites),
             dict(job_id=self.job_id, realizations=realizations),
-            flatten_results=True)
-
-        if curve_serializer:
-            LOG.info("Serializing mean hazard curves")
-
-            curve_serializer(sites)
+            flatten_results=True, ath=curve_serializer)
 
         if self.poes_hazard_maps:
             assert map_func, "No calculation function for mean hazard maps set"
@@ -244,13 +236,7 @@
             self.number_of_tasks(), curve_task, ("sites", sites),
             dict(job_id=self.job_id, realizations=realizations,
                  quantiles=quantiles),
-            flatten_results=True)
-
-        if curve_serializer:
-            LOG.info("Serializing quantile curves for %s values"
-                     % len(quantiles))
-            for quantile in quantiles:
-                curve_serializer(sites, quantile)
+            flatten_results=True, ath=curve_serializer)
 
         if self.poes_hazard_maps:
             assert map_func, "No calculation function for quantile maps set."
@@ -269,10 +255,15 @@
     @java.jexception
     @preload
     @create_java_cache
-    def execute(self):
+    def execute(self, kvs_keys_purged=None):
         """
         Trigger the calculation and serialization of hazard curves, mean hazard
         curves/maps and quantile curves.
+
+        :param kvs_keys_purged: a list only passed by tests who check the
+            kvs keys used/purged in the course of the calculation.
+        :returns: the keys used in the course of the calculation (for the sake
+            of testability).
         """
         sites = self.sites_to_compute()
         realizations = self["NUMBER_OF_LOGIC_TREE_SAMPLES"]
@@ -284,20 +275,82 @@
         stats.set_total(
             self.job_id, "classical:execute:realizations", realizations)
 
-        self.do_curves(sites, realizations,
-            serializer=self.serialize_hazard_curve_of_realization)
+        block_size = config.hazard_block_size()
+        for start in xrange(0, len(sites), block_size):
+            end = start + block_size
+
+            data = sites[start:end]
+
+            self.do_curves(data, realizations,
+                serializer=self.serialize_hazard_curve_of_realization)
+
+            # mean curves
+            self.do_means(data, realizations,
+                curve_serializer=self.serialize_mean_hazard_curves,
+                map_func=classical_psha.compute_mean_hazard_maps,
+                map_serializer=self.serialize_mean_hazard_map)
+
+            # quantile curves
+            quantiles = self.quantile_levels
+            self.do_quantiles(data, realizations, quantiles,
+                curve_serializer=self.serialize_quantile_hazard_curves,
+                map_func=classical_psha.compute_quantile_hazard_maps,
+                map_serializer=self.serialize_quantile_hazard_map)
+
+            # Done with this chunk, purge intermediate results from kvs.
+            self.release_curve_data_from_kvs(
+                data, realizations, quantiles, self.poes_hazard_maps,
+                kvs_keys_purged)
+
+    def release_curve_data_from_kvs(self, sites, realizations, quantiles, poes,
+                                    kvs_keys_purged):
+        """Purge the hazard curve data for the given `sites` from the kvs.
+
+        The parameters below will be used to construct kvs keys for
+            - hazard curves (including means and quantiles)
+            - hazard maps (including means)
+
+        :param list sites: the sites for which to purge content from the kvs
+        :param int sites: the number of logic tree passes for this calculation
+        :param list sites: the quantiles specified for this calculation
+        :param list poes: the probabilities of exceedence specified for this
+            calculation
+        :param kvs_keys_purged: a list only passed by tests who check the
+            kvs keys used/purged in the course of the calculation.
+        """
+        for realization in xrange(0, realizations):
+            template = kvs.tokens.hazard_curve_poes_key_template(
+                self.job_id, realization)
+            keys = [template % hash(site) for site in sites]
+            kvs.get_client().delete(*keys)
+            if kvs_keys_purged is not None:
+                kvs_keys_purged.extend(keys)
+
+        template = kvs.tokens.mean_hazard_curve_key_template(self.job_id)
+        keys = [template % hash(site) for site in sites]
+        kvs.get_client().delete(*keys)
+        if kvs_keys_purged is not None:
+            kvs_keys_purged.extend(keys)
+
+        for quantile in quantiles:
+            template = kvs.tokens.quantile_hazard_curve_key_template(
+                self.job_id, str(quantile))
+            keys = [template % hash(site) for site in sites]
+            for poe in poes:
+                template = kvs.tokens.quantile_hazard_map_key_template(
+                    self.job_id, poe, quantile)
+                keys.extend([template % hash(site) for site in sites])
+            kvs.get_client().delete(*keys)
+            if kvs_keys_purged is not None:
+                kvs_keys_purged.extend(keys)
 
-        # mean curves
-        self.do_means(sites, realizations,
-            curve_serializer=self.serialize_mean_hazard_curves,
-            map_func=classical_psha.compute_mean_hazard_maps,
-            map_serializer=self.serialize_mean_hazard_map)
-
-        # quantile curves
-        self.do_quantiles(sites, realizations, self.quantile_levels,
-            curve_serializer=self.serialize_quantile_hazard_curves,
-            map_func=classical_psha.compute_quantile_hazard_maps,
-            map_serializer=self.serialize_quantile_hazard_map)
+        for poe in poes:
+            template = kvs.tokens.mean_hazard_map_key_template(
+                self.job_id, poe)
+            keys = [template % hash(site) for site in sites]
+            kvs.get_client().delete(*keys)
+            if kvs_keys_purged is not None:
+                kvs_keys_purged.extend(keys)
 
     def serialize_hazard_curve_of_realization(self, sites, realization):
         """
@@ -328,7 +381,7 @@
         self.serialize_hazard_curve(nrml_file, key_template, hc_attrib_update,
                                     sites)
 
-    def serialize_quantile_hazard_curves(self, sites, quantile):
+    def serialize_quantile_hazard_curves(self, sites, quantiles):
         """
         Serialize the quantile hazard curves of a set of sites for a given
         quantile.
@@ -338,16 +391,15 @@
         :param quantile: the quantile to be serialized
         :type quantile: :py:class:`float`
         """
-        hc_attrib_update = {
-            'statistics': 'quantile',
-            'quantileValue': quantile}
-        nrml_file = self.quantile_hazard_curve_filename(quantile)
-        key_template = \
-            kvs.tokens.quantile_hazard_curve_key_template(self.job_id,
-                                                          str(quantile))
-
-        self.serialize_hazard_curve(nrml_file, key_template, hc_attrib_update,
-                                    sites)
+        for quantile in quantiles:
+            hc_attrib_update = {
+                'statistics': 'quantile',
+                'quantileValue': quantile}
+            nrml_file = self.quantile_hazard_curve_filename(quantile)
+            key_template = kvs.tokens.quantile_hazard_curve_key_template(
+                self.job_id, str(quantile))
+            self.serialize_hazard_curve(nrml_file, key_template,
+                                        hc_attrib_update, sites)
 
     def serialize_hazard_curve(self, nrml_file, key_template, hc_attrib_update,
                                sites):
@@ -368,25 +420,57 @@
         :param sites: the sites of which the curve will be serialized
         :type sites: list of :py:class:`openquake.shapes.Site`
         """
+
+        def duration_generator(value):
+            """
+            Returns the initial value when called for the first time and
+            the double value upon each subsequent invocation.
+
+            N.B.: the maximum value returned will never exceed 90 (seconds).
+            """
+            yield value
+            while True:
+                if value < 45:
+                    value *= 2
+                yield value
+
         nrml_path = self.build_nrml_path(nrml_file)
 
         curve_writer = hazard_output.create_hazardcurve_writer(
             self.job_id, self.serialize_results_to, nrml_path)
         hc_data = []
 
-        for site in sites:
-            # Use hazard curve ordinate values (PoE) from KVS and abscissae
-            # from the IML list in config.
-            hc_attrib = {
-                'investigationTimeSpan': self['INVESTIGATION_TIME'],
-                'IMLValues': self.imls,
-                'IMT': self['INTENSITY_MEASURE_TYPE'],
-
-                'PoEValues': kvs.get_value_json_decoded(key_template
-                                                        % hash(site))}
-
-            hc_attrib.update(hc_attrib_update)
-            hc_data.append((site, hc_attrib))
+        sites = set(sites)
+        accounted_for = set()
+        dgen = duration_generator(0.1)
+        duration = dgen.next()
+
+        while accounted_for != sites:
+            # Sleep a little before checking the availability of additional
+            # hazard curve results.
+            time.sleep(duration)
+            results_found = 0
+            for site in sites:
+                key = key_template % hash(site)
+                value = kvs.get_value_json_decoded(key)
+                if value is None or site in accounted_for:
+                    # The curve for this site is not ready yet. Proceed to
+                    # the next.
+                    continue
+                # Use hazard curve ordinate values (PoE) from KVS and abscissae
+                # from the IML list in config.
+                hc_attrib = {
+                    'investigationTimeSpan': self['INVESTIGATION_TIME'],
+                    'IMLValues': self.imls,
+                    'IMT': self['INTENSITY_MEASURE_TYPE'],
+                    'PoEValues': value}
+                hc_attrib.update(hc_attrib_update)
+                hc_data.append((site, hc_attrib))
+                accounted_for.add(site)
+                results_found += 1
+            if not results_found:
+                # No results found, increase the sleep duration.
+                duration = dgen.next()
 
         curve_writer.serialize(hc_data)
 
@@ -406,8 +490,8 @@
             nrml_file = self.mean_hazard_map_filename(poe)
 
             hm_attrib_update = {'statistics': 'mean'}
-            key_template = kvs.tokens.mean_hazard_map_key_template(self.job_id,
-                                                          poe)
+            key_template = kvs.tokens.mean_hazard_map_key_template(
+                self.job_id, poe)
 
             self.serialize_hazard_map_at_poe(sites, poe, key_template,
                                              hm_attrib_update, nrml_file)
@@ -459,21 +543,21 @@
         """
         nrml_path = self.build_nrml_path(nrml_file)
 
-        LOG.debug("Generating NRML hazard map file for PoE %s, "\
-            "%s nodes in hazard map: %s" % (
-            poe, len(sites), nrml_file))
+        LOG.info("Generating NRML hazard map file for PoE %s, "
+                 "%s nodes in hazard map: %s" % (poe, len(sites), nrml_file))
 
         map_writer = hazard_output.create_hazardmap_writer(
             self.job_id, self.serialize_results_to, nrml_path)
         hm_data = []
 
         for site in sites:
+            key = key_template % hash(site)
             # use hazard map IML values from KVS
             hm_attrib = {
                 'investigationTimeSpan': self['INVESTIGATION_TIME'],
                 'IMT': self['INTENSITY_MEASURE_TYPE'],
                 'vs30': self['REFERENCE_VS30_VALUE'],
-                'IML': kvs.get_value_json_decoded(key_template % hash(site)),
+                'IML': kvs.get_value_json_decoded(key),
                 'poE': poe}
 
             hm_attrib.update(hm_attrib_update)
@@ -628,7 +712,7 @@
             for j in range(0, realizations):
                 stochastic_set_key = kvs.tokens.stochastic_set_key(self.job_id,
                                                                    i, j)
-                print "Writing output for ses %s" % stochastic_set_key
+                LOG.info("Writing output for ses %s" % stochastic_set_key)
                 ses = kvs.get_value_json_decoded(stochastic_set_key)
                 if ses:
                     self.serialize_gmf(ses)
--- a/openquake/hazard/tasks.py
+++ b/openquake/hazard/tasks.py
@@ -29,7 +29,6 @@
 import json
 
 from celery.task import task
-from celery.task.sets import subtask
 
 from openquake import job
 from openquake import kvs
@@ -67,30 +66,25 @@
 @task
 @java.unpack_exception
 @stats.progress_indicator
-def compute_ground_motion_fields(job_id, site_list, history, realization,
-                                 seed):
+def compute_ground_motion_fields(job_id, sites, history, realization, seed):
     """ Generate ground motion fields """
-    # TODO(JMC): Use a block_id instead of a site_list
+    # TODO(JMC): Use a block_id instead of a sites list
     check_job_status(job_id)
     hazengine = job.Job.from_kvs(job_id)
     with mixins.Mixin(hazengine, hazjob.HazJobMixin):
-        hazengine.compute_ground_motion_fields(site_list, history, realization,
+        hazengine.compute_ground_motion_fields(sites, history, realization,
                                                seed)
 
 
-@task
+@task(ignore_result=True)
 @java.unpack_exception
 @stats.progress_indicator
-def compute_hazard_curve(job_id, site_list, realization, callback=None):
+def compute_hazard_curve(job_id, sites, realization):
     """ Generate hazard curve for a given site list. """
     check_job_status(job_id)
     hazengine = job.Job.from_kvs(job_id)
     with mixins.Mixin(hazengine, hazjob.HazJobMixin):
-        keys = hazengine.compute_hazard_curve(site_list, realization)
-
-        if callback:
-            subtask(callback).delay(job_id, site_list)
-
+        keys = hazengine.compute_hazard_curve(sites, realization)
         return keys
 
 
@@ -120,7 +114,7 @@
     return json.JSONDecoder().decode(mgm)
 
 
-@task
+@task(ignore_result=True)
 @java.unpack_exception
 @stats.progress_indicator
 def compute_mean_curves(job_id, sites, realizations):
@@ -128,13 +122,13 @@
 
     check_job_status(job_id)
     HAZARD_LOG.info("Computing MEAN curves for %s sites (job_id %s)"
-            % (len(sites), job_id))
+                    % (len(sites), job_id))
 
     return classical_psha.compute_mean_hazard_curves(job_id, sites,
-        realizations)
+                                                     realizations)
 
 
-@task
+@task(ignore_result=True)
 @java.unpack_exception
 @stats.progress_indicator
 def compute_quantile_curves(job_id, sites, realizations, quantiles):
@@ -142,7 +136,7 @@
 
     check_job_status(job_id)
     HAZARD_LOG.info("Computing QUANTILE curves for %s sites (job_id %s)"
-            % (len(sites), job_id))
+                    % (len(sites), job_id))
 
-    return classical_psha.compute_quantile_hazard_curves(job_id, sites,
-        realizations, quantiles)
+    return classical_psha.compute_quantile_hazard_curves(
+        job_id, sites, realizations, quantiles)
--- a/openquake/kvs/__init__.py
+++ b/openquake/kvs/__init__.py
@@ -118,6 +118,8 @@
     """ Get value from kvs and json decode """
     try:
         value = get_client().get(key)
+        if not value:
+            return value
         decoder = json.JSONDecoder()
         return decoder.decode(value)
     except (TypeError, ValueError), e:
--- a/openquake/utils/config.py
+++ b/openquake/utils/config.py
@@ -105,3 +105,19 @@
             % pwd.getpwuid(os.geteuid()).pw_name)
         print msg
         sys.exit(2)
+
+
+def hazard_block_size(default=8192):
+    """Return the default or configured hazard block size."""
+    block_size = 0
+
+    configured_size = get("hazard", "block_size")
+    if configured_size is not None:
+        configured_size = int(configured_size.strip())
+
+    if configured_size and configured_size > 0:
+        block_size = configured_size
+    else:
+        block_size = default
+
+    return block_size
--- a/openquake/utils/tasks.py
+++ b/openquake/utils/tasks.py
@@ -22,33 +22,38 @@
 Utility functions related to splitting work into tasks.
 """
 
+import inspect
 import itertools
 
 from celery.task.sets import TaskSet
 
 from openquake.job import Job
 from openquake import logs
-from openquake.utils import config
 
 
-# Do not create batches of more than DEFAULT_BLOCK_SIZE celery subtasks.
-# Celery cannot cope with these and dies.
-DEFAULT_BLOCK_SIZE = 4096
+def _prepare_kwargs(name, data, other_args, func=None):
+    """
+    Construct the (full) set of keyword parameters for the task to be
+    invoked and/or its associated asynchronous task handler function.
 
+    If a `func` is passed it will be inspected and only parameters it is
+    prepared to receive will be included in the resulting `dict`.
 
-def _prepare_kwargs(name, data, other_args):
-    """
-    Construct the full set of keyword parameters for the task to be
-    invoked.
     """
-    return dict(other_args, **{name: data}) if other_args else {name: data}
+    params = dict(other_args, **{name: data}) if other_args else {name: data}
+    if func:
+        # A function was passed, remove params it is not prepared to receive.
+        func_params = inspect.getargspec(func).args
+        filtered_params = [(k, params[k]) for k in params if k in func_params]
+        params = dict(filtered_params)
+    return params
 
 
 # Too many local variables
 # pylint: disable=R0914
-def distribute(cardinality, the_task, (name, data), other_args=None,
+def distribute(cardinality, a_task, (name, data), other_args=None,
                flatten_results=False, ath=None):
-    """Runs `the_task` in a task set with the given `cardinality`.
+    """Runs `a_task` in a task set with the given `cardinality`.
 
     The given `data` is portioned across the subtasks in the task set.
     The results returned by the subtasks are returned in a list e.g.:
@@ -61,17 +66,17 @@
     Please note that for tasks with ignore_result=True
         - no results are returned
         - the control flow returns to the caller immediately i.e. this
-          function does *not* block while the tasks are running
-        - the user may pass in an asynchronous task handler function (`ath`)
-          that will be run as soon as the tasks have been started.
-          It can be used to check/wait for task results as appropriate. The
-          asynchronous task handler function is likely to execute in parallel
-          with longer running tasks.
+          function does *not* block while the tasks are running unless
+          the caller specifies an asynchronous task handler function.
+        - if specified, an asynchronous task handler function (`ath`)
+          will be run as soon as the tasks have been started.
+          It can be used to check/wait for task results as appropriate
+          and is likely to execute in parallel with longer running tasks.
 
     :param int cardinality: The size of the task set.
-    :param the_task: A `celery` task callable.
+    :param a_task: A `celery` task callable.
     :param str name: The parameter name under which the portioned `data` is to
-        be passed to `the_task`.
+        be passed to `a_task`.
     :param data: The `data` that is to be portioned and passed to the subtasks
         for processing.
     :param dict other_args: The remaining (keyword) parameters that are to be
@@ -83,67 +88,6 @@
     :returns: A list where each element is a result returned by a subtask.
         If an `ath` function is passed we return whatever it returns.
     """
-    logs.HAZARD_LOG.info("cardinality: %s" % cardinality)
-
-    block_size = config.get("tasks", "block_size")
-    block_size = int(block_size) if block_size else DEFAULT_BLOCK_SIZE
-
-    logs.HAZARD_LOG.debug("block_size: %s" % block_size)
-
-    data_length = len(data)
-    logs.HAZARD_LOG.debug("data_length: %s" % data_length)
-
-    ignore_results = the_task.ignore_result
-    results = []
-
-    for start in xrange(0, data_length, block_size):
-        end = start + block_size
-        logs.HAZARD_LOG.debug("data[%s:%s]" % (start, end))
-        chunk = data[start:end]
-        iresults = _distribute(cardinality, the_task, name, chunk, other_args,
-                               flatten_results, ignore_results)
-        if ignore_results:
-            # Did the user specify a asynchronous task handler function?
-            if ath:
-                pp_results = ath(**_prepare_kwargs(name, chunk, other_args))
-                results.extend(pp_results)
-        else:
-            results.extend(iresults)
-
-    return results
-
-
-# Too many local arguments
-# pylint: disable=R0913
-def _distribute(cardinality, a_task, name, data, other_args, flatten_results,
-                ignore_results):
-    """Runs `a_task` in a task set with the given `cardinality`.
-
-    The given `data` is portioned across the subtasks in the task set.
-    The results returned by the subtasks are returned in a list e.g.:
-        [result1, result2, ..]
-    If each subtask returns a list that will result in list of lists. Please
-    set `flatten_results` to `True` if you want the results to be returned in a
-    single list.
-
-    :param int cardinality: The size of the task set.
-    :param a_task: A `celery` task callable.
-    :param str name: The parameter name under which the portioned `data` is to
-        be passed to `a_task`.
-    :param data: The `data` that is to be portioned and passed to the subtasks
-        for processing.
-    :param dict other_args: The remaining (keyword) parameters that are to be
-        passed to the subtasks.
-    :param bool flatten_results: If set, the results will be returned as a
-        single list (as opposed to [[results1], [results2], ..]).
-    :param bool ignore_results: If set, the task's results are to be ignored
-        i.e. there will be no result messages.
-    :returns: A list where each element is a result returned by a subtask.
-        The result order is the same as the subtask order.
-    """
-    # Too many local variables
-    # pylint: disable=R0914
-
     data_length = len(data)
     logs.HAZARD_LOG.debug("-data_length: %s" % data_length)
 
@@ -175,9 +119,12 @@
     # a portion of the data that is to be processed. Now we will create
     # and run the task set.
     logs.HAZARD_LOG.debug("-#subtasks: %s" % len(subtasks))
-    if ignore_results:
+    if a_task.ignore_result:
         TaskSet(tasks=subtasks).apply_async()
-        return None
+        # Did the user specify a asynchronous task handler function?
+        if ath:
+            params = _prepare_kwargs(name, data, other_args, ath)
+            return ath(**params)
     else:
         # Only called when we expect result messages to come back.
         return _handle_subtasks(subtasks, flatten_results)
